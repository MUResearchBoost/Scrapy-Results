introduction,name,urlofmajors
,,
"In the United States, Geospatial Intelligence, GEOINT (GEOspatial INTelligence) is intelligence about the human activity on earth derived from the exploitation and analysis of imagery and geospatial information that describes, assesses, and visually depicts physical features and geographically referenced activities on the Earth. GEOINT, as defined in US Code, consists of imagery, imagery intelligence (IMINT) and geospatial information.[1]GEOINT knowledge and related tradecraft is no longer confined to the U.S. government, or even the world?€?s leading military powers. Additionally, countries such as India are holding GEOINT-specific conferences. While other countries may define geospatial intelligence somewhat differently than does the U.S., the use of GEOINT data and services is the same.[2]",Geospatial intelligence,https://en.wikipedia.org/wiki/Geospatial_intelligence
,,
"Fluorescence is the emission of light by a substance that has absorbed light or other electromagnetic radiation. It is a form of luminescence. In most cases, the emitted light has a longer wavelength, and therefore lower energy, than the absorbed radiation. The most striking example of fluorescence occurs when the absorbed radiation is in the ultraviolet region of the spectrum, and thus invisible to the human eye, while the emitted light is in the visible region, which gives the fluorescent substance a distinct color that can be seen only when exposed to UV light. Fluorescent materials cease to glow nearly immediately when the radiation source stops, unlike phosphorescent materials, which continue to emit light for some time after.Fluorescence has many practical applications, including mineralogy, gemology, medicine, chemical sensors (fluorescence spectroscopy), fluorescent labelling, dyes, biological detectors, cosmic-ray detection, and, most commonly, fluorescent lamps. Fluorescence also occurs frequently in nature in some minerals and in various biological states in many branches of the animal kingdom.",Fluorescence,https://en.wikipedia.org/wiki/Fluorescence
,,
This belongs to Disambiguation pages. Open it in your browser,Foundation,https://en.wikipedia.org/wiki/Foundation
,,
"Fluid mechanics is the branch of physics concerned with the mechanics of fluids (liquids, gases, and plasmas) and the forces on them.  Fluid mechanics has a wide range of applications, including mechanical engineering, civil engineering, chemical engineering, biomedical engineering, geophysics, astrophysics, and biology. Fluid mechanics can be divided into fluid statics, the study of fluids at rest; and fluid dynamics, the study of the effect of forces on fluid motion.  It is a branch of continuum mechanics, a subject which models matter without using the information that it is made out of atoms; that is, it models matter from a macroscopic viewpoint rather than from microscopic. Fluid mechanics, especially fluid dynamics, is an active field of research with many problems that are partly or wholly unsolved.  Fluid mechanics can be mathematically complex, and can best be solved by numerical methods, typically using computers.  A modern discipline, called computational fluid dynamics (CFD), is devoted to this approach to solving fluid mechanics problems. Particle image velocimetry, an experimental method for visualizing and analyzing fluid flow, also takes advantage of the highly visual nature of fluid flow.",Fluid mechanics,https://en.wikipedia.org/wiki/Fluid_mechanics
,,
"In the context of hardware and software systems, formal verification is the act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.[1]Formal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code.The verification of these systems is done by providing a formal proof on an abstract mathematical model of the system, the correspondence between the mathematical model and the nature of the system being otherwise known by construction.  Examples of mathematical objects often used to model systems are: finite state machines, labelled transition systems, Petri nets, vector addition systems, timed automata, hybrid automata, process algebra, formal semantics of programming languages such as operational semantics, denotational semantics, axiomatic semantics and Hoare logic.[2]",Formal verification,https://en.wikipedia.org/wiki/Formal_verification
,,
"A geographic information system (GIS) is a system designed to capture, store, manipulate, analyze, manage, and present spatial or geographic data. GIS applications are tools that allow users to create interactive queries (user-created searches), analyze spatial information, edit data in maps, and present the results of all these operations.[1][2] GIS (more commonly GIScience) sometimes refers to  geographic information science (GIScience),  the science underlying geographic concepts, applications, and systems.[3]GIS can refer to a number of different technologies, processes, and methods. It is attached to many operations and has many applications related to engineering, planning, management, transport/logistics, insurance, telecommunications, and business.[2] For that reason, GIS and location intelligence applications can be the foundation for many location-enabled services that rely on analysis and visualization.GIS can relate unrelated information by using location as the key index variable. Locations or extents in the Earth space?€?time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude, latitude, and elevation, respectively. All Earth-based spatial?€?temporal location and extent references should be relatable to one another and ultimately to a ""real"" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry.",Geographic information system,https://en.wikipedia.org/wiki/Geographic_information_system
,,
"In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically based techniques for the specification, development and verification of software and hardware systems.[1] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.[2]Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, discrete event dynamic system and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.[3]",Formal methods,https://en.wikipedia.org/wiki/Formal_methods
,,
"A fracture is the separation of an object or material into two or more pieces under the action of stress.  The fracture of a solid usually occurs due to the development of certain displacement discontinuity surfaces within the solid. If a displacement develops perpendicular to the surface of displacement, it is called a normal tensile crack or simply a crack; if a displacement develops tangentially to the surface of displacement, it is called a shear crack, slip band, or dislocation.[1]Brittle fractures occur with no apparent deformation before fracture; ductile fractures occur when visible deformation does occur before separation.  Fracture strength or breaking strength is the stress when a specimen fails or fractures.  A detailed understanding of how fracture occurs in materials may be assisted by the study of fracture mechanics.",Fracture,https://en.wikipedia.org/wiki/Fracture
,,
"A field-programmable gate array (FPGA) is an integrated circuit designed to be configured by a customer or a designer after manufacturing???€?  hence ""field-programmable"". The FPGA configuration is generally specified using a hardware description language (HDL), similar to that used for an application-specific integrated circuit (ASIC) (circuit diagrams were previously used to specify the configuration, as they were for ASICs, but this is increasingly rare).",Field-programmable gate array,https://en.wikipedia.org/wiki/Field-programmable_gate_array
,,
"Genomics is an interdisciplinary field of science focusing on the structure, function, evolution, mapping, and editing of genomes. A genome is an organism's complete set of DNA, including all of its genes. In contrast to genetics, which refers to the study of individual genes and their roles in inheritance, genomics aims at the collective characterization and quantification of genes, which direct the production of proteins with the assistance of enzymes and messenger molecules. In turn, proteins make up body structures such as organs and tissues as well as control chemical reactions and carry signals between cells. Genomics also involves the sequencing and analysis of genomes through uses of high throughput DNA sequencing and bioinformatics to assemble and analyze the function and structure of entire genomes.[1][2][3] Advances in genomics have triggered a revolution in discovery-based research and systems biology to facilitate understanding of even the most complex biological systems such as the brain.[4]The field also includes studies of intragenomic (within the genome) phenomena such as epistasis (effect of one gene on another), pleiotropy (one gene affecting more than one trait), heterosis (hybrid vigour), and other interactions between loci and alleles within the genome.[5]",Genomics,https://en.wikipedia.org/wiki/Genomics
,,
This belongs to Disambiguation pages. Open it in your browser,Faculty,https://en.wikipedia.org/wiki/Faculty
,,
"The finite element method (FEM), is a numerical method for solving problems of engineering and mathematical physics. Typical problem areas of interest include structural analysis, heat transfer, fluid flow, mass transport, and electromagnetic potential. The  analytical solution of these problems generally require the solution to boundary value problems for partial differential equations. The finite element method formulation of the problem results in a system of algebraic equations. The method yields approximate values of the unknowns at discrete number of points over the domain.[1] To solve the problem, it subdivides a large problem into smaller, simpler parts that are called finite elements. The simple equations that model these finite elements are then assembled into a larger system of equations that models the entire problem. FEM then uses variational methods from the calculus of variations to approximate a solution by minimizing an associated error function.Studying or analyzing a phenomenon with FEM is often referred to as finite element analysis (FEA).",Finite element method,https://en.wikipedia.org/wiki/Finite_element_method
,,
"An explosive material, also called an explosive, is a reactive substance that contains a great amount of potential energy that can produce an explosion if released suddenly, usually accompanied by the production of light, heat, sound, and pressure. An explosive charge is a measured quantity of explosive material, which may be composed of a single ingredient or a combination of two or more.The potential energy stored in an explosive material may, for example, bechemical energy, such as nitroglycerin or grain dust
pressurized gas, such as a gas cylinder or aerosol can
nuclear energy, such as in the fissile isotopes uranium-235 and plutonium-239",Explosive,https://en.wikipedia.org/wiki/Explosive
,,
"Environmental engineering system is the branch of engineering concerned with the application of scientific and engineering principles for protection of human populations from the effects of adverse environmental factors; protection of environments, both local and global, from  potentially deleterious effects of natural and human activities; and improvement of environmental quality.[1]Environmental engineering system can also be described as a branch of applied science and technology that addresses the issues of energy preservation, protection of assets and control of waste from human and animal activities. Furthermore, it is concerned with finding plausible solutions in the field of public health, such as waterborne diseases, implementing laws which promote adequate sanitation in urban, rural and recreational areas. It involves waste water management, air pollution control, recycling, waste disposal, radiation protection, industrial hygiene, animal agriculture, environmental sustainability, public health and environmental engineering law. It also includes studies on the environmental impact of proposed construction projects.Environmental engineers system study the effect of technological advances on the environment. To do so, they conduct studies on hazardous-waste management to evaluate the significance of such hazards, advise on treatment and containment, and develop regulations to prevent mishaps. Environmental engineers design municipal water supply and industrial wastewater treatment systems.[2][3] They address local and worldwide environmental issues such as the effects of acid rain, global warming, ozone depletion, water pollution and air pollution from automobile exhausts and industrial sources.[4][5][6][7]Many universities offer environmental engineering programs at either the department of civil engineering or the department of chemical engineering at engineering faculties. Environmental ""civil"" engineers focus on hydrology, water resources management, bioremediation, and water treatment plant design. Environmental ""chemical"" engineers, on the other hand, focus on environmental chemistry, advanced air and water treatment technologies and separation processes.[citation needed] Some subdivision of environmental engineering include natural resources engineering and agricultural engineering.More engineers are obtaining specialized training in law (J.D.) and are utilizing their technical expertise in the practices of environmental engineering law.[citation needed]Most jurisdictions also impose licensing and registration requirements.",Environmental engineering,https://en.wikipedia.org/wiki/Environmental_engineering
,,
"Filmmaking (or, in an academic context, film production) is the process of making a film, generally in the sense of films intended for extensive theatrical exhibition. Filmmaking involves a number of discrete stages including an initial story, idea, or commission, through screenwriting, casting, shooting, sound recording and reproduction, editing, and screening the finished product before an audience that may result in a film release and exhibition. Filmmaking takes place in many places around the world in a range of economic, social, and political contexts, and using a variety of technologies and cinematic techniques. Typically, it involves a large number of people, and can take from a few months to several years to complete.",Filmmaking,https://en.wikipedia.org/wiki/Filmmaking
,,
"Entertainment technology is the discipline of using manufactured or created components to enhance or make possible any sort of entertainment experience.  Because entertainment categories are so broad, and because entertainment models the world in many ways, the types of implemented technology are derived from a variety of sources.  Thus, in theatre, for example, entertainment technology practitioners must be able to design and construct scenery, install electrical systems, build clothing, use motors if there is scenery automation,[clarification needed] provide plumbing (if functioning kitchen fixtures are required, or if ""singing in the rain""), etc. In this way, the entertainment technology field intersects with most other types of technology.Traditionally, entertainment technology is derived from theatrical stagecraft, and stagecraft is an important subset of the discipline. However, the rise of new types and venues for entertainment, as well as rapidly advancing technological development, has increased the range and scope of its practice.Entertainment technology includes:Scenery fabrication
Properties
Costume
Lighting
Sound
Video
Show control
Automation
Animatronics
Interactive environments
Computer simulation",Entertainment technology,https://en.wikipedia.org/wiki/Entertainment_technology
,,
"Historically, women in the United States have been represented at lower rates than men in both science and engineering college programs and careers.  Over time, this pattern has led to a significantly higher concentration of male professional engineers compared to women.[1][2]  Additionally, this disparity has led to careers in Education, History, English, Humanities and the like to be seen as ?€?feminine?€? careers and areas of study.[1][2] Some Feminist theorists suggest that these social and historical factors have perpetuated women?€?s low participation rates in engineering over time.[2] Numerous explanations and points of view have been offered to explain women's participation rates in this field. These explanations include beliefs regarding women's lack of interest in science and engineering, their physiological inability to succeed as engineers, and environmental factors in women's childhoods that discourage them from entering science and engineering fields.[1][2]Negative perceptions of female engineers may play a role in explaining their low numbers within the field.[3] According to recent statistics, college-educated women are less than half as likely as men to be employed in science and engineering jobs.Two forms of activism tasked with raising awareness include both organizations on college campuses and those geared towards society at large.",Women in engineering in the United States,https://en.wikipedia.org/wiki/Women_in_engineering_in_the_United_States
,,
"A wireless network is a computer network that uses wireless data connections between network nodes.[1]Wireless networking is a method by which homes, telecommunications networks and business installations avoid the costly process of introducing cables into a building, or as a connection between various equipment locations.[2] Wireless telecommunications networks are generally implemented and administered using radio communication. This implementation takes place at the physical level (layer) of the OSI model network structure.[3]Examples of wireless networks include cell phone networks, wireless local area networks (WLANs), wireless sensor networks, satellite communication networks, and terrestrial microwave networks.[4]",Wireless network,https://en.wikipedia.org/wiki/Wireless_network
,,
"Video content analysis (also video content analytics, VCA) is the capability of automatically analyzing video to detect and determine temporal and spatial events.This technical capability is used in a wide range of domains including entertainment,[1] health-care, retail, automotive, transport, home automation, flame and smoke detection, safety and security.[2] The algorithms can be implemented as software on general purpose machines, or as hardware in specialized video processing units.Many different functionalities can be implemented in VCA. Video Motion Detection is one of the simpler forms where motion is detected with regard to a fixed background scene. More advanced functionalities include video tracking and egomotion estimation.Based on the internal representation that VCA generates in the machine, it is possible to build other functionalities, such as identification, behavior analysis or other forms of situation awareness.VCA relies on good input video, so it is often combined with video enhancement technologies such as video denoising, image stabilization, unsharp masking and super-resolution.",Video content analysis,https://en.wikipedia.org/wiki/Video_content_analysis
,,
"A brain tumor occurs when abnormal cells form within the brain.[2] There are two main types of tumors: malignant or cancerous tumors and benign tumors.[2] Cancerous tumors can be divided into primary tumors that start within the brain, and secondary tumors that have spread from elsewhere, known as brain metastasis tumors.[1] All types of brain tumors may produce symptoms that vary depending on the part of the brain involved.[2] These symptoms may include headaches, seizures, problems with vision, vomiting, and mental changes.[1][2][7] The headache is classically worse in the morning and goes away with vomiting.[2] Other symptoms may include difficulty walking, speaking, or with sensations.[1][3] As the disease progresses, unconsciousness may occur.[3]The cause of most brain tumors is unknown.[2] Uncommon risk factors include inherited neurofibromatosis, exposure to vinyl chloride, Epstein?€?Barr virus, and ionizing radiation.[1][2][3] The evidence for mobile phones is not clear.[3] The most common types of primary tumors in adults are meningiomas (usually benign), and astrocytomas such as glioblastomas.[1] In children, the most common type is a malignant medulloblastoma.[3] Diagnosis is usually by medical examination along with computed tomography or magnetic resonance imaging.[2] This is then often confirmed by a biopsy.[1] Based on the findings, the tumors are divided into different grades of severity.[1]Treatment may include some combination of surgery, radiation therapy, and chemotherapy.[1] Anticonvulsant medication may be needed if seizures occur.[1] Dexamethasone and furosemide may be used to decrease swelling around the tumor.[1] Some tumors grow gradually, requiring only monitoring and possibly needing no further intervention.[1] Treatments that use a person's immune system are being studied.[2] Outcome varies considerably depending on the type of tumor and how far it has spread at diagnosis.[3] Glioblastomas usually have poor outcomes while meningiomas usually have good outcomes.[3] The average five-year survival rate for all brain cancer in the United States is 33%.[4]Secondary or metastatic brain tumors are more common than primary brain tumors,[2] with about half of metastases coming from lung cancer.[2] Primary brain tumors occur in around 250,000 people a year globally, making up less than 2% of cancers.[3] In children younger than 15, brain tumors are second only to acute lymphoblastic leukemia as the most common form of cancer.[8] In Australia the average lifetime economic cost of a case of brain cancer is $1.9 million, the greatest of any type of cancer.[9]",Brain tumor,https://en.wikipedia.org/wiki/Brain_tumor
,,
"Water is a transparent, tasteless, odorless, and nearly colorless chemical substance, which is the main constituent of Earth's streams, lakes, and oceans, and the fluids of most living organisms. It is vital for all known forms of life, even though it provides no calories or organic nutrients. Its chemical formula is H2O, meaning that each of its molecules contains one oxygen and two hydrogen atoms connected by covalent bonds. Water is the name of the liquid state of H2O at standard ambient temperature and pressure. It forms precipitation in the form of rain and aerosols in the form of fog. Clouds   are formed from suspended droplets of water and ice, its solid state. When finely divided, crystalline ice may precipitate in the form of snow. The gaseous state of water is steam or water vapor. Water moves continually through the water cycle of evaporation, transpiration (evapotranspiration), condensation, precipitation, and runoff, usually reaching the sea.Water covers 71% of the Earth's surface, mostly in seas and oceans.[1] Small portions of water occur as groundwater (1.7%), in the glaciers and the ice caps of Antarctica and Greenland (1.7%), and in the air as vapor, clouds (formed of ice and liquid water suspended in air), and precipitation (0.001%).[2][3]Water plays an important role in the world economy. Approximately 70% of the freshwater used by humans goes to agriculture.[4] Fishing in salt and fresh water bodies is a major source of food for many parts of the world. Much of long-distance trade of commodities (such as oil and natural gas) and manufactured products is transported by boats through seas, rivers, lakes, and canals. Large quantities of water, ice, and steam are used for cooling and heating, in industry and homes. Water is an excellent solvent for a wide variety of chemical substances; as such it is widely used in industrial processes, and in cooking and washing. Water is also central to many sports and other forms of entertainment, such as swimming, pleasure boating, boat racing, surfing, sport fishing, and diving.",Water,https://en.wikipedia.org/wiki/Water
,,
"The National Transportation Safety Board (NTSB) is an independent U.S. government investigative agency responsible for civil transportation accident investigation. In this role, the NTSB investigates and reports on aviation accidents and incidents, certain types of highway crashes, ship and marine accidents, pipeline incidents, and railroad accidents.[2] When requested, the NTSB will assist the military and foreign governments with accident investigation.[2] The NTSB is also in charge of investigating cases of hazardous materials releases that occur during transportation.  The agency is based in Washington, D.C.  As of  December??2014[update], it has four regional offices located in Anchorage, Alaska; Denver, Colorado; Ashburn, Virginia; and Seattle, Washington.[3]  The agency also operates a national training center at its Ashburn facility.[4]",National Transportation Safety Board,https://en.wikipedia.org/wiki/National_Transportation_Safety_Board
,,
"Transport or transportation is the movement of humans, animals and goods from one location to another.  Modes of transport include air, land (rail and road), water, cable, pipeline and space. The field can be divided into infrastructure, vehicles and operations. Transport is important because it enables trade between people, which is essential for the development of civilizations.Transport infrastructure consists of the fixed installations, including roads, railways, airways, waterways, canals and pipelines and terminals such as airports, railway stations, bus stations, warehouses, trucking terminals, refueling depots (including fueling docks and fuel stations) and seaports. Terminals may be used both for interchange of passengers and cargo and for maintenance.Vehicles traveling on these networks may include automobiles, bicycles, buses, trains, trucks, helicopters, watercraft, spacecraft and aircraft.Operations deal with the way the vehicles are operated, and the procedures set for this purpose, including financing, legalities, and policies. In the transport industry, operations and ownership of infrastructure can be either public or private, depending on the country and mode.Passenger transport may be public, where operators provide scheduled services, or private. Freight transport has become focused on containerization, although bulk transport is used for large volumes of durable items. Transport plays an important part in economic growth and globalization, but most types cause air pollution and use large amounts of land. While it is heavily subsidized by governments, good planning of transport is essential to make traffic flow and restrain urban sprawl.",Transport,https://en.wikipedia.org/wiki/Transport
,,
This belongs to Disambiguation pages. Open it in your browser,Tissue,https://en.wikipedia.org/wiki/Tissue
,,
"Thermodynamics is the branch of physics concerned with heat and temperature and their relation to energy and work. The behavior of these quantities is governed by the four laws of thermodynamics, irrespective of the composition or specific properties of the material or system in question.  The laws of thermodynamics are explained in terms of microscopic constituents by statistical mechanics.  Thermodynamics applies to a wide variety of topics in science and engineering, especially physical chemistry, chemical engineering and mechanical engineering.Historically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of French physicist Nicolas L??onard Sadi Carnot (1824) who believed that engine efficiency was the key that could help France win the Napoleonic Wars.[1] Scottish physicist Lord Kelvin was the first to formulate a concise definition of thermodynamics in 1854[2] which stated, ""Thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency.""The initial application of thermodynamics to mechanical heat engines was extended early on to the study of chemical compounds and chemical reactions. Chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field.[3][4][5][6][7][8][9][10][11] Other formulations of thermodynamics emerged in the following decades. Statistical thermodynamics, or statistical mechanics, concerned itself with statistical predictions of the collective motion of particles from their microscopic behavior. In 1909, Constantin Carath??odory presented a purely mathematical approach to the field in his axiomatic formulation of thermodynamics, a description often referred to as geometrical thermodynamics.",Thermodynamics,https://en.wikipedia.org/wiki/Thermodynamics
,,
"Waste management or waste disposal are all the activities and actions required to manage waste from its inception to its final disposal.[1]
This includes amongst other things collection, transport, treatment and disposal of waste together with monitoring and regulation. It also encompasses the legal and regulatory framework that relates to waste management encompassing guidance on recycling.Waste can take any form that is solid, liquid, or gas and each have different methods of disposal and management. Waste management normally deals with all types of waste whether it was created in forms that are industrial, biological, household, and special cases where it may pose a threat to human health.[2] It is produced due to human activity such as when factories extract and process raw materials.[3] Waste management is intended to reduce adverse effects of waste on health, the environment or aesthetics.Waste management practices are not uniform among countries (developed and developing nations); regions (urban and rural areas), and sectors (residential and industrial).[4]A large portion of waste management practices deal with municipal solid waste (MSW) which is the bulk of the waste that is created by household, industrial, and commercial activity.[5]",Waste management,https://en.wikipedia.org/wiki/Waste_management
,,
"A supercritical fluid (SCF[1]) is any substance at a temperature and pressure above its critical point, where distinct liquid and gas phases do not exist. It can effuse through solids like a gas, and dissolve materials like a liquid. In addition, close to the critical point, small changes in pressure or temperature result in large changes in density, allowing many properties of a supercritical fluid to be ""fine-tuned"". Supercritical fluids are suitable as a substitute for organic solvents in a range of industrial and laboratory processes. Carbon dioxide and water are the most commonly used supercritical fluids, being used for decaffeination and power generation, respectively.",Supercritical fluid,https://en.wikipedia.org/wiki/Supercritical_fluid
,,
"In commerce, supply chain management (SCM), the management of the flow of goods and services,[2] involves the movement and storage of raw materials, of work-in-process inventory, and of finished goods from point of origin to point of consumption. Interconnected or interlinked networks, channels and node businesses combine in the provision of products and services required by end customers in a supply chain.[3] Supply-chain management has been defined [4] as the ""design, planning, execution, control, and monitoring of supply chain activities with the objective of creating net value, building a competitive infrastructure, leveraging worldwide logistics, synchronizing supply with demand and measuring performance globally.""[5]
SCM practice draws heavily from the areas of industrial engineering, systems engineering, operations management, logistics, procurement, information technology, and marketing [6] and strives for an integrated approach.[citation needed] Marketing channels play an important role in supply chain management.[6] Current research in supply chain management is concerned with topics related to sustainability and risk management,[7] among others. Some suggest that the ?€?people dimension?€? of SCM, ethical issues, internal integration, transparency/visibility, and human capital/talent management are topics that have, so far, been underrepresented on the research agenda.[8]",Supply chain management,https://en.wikipedia.org/wiki/Supply_chain_management
,,
"The Student Success Act (H.R. 5) is a bill that was introduced into the United States House of Representatives during the 113th Congress.  The bill deals with education policy and would alter parts of both the Elementary and Secondary Education Act and the No Child Left Behind Act.[1] The Student Success Act passed in a House vote of 221-207 on July 19, 2013.",Student Success Act,https://en.wikipedia.org/wiki/Student_Success_Act
,,
"Structure is an arrangement and organization of interrelated elements in a material object or system, or the object or system so organized.[1] Material structures include man-made objects such as buildings and machines and natural objects such as biological organisms, minerals and chemicals. Abstract structures include data structures in computer science and musical form. Types of structure include a hierarchy (a cascade of one-to-many relationships), a network featuring many-to-many links, or a lattice featuring connections between components that are neighbors in space.",Structure,https://en.wikipedia.org/wiki/Structure
,,
"Statistics is a branch of mathematics dealing with the collection, organization, analysis, interpretation and presentation of data.[1][2] In applying statistics to, for example, a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as ""all people living in a country"" or ""every atom composing a crystal"". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1]
See glossary of probability and statistics.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[3] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.A standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a ""false positive"") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a ""false negative"").[4] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. In more recent years statistics has relied more on statistical software to produce tests such as descriptive analysis.[5]",Statistics,https://en.wikipedia.org/wiki/Statistics
,,
"Sustainability is the process of maintaining change in a balanced fashion, in which the exploitation of resources, the direction of investments, the orientation of technological development and institutional change are all in harmony and enhance both current and future potential to meet human needs and aspirations.[1] For many in the field, sustainability is defined through the following interconnected domains or pillars: environment, economic and social.[2] Sub-domains of sustainable development have been considered also: cultural, technological and political.[3] While sustainable development may be the organizing principle for sustainability for some, for others, the two terms are paradoxical (i.e. development is inherently unsustainable).[4][5] Sustainable development is the development that meets the needs of the present without compromising the ability of future generations to meet their own needs.[1] Brundtland Report for the World Commission on Environment and Development (1987) introduced the term of sustainable development.Sustainability can also be defined as a socio-ecological process characterized by the pursuit of a common ideal.[6] 
An ideal is by definition unattainable in a given time and space. However, by persistently and dynamically approaching it, the process results in a sustainable system.[6]Healthy ecosystems and environments are necessary to the survival of humans and other organisms. Ways of reducing negative human impact are environmentally-friendly chemical engineering, environmental resources management and environmental protection. Information is gained from green computing, green chemistry, earth science, environmental science and conservation biology. Ecological economics studies the fields of academic research that aim to address human economies and natural ecosystems.[7]Moving towards sustainability is also a social challenge that entails international and national law, urban planning and transport, local and individual lifestyles and ethical consumerism. Ways of living more sustainably can take many forms from reorganizing living conditions (e.g., ecovillages, eco-municipalities and sustainable cities), reappraising economic sectors (permaculture, green building, sustainable agriculture), or work practices (sustainable architecture), using science to develop new technologies (green technologies, renewable energy and sustainable fission and fusion power), or designing systems in a flexible and reversible manner,[8][9] and adjusting individual lifestyles that conserve natural resources.[10]""The term 'sustainability' should be viewed as humanity's target goal of human-ecosystem equilibrium (homeostasis), while 'sustainable development' refers to the holistic approach and temporal processes that lead us to the end point of sustainability."" (305)[11] Despite the increased popularity of the use of the term ""sustainability"", the possibility that human societies will achieve environmental sustainability has been, and continues to be, questioned?€?in light of environmental degradation, climate change, overconsumption, population growth and societies' pursuit of unlimited economic growth in a closed system.[12][13]",Sustainability,https://en.wikipedia.org/wiki/Sustainability
,,
This belongs to Disambiguation pages. Open it in your browser,Staff,https://en.wikipedia.org/wiki/Staff
,,
"Stormwater, also spelled storm water, is water that originates during precipitation events and snow/ice melt.  Stormwater can soak into the soil (infiltrate), be held on the surface and evaporate, or runoff and end up in nearby streams, rivers, or other water bodies (surface water).In natural landscapes such as forests, the soil absorbs much of the stormwater and plants help hold stormwater close to where it falls. In developed environments, unmanaged stormwater can create two major issues: one related to the volume and timing of runoff water (flooding) and the other related to potential contaminants that the water is carrying (water pollution).Stormwater is also an important resource as the world's human population demand exceeds the availability of readily available water. Techniques of stormwater harvesting with point source water management and purification can potentially make urban environments self-sustaining in terms of water.",Stormwater,https://en.wikipedia.org/wiki/Stormwater
,,
"Social network analysis (SNA) is the process of investigating social structures  through the use of networks and graph theory.[1] It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them.  Examples of social structures commonly visualized through social network analysis include social media networks,[2] memes spread,[3] friendship and acquaintance networks, collaboration graphs, kinship, disease transmission, and sexual relationships.[4][5] These networks are often visualized through sociograms in which nodes are represented as points and ties are represented as lines.Social network analysis has emerged as a key technique in modern sociology.  It has also gained a significant following in anthropology, biology, demography, communication studies, economics, geography, history,[6] information science, organizational studies, political science, social psychology, development studies, sociolinguistics, and computer science and is now commonly available as a consumer tool (see the list of SNA software).[7][8][9][10]",Social network analysis,https://en.wikipedia.org/wiki/Social_network_analysis
,,
"A smart city is an urban area that uses different types of electronic data collection sensors to supply information which is used to manage assets and resources efficiently. This includes data collected from citizens, devices, and assets that is processed and analyzed to monitor and manage traffic and transportation systems, power plants, water supply networks, waste management, law enforcement, information systems, schools, libraries, hospitals, and other community services.[1][2][page??needed] The smart city concept integrates information and communication technology  (ICT), and various physical devices connected to the network (the Internet of things or IoT) to optimize the efficiency of city operations and services and connect to citizens.[3][4] Smart city technology allows city officials to interact directly with both community and city infrastructure and to monitor what is happening in the city and how the city is evolving.ICT is used to enhance quality, performance and interactivity of urban services, to reduce costs and resource consumption and to increase contact between citizens and government.[5]
Smart city applications are developed to manage urban flows and allow for real-time responses.[6] A smart city may therefore be more prepared to respond to challenges than one with a simple ""transactional"" relationship with its citizens.[7][8] Yet, the term itself remains unclear to its specifics and therefore, open to many interpretations.[9]Other terms that have been used for similar concepts include cyberville, digital city, electronic communities, flexicity, information city, intelligent city, knowledge-based city, MESH city, telecity, teletopia, Ubiquitous city, wired city.Major technological, economic and environmental changes have generated interest in smart cities, including climate change, economic restructuring, the move to online retail and entertainment, ageing populations, urban population growth and pressures on public finances.[10] The European Union (EU) has devoted constant efforts to devising a strategy for achieving 'smart' urban growth for its metropolitan city-regions.[11][12] The EU has developed a range of programmes under  'Europe's Digital Agenda"".[13] In 2010, it highlighted its focus on strengthening innovation and investment in ICT services for the purpose of improving public services and quality of life.[12] Arup estimates that the global market for smart urban services will be $400 billion per annum by 2020.[14] Examples of Smart City technologies and programs have been implemented in Singapore[15], Dubai,[16] Milton Keynes,[17] Southampton,[18] Amsterdam,[19] Barcelona,[20] Madrid,[21] Stockholm,[22] China[23] and New York.[24]",Smart city,https://en.wikipedia.org/wiki/Smart_city
,,
"Six Sigma (6??) is a set of techniques and tools for process improvement. It was introduced by engineer Bill Smith while working at Motorola in 1986.[1][2] Jack Welch  made it central to his business strategy at General Electric in 1995.Six Sigma strategies seek to improve the quality of the output of a process by identifying and removing the causes of defects and minimizing variability in manufacturing and business processes.  It uses a set of quality management methods, mainly empirical, statistical methods, and creates a special infrastructure of people within the organization who are experts in these methods. Each Six Sigma project carried out within an organization follows a defined sequence of steps and has specific value targets, for example: reduce process cycle time, reduce pollution, reduce costs, increase customer satisfaction, and increase profits.The term Six Sigma (capitalized because it was written that way when registered as a Motorola trademark on December 28, 1993) originated from terminology associated with statistical modeling of manufacturing processes. The maturity of a manufacturing process can be described by a sigma rating indicating its yield or the percentage of defect-free products it creates. A six sigma process is one in which 99.99966% of all opportunities to produce some feature of a part are statistically expected to be free of defects (3.4 defective features per million opportunities). Motorola set a goal of ""six sigma"" for all of its manufacturing.",Six Sigma,https://en.wikipedia.org/wiki/Six_Sigma
,,
"The Society of Women Engineers (SWE), founded in 1950, is a not-for-profit educational and service organization in the United States. SWE has over 37,000 members in nearly 100 professional sections and 300 student sections throughout the United States.",Society of Women Engineers,https://en.wikipedia.org/wiki/Society_of_Women_Engineers
,,
"Signal processing concerns the analysis, synthesis, and modification of signals, which are broadly defined as functions conveying ""information about the behavior or attributes of some phenomenon"",[1] such as sound, images, and biological measurements.[2] For example, signal processing techniques are used to improve signal transmission fidelity, storage efficiency, and subjective quality, and to emphasize or detect components of interest in a measured signal.[3]",Signal processing,https://en.wikipedia.org/wiki/Signal_processing
,,
This belongs to Disambiguation pages. Open it in your browser,Separation,https://en.wikipedia.org/wiki/Separation
,,
"A simulation is an imitation of the operation of a real-world process or system.[1] The act of simulating something first requires that a model be developed; this model represents the key characteristics, behaviors and functions of the selected physical or abstract system or process.  The model represents the system itself, whereas the simulation represents the operation of the system over time.Simulation is used in many contexts, such as simulation of technology for performance optimization, safety engineering, testing, training, education, and video games.  Often, computer experiments are used to study simulation models. Simulation is also used with scientific modelling of natural systems or human systems to gain insight into their functioning,[2] as in economics. Simulation can be used to show the eventual real effects of alternative conditions and courses of action.  Simulation is also used when the real system cannot be engaged, because it may not be accessible, or it may be dangerous or unacceptable to engage, or it is being designed but not yet built, or it may simply not exist.[3]Key issues in simulation include acquisition of valid source information about the relevant selection of key characteristics and behaviours, the use of simplifying approximations and assumptions within the simulation, and fidelity and validity of the simulation outcomes. Procedures and protocols for model verification and validation are an ongoing field of academic study, refinement, research and development in simulations technology or practice, particularly in the field of computer simulation.",Simulation,https://en.wikipedia.org/wiki/Simulation
,,
"Semiconductor devices are electronic components that exploit the electronic properties of semiconductor materials, principally silicon, germanium, and gallium arsenide, as well as organic semiconductors. Semiconductor devices have replaced thermionic devices (vacuum tubes) in most applications. They use electronic conduction in the solid state as opposed to the gaseous state or thermionic emission in a high vacuum.Semiconductor devices are manufactured both as single discrete devices and as integrated circuits (ICs), which consist of a number?€?from a few (as low as two) to billions?€?of devices manufactured and interconnected on a single semiconductor substrate, or wafer.Semiconductor materials are useful because their behavior can be easily manipulated by the addition of impurities, known as doping. Semiconductor conductivity can be controlled by the introduction of an electric or magnetic field, by exposure to light or heat, or by the mechanical deformation of a doped monocrystalline grid; thus, semiconductors can make excellent sensors. Current conduction in a semiconductor occurs via mobile or ""free"" electrons and holes, collectively known as charge carriers. Doping a semiconductor such as silicon with a small proportion of an atomic impurity, such as phosphorus or boron, greatly increases the number of free electrons or holes within the semiconductor. When a doped semiconductor contains excess holes it is called ""p-type"", and when it contains excess free electrons it is known as ""n-type"", where p (positive for holes) or n (negative for electrons) is the sign of the charge of the majority mobile charge carriers. The semiconductor material used in devices is doped under highly controlled conditions in a fabrication facility, or fab, to control precisely the location and concentration of p- and n-type dopants. The junctions which form where n-type and p-type semiconductors join together are called p?€?n junctions.Semiconductor devices made per year have been growing by 9.1% on average since 1978 and shipments in 2018 are predicted for the first time to exceed 1 trillion,[1] meaning well over 7  trillion has been made to date, in just in the decade prior.",Semiconductor device,https://en.wikipedia.org/wiki/Semiconductor_device
,,
"Security is freedom from, or resilience against, potential harm (or other unwanted coercive change) from external forces. Beneficiaries (technically referents) of security may be persons and social groups, objects and institutions, ecosystems, and any other entity or phenomenon vulnerable to unwanted change by its environment.",Security,https://en.wikipedia.org/wiki/Security
,,
"In the broadest definition, a sensor is a device, module, or subsystem whose purpose is to detect events or changes in its environment and send the information to other electronics, frequently a computer processor.  A sensor is always used with other electronics, whether as simple as a light or as complex as a computer.Sensors are used in everyday objects such as touch-sensitive elevator buttons (tactile sensor) and lamps which dim or brighten by touching the base, besides innumerable applications of which most people are never aware. With advances in micromachinery and easy-to-use microcontroller platforms, the uses of sensors have expanded beyond the traditional fields of temperature, pressure or flow measurement,[1] for example into MARG sensors. Moreover, analog sensors such as potentiometers and force-sensing resistors are still widely used. Applications include manufacturing and machinery, airplanes and aerospace, cars, medicine, robotics and many other aspects of our day-to-day life.A sensor's sensitivity indicates how much the sensor's output changes when the input quantity being measured changes. For instance, if the mercury in a thermometer moves 1?? cm when the temperature changes by 1????C, the sensitivity is 1??cm/??C (it is basically the slope Dy/Dx assuming a linear characteristic). Some sensors can also affect what they measure; for instance, a room temperature thermometer inserted into a hot cup of liquid cools the liquid while the liquid heats the thermometer.  Sensors are usually designed to have a small effect on what is measured; making the sensor smaller often improves this and may introduce other advantages.[2]  Technological progress allows more and more sensors to be manufactured on a microscopic scale as microsensors using MEMS technology. In most cases, a microsensor reaches a significantly higher speed and sensitivity compared with macroscopic approaches.[3][4]",Sensor,https://en.wikipedia.org/wiki/Sensor
,,
"Remote sensing is the acquisition of information about an object or phenomenon without making physical contact with the object and thus in contrast to on-site observation. Remote sensing is used in numerous fields, including geography, land surveying and most Earth Science disciplines (for example, hydrology, ecology, oceanography, glaciology, geology); it also has military, intelligence, commercial, economic, planning, and humanitarian applications.In current usage, the term ""remote sensing"" generally refers to the use of satellite- or aircraft-based sensor technologies to detect and classify objects on Earth, including on the surface and in the atmosphere and oceans, based on propagated signals (e.g. electromagnetic radiation). It may be split into ""active"" remote sensing (i.e., when a signal is emitted by a satellite or aircraft and its reflection by the object is detected by the sensor) and ""passive"" remote sensing (i.e., when the reflection of sunlight is detected by the sensor).[1][2][3][4][5]",Remote sensing,https://en.wikipedia.org/wiki/Remote_sensing
,,
"In accountancy, depreciation refers to two aspects of the same concept:[1]The decrease in value of assets (fair value depreciation)
The allocation of the cost of assets to periods in which the assets are used (depreciation with the matching principle)",Depreciation,https://en.wikipedia.org/wiki/Depreciation
,,
"Regenerative medicine is a branch of translational research[1] in tissue engineering and molecular biology which deals with the ""process of replacing, engineering or regenerating human cells, tissues or organs to restore or establish normal function"".[2] This field holds the promise of engineering damaged tissues and organs by stimulating the body's own repair mechanisms to functionally heal previously irreparable tissues or organs.[3]Regenerative medicine also includes the possibility of growing tissues and organs in the laboratory and implanting them when the body cannot heal itself. If a regenerated organ's cells would be derived from the patient's own tissue or cells,[4] this would potentially solve the problem of the shortage of organs available for donation, and the problem of organ transplant rejection.[5][6][7]Some of the biomedical approaches within the field of regenerative medicine may involve the use of stem cells.[8] Examples include the injection of stem cells or progenitor cells obtained through directed differentiation (cell therapies); the induction of regeneration by biologically active molecules administered alone or as a secretion by infused cells (immunomodulation therapy); and transplantation of in vitro grown organs and tissues (tissue engineering).[9][10]",Regenerative medicine,https://en.wikipedia.org/wiki/Regenerative_medicine
,,
"Soil is a mixture of organic matter, minerals, gases, liquids, and organisms that together support life. Earth's body of soil is the pedosphere, which has four important functions: it is a medium for plant growth; it is a means of water storage, supply and purification; it is a modifier of Earth's atmosphere; it is a habitat for organisms; all of which, in turn, modify the soil.The pedosphere interfaces with the lithosphere, the hydrosphere, the atmosphere, and the biosphere.[1] The term pedolith, used commonly to refer to the soil, translates to ground stone. Soil consists of a solid phase of minerals and organic matter (the soil matrix), as well as a porous phase that holds gases (the soil atmosphere) and water (the soil solution).[2][3][4] Accordingly, soils are often treated as a three-state system of solids, liquids, and gases.[5]Soil is a product of the influence of climate, relief (elevation, orientation, and slope of terrain), organisms, and its parent materials (original minerals) interacting over time.[6] It continually undergoes development by way of numerous physical, chemical and biological processes, which include weathering with associated erosion. Given its complexity and strong internal connectedness, it is considered an ecosystem by soil ecologists.[7]Most soils have a dry bulk density (density of soil taking into account voids when dry) between 1.1 and 1.6??g/cm3, while the soil particle density is much higher, in the range of 2.6 to 2.7??g/cm3.[8] Little of the soil of planet Earth is older than the Pleistocene and none is older than the Cenozoic,[9] although fossilized soils are preserved from as far back as the Archean.[10]Soil science has two basic branches of study: edaphology and pedology. Edaphology is concerned with the influence of soils on living things.[11] Pedology is focused on the formation, description (morphology), and classification of soils in their natural environment.[12] In engineering terms, soil is included in the broader concept of regolith, which also includes other loose material that lies above the bedrock, as can be found on the Moon and other celestial objects, as well.[13] Soil is also commonly referred to as earth or dirt; some scientific definitions distinguish dirt from soil by restricting the former term specifically to displaced soil.[14]",Soil,https://en.wikipedia.org/wiki/Soil
,,
This belongs to Disambiguation pages. Open it in your browser,Reliability,https://en.wikipedia.org/wiki/Reliability
,,
"Chemical reaction engineering (reaction engineering or reactor engineering) is a specialty in chemical engineering or industrial chemistry dealing with chemical reactors.  Frequently the term relates specifically to catalytic reaction systems where either a homogeneous or heterogeneous catalyst is present in the reactor.  Sometimes a reactor per se is not present by itself, but rather is integrated into a process, for example in reactive separations vessels, retorts, certain fuel cells, and photocatalytic surfaces. The issue of solvent effects on reaction kinetics is also considered as an integral part.[1]",Chemical reaction engineering,https://en.wikipedia.org/wiki/Chemical_reaction_engineering
,,
"Pulsed power is the science and technology of accumulating energy over a relatively long period of time and releasing it very quickly, thus increasing the instantaneous power.",Pulsed power,https://en.wikipedia.org/wiki/Pulsed_power
,,
"A programming language is a formal language, which comprises a set of instructions used to produce various kinds of output. Programming languages are used to create programs that implement specific algorithms.Most programming languages consist of instructions for computers, although there are programmable machines that use a limited set of specific instructions, rather than the general programming languages of modern computers. Early ones preceded the invention of the digital computer, the first probably being the automatic flute player described in the 9th century by the brothers Musa in Baghdad, during the Islamic Golden Age.[1] From the early 1800s, programs were used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos.[2] However, their programs (such as a player piano's scrolls) could not produce different behavior in response to some input or condition.Thousands of different programming languages have been created, mainly in the computer field, and many more still are being created every year. Many programming languages require computation to be specified in an imperative form (i.e., as a sequence of operations to perform) while other languages use other forms of program specification such as the declarative form (i.e. the desired result is specified, not how to achieve it).The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.",Programming language,https://en.wikipedia.org/wiki/Programming_language
,,
This belongs to Disambiguation pages. Open it in your browser,Production,https://en.wikipedia.org/wiki/Production
,,
This belongs to Disambiguation pages. Open it in your browser,Plasma,https://en.wikipedia.org/wiki/Plasma
,,
This belongs to Disambiguation pages. Open it in your browser,Pavement,https://en.wikipedia.org/wiki/Pavement
,,
"In integrated circuit design, physical design is a step in the standard design cycle which follows after the circuit design. At this step, circuit representations of the components (devices and interconnects) of the design are converted into geometric representations of shapes which, when manufactured in the corresponding layers of materials, will ensure the required functioning of the components. This geometric representation is called integrated circuit layout. This step is usually split into several sub-steps, which include both design and verification and validation of the layout.[1][2]Modern day Integrated Circuit (IC) design is split up into Front-end design using HDLs, Verification, and Back-end Design or Physical Design. The next step after Physical Design is the Manufacturing process or Fabrication Process that is done in the Wafer Fabrication Houses. Fab-houses fabricate designs onto silicon dies which are then packaged into ICs.Each of the phases mentioned above has design flows associated with them. These design flows lay down the process and guide-lines/framework for that phase. Physical design flow uses the technology libraries that are provided by the fabrication houses. These technology files provide information regarding the type of silicon wafer used, the standard-cells used, the layout rules (like DRC in VLSI), etc.",Physical design (electronics),https://en.wikipedia.org/wiki/Physical_design_(electronics)
,,
"Pattern recognition is the automated recognition of patterns and regularities in data. Pattern recognition is closely related to artificial intelligence and machine learning,[1] together with applications such as data mining and knowledge discovery in databases (KDD), and is often used interchangeably with these terms. However, these are distinguished: machine learning is one approach to pattern recognition, while other approaches include hand-crafted (not learned) rules or heuristics; and pattern recognition is one approach to artificial intelligence, while other approaches include symbolic artificial intelligence.[2] A modern definition of pattern recognition is:",Pattern recognition,https://en.wikipedia.org/wiki/Pattern_recognition
,,
This belongs to Disambiguation pages. Open it in your browser,Parallel processing,https://en.wikipedia.org/wiki/Parallel_processing
,,
"In mathematics, computer science and operations research, mathematical optimization or mathematical programming, alternatively spelled optimisation, is the selection of a best element (with regard to some criterion) from some set of available alternatives.[1]In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding ""best available"" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.",Mathematical optimization,https://en.wikipedia.org/wiki/Mathematical_optimization
,,
"Nuclear engineering is the branch of engineering concerned with the application of breaking down  atomic nuclei (fission) or of combining atomic nuclei (fusion),  or with the application of other sub-atomic processes based on the principles of nuclear physics. In the sub-field of nuclear fission, it particularly includes the design, interaction, and maintenance of systems and components like nuclear reactors, nuclear power plants, or nuclear weapons. The field also includes the study of medical and other applications of radiation, particularly Ionizing radiation, nuclear safety, heat/thermodynamics transport, nuclear fuel, or other related technology (e.g.,??radioactive waste disposal) and the problems of nuclear proliferation.",Nuclear engineering,https://en.wikipedia.org/wiki/Nuclear_engineering
,,
"Nondestructive testing or non-destructive testing (NDT) is a wide group of analysis techniques used in science and technology industry to evaluate the properties of a material, component or system without causing damage.[1]
The terms nondestructive examination (NDE), nondestructive inspection (NDI), and nondestructive evaluation (NDE) are also commonly used to describe this technology.[2]
Because NDT does not permanently alter the article being inspected, it is a highly valuable technique that can save both money and time in product evaluation, troubleshooting, and research. The six most frequently used NDT methods are eddy-current, magnetic-particle, liquid penetrant, radiographic, ultrasonic, and visual testing.[3] NDT is commonly used in forensic engineering, mechanical engineering, petroleum engineering, electrical engineering, civil engineering, systems engineering, aeronautical engineering, medicine, and art.[1] Innovations in the field of nondestructive testing have had a profound impact on medical imaging, including on echocardiography, medical ultrasonography, and digital radiography. With the advent of machine learning, computer vision and data-driven techniques, nondestructive testing have been improved in different fields. [4] applied data-driven and machine learning techniques to predict damage state based solely on visual observations in infrastructures.Various national and international trade associations exist to promote the industry, knowledge about non-destructive testing, and to develop standard methods and training. These include the American Society for Nondestructive Testing, the Non-Destructive Testing Management Association, the International Committee for Non-Destructive Testing, the European Federation for Non-Destructive Testing and the British Institute of Non-Destructive Testing.NDT methods rely upon use of electromagnetic radiation, sound and other signal conversions to examine a wide variety of articles (metallic and non-metallic, food-product, artifacts and antiquities, infrastructure) for integrity, composition, or condition with no alteration of the article undergoing examination.  Visual inspection (VT), the most commonly applied NDT method, is quite often enhanced by the use of magnification, borescopes, cameras, or other optical arrangements for direct or remote viewing.  The internal structure of a sample can be examined for a volumetric inspection with penetrating radiation (RT), such as X-rays, neutrons or gamma radiation.  Sound waves are utilized in the case of ultrasonic testing (UT), another volumetric NDT method ?€? the mechanical signal (sound) being reflected by conditions in the test article and evaluated for amplitude and distance from the search unit (transducer). Another commonly used NDT method used on ferrous materials involves the application of fine iron particles (either suspended in liquid or dry powder ?€? fluorescent or colored) that are applied to a part while it is magnetized, either continually or residually.  The particles will be attracted to leakage fields of magnetism on or in the test object, and form indications (particle collection) on the object's surface, which are evaluated visually.  Contrast and probability of detection for a visual examination by the unaided eye is often enhanced by using liquids to penetrate the test article surface, allowing for visualization of flaws or other surface conditions.  This method (liquid penetrant testing) (PT) involves using dyes, fluorescent or colored (typically red), suspended in fluids and is used for non-magnetic materials, usually metals.Analyzing and documenting a nondestructive failure mode can also be accomplished using a high-speed camera recording continuously (movie-loop) until the failure is detected. Detecting the failure can be accomplished using a sound detector or stress gauge which produces a signal to trigger the high-speed camera.  These high-speed cameras have advanced recording modes to capture some non-destructive failures.[5]  After the failure the high-speed camera will stop recording.  The capture images can be played back in slow motion showing precisely what happen before, during and after the nondestructive event, image by image.",Nondestructive testing,https://en.wikipedia.org/wiki/Nondestructive_testing
,,
This belongs to Disambiguation pages. Open it in your browser,Neural network (disambiguation),https://en.wikipedia.org/wiki/Neural_network_(disambiguation)
,,
"Optics is the branch of physics which involves the behaviour and properties of light, including its interactions with matter and the construction of instruments that use or detect it.[1] Optics usually describes the behaviour of visible, ultraviolet, and infrared light. Because light is an electromagnetic wave, other forms of electromagnetic radiation such as X-rays, microwaves, and radio waves exhibit similar properties.[1]Most optical phenomena can be accounted for using the classical electromagnetic description of light. Complete electromagnetic descriptions of light are, however, often difficult to apply in practice. Practical optics is usually done using simplified models. The most common of these, geometric optics, treats light as a collection of rays that travel in straight lines and bend when they pass through or reflect from surfaces. Physical optics is a more comprehensive model of light, which includes wave effects such as diffraction and interference that cannot be accounted for in geometric optics. Historically, the ray-based model of light was developed first, followed by the wave model of light. Progress in electromagnetic theory in the 19th century led to the discovery that light waves were in fact electromagnetic radiation.Some phenomena depend on the fact that light has both wave-like and particle-like properties. Explanation of these effects requires quantum mechanics. When considering light's particle-like properties, the light is modelled as a collection of particles called ""photons"". Quantum optics deals with the application of quantum mechanics to optical systems.Optical science is relevant to and studied in many related disciplines including astronomy, various engineering fields, photography, and medicine (particularly ophthalmology and optometry). Practical applications of optics are found in a variety of technologies and everyday objects, including mirrors, lenses, telescopes, microscopes, lasers, and fibre optics.",Optics,https://en.wikipedia.org/wiki/Optics
,,
"Nanomedicine is the medical application of nanotechnology.[1] Nanomedicine ranges from the medical applications of nanomaterials and biological devices, to nanoelectronic biosensors, and even possible future applications of molecular nanotechnology such as biological machines. Current problems for nanomedicine involve understanding the issues related to toxicity and environmental impact of nanoscale materials (materials whose structure is on the scale of nanometers, i.e. billionths of a meter).",Nanomedicine,https://en.wikipedia.org/wiki/Nanomedicine
,,
"Nanotechnology (""nanotech"") is  manipulation of matter on an atomic, molecular, and supramolecular scale. The earliest, widespread description of nanotechnology[1][2]  referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology. A more generalized description of nanotechnology was subsequently established by the National Nanotechnology Initiative, which defines nanotechnology as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers. This definition reflects the fact that quantum mechanical effects are important at this quantum-realm scale, and so the definition shifted from a particular technological goal to a research category inclusive of all types of research and technologies that deal with the special properties of matter which occur below the given size threshold. It is therefore common to see the plural form ""nanotechnologies"" as well as ""nanoscale technologies"" to refer to the broad range of research and applications whose common trait is size. Because of the variety of potential applications (including industrial and military), governments have invested billions of dollars in nanotechnology research. Through 2012, the USA has invested $3.7 billion using its National Nanotechnology Initiative, the European Union has invested $1.2 billion, and Japan has invested $750 million.[3]Nanotechnology as defined by size is naturally very broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage,[4][5] microfabrication,[6] molecular engineering, etc.[7]  The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly,[8] from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.Scientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications, such as in nanomedicine, nanoelectronics, biomaterials energy production, and consumer products. On the other hand, nanotechnology raises many of the same issues as any new technology, including concerns about the toxicity and environmental impact of nanomaterials,[9] and their potential effects on global economics, as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.",Nanotechnology,https://en.wikipedia.org/wiki/Nanotechnology
,,
"In its most basic sense, multimodality is a theory of communication and social semiotics. Multimodality describes communication practices in terms of the textual, aural, linguistic, spatial, and visual resources - or modes - used to compose messages.[1]  Where media are concerned, multimodality is the use of several modes (media) to create a single artifact. The collection of these modes, or elements, contributes to how multimodality affects different rhetorical situations, or opportunities for increasing an audience's reception of an idea or concept.  Everything from the placement of images to the organization of the content creates meaning. This is the result of a shift from isolated text being relied on as the primary source of communication, to the image being utilized more frequently in the digital age.[2]  While multimodality as an area of academic study did not gain traction until the twentieth century, all communication, literacy, and composing practices are and always have been multimodal.[3]",Multimodality,https://en.wikipedia.org/wiki/Multimodality
,,
"Nanomanufacturing is both the production of nanoscaled materials, which can be powders or fluids, and the manufacturing of parts ""bottom up"" from nanoscaled materials or ""top down"" in smallest steps for high precision, used in several technologies such as laser ablation, etching and others. Nanomanufacturing differs from molecular manufacturing, which is the manufacture of complex, nanoscale structures by means of nonbiological mechanosynthesis (and subsequent assembly).[1]The term ""nanomanufacturing"" is widely used, e.g. by the European Technology Platform MINAM[2] and the U.S. National Nanotechnology Initiative (NNI).[3] The NNI refers to the sub-domain of nanotechnology as one of its five ""priority areas.""[4] There is also a nanomanufacturing program at the U.S. National Science Foundation, through which the National Nanomanufacturing Network (NNN) has been established. The NNN is an organization that works to expedite the transition of nanotechnologies from laboratory research to production manufacturing and it does so through information exchange,[5] strategic workshops, and roadmap development.The NNI has defined nanotechnology very broadly,[6] to include a wide range of tiny structures, including those created by large and imprecise tools. However, nanomanufacturing is not defined in the NNI's recent report, Instrumentation and Metrology for Nanotechnology. In contrast, another ""priority area,"" nanofabrication, is defined as ""the ability to fabricate, by directed or self-assembly methods, functional structures or devices at the atomic or molecular level"" (p.??67). Nanomanufacturing appears to be the near-term, industrial-scale manufacture of nanotechnology-based objects, with emphasis on low cost and reliability. Many professional societies have formed Nanotechnology technical groups. The Society of Manufacturing Engineers, for example, has formed a Nanomanufacturing Technical Group to both inform members of the developing technologies and to address the organizational and IP (intellectual property) legal issues that must be addressed for broader commercialization.In 2014 the Government Accountability Office noted that America's leadership in nanotechnology was put at risk by a failure of the government to invest in preparing basic research for commercial application.[7]",Nanomanufacturing,https://en.wikipedia.org/wiki/Nanomanufacturing
,,
"Nanomaterials describe, in principle, materials of which a single unit is sized (in at least one dimension) between 1 to 1000 nanometres (10???9 meter) but usually is 1 to 100??nm  (the usual definition of nanoscale[1]).Nanomaterials research takes a materials science-based approach to nanotechnology, leveraging advances in materials metrology and synthesis which have been developed in support of microfabrication research.  Materials with structure at the nanoscale often have unique optical, electronic, or mechanical properties.[2]Nanomaterials are slowly becoming commercialized[3] and beginning to emerge as commodities.[4]",Nanomaterials,https://en.wikipedia.org/wiki/Nanomaterials
,,
"Multimedia  is content that uses a combination of different content forms such as text, audio, images, animations, video and interactive content. Multimedia contrasts with media that use only rudimentary computer displays such as text-only or traditional forms of printed or hand-produced material.Multimedia can be recorded and played, displayed, interacted with or accessed by information content processing devices, such as computerized and electronic devices, but can also be part of a live performance.  Multimedia devices are electronic media devices used to store and experience multimedia content. Multimedia is distinguished from mixed media in fine art; for example, by including audio it has a broader scope. In the early years of multimedia the term ""rich media"" was synonymous with interactive multimedia, and ""hypermedia"" was an application of multimedia.",Multimedia,https://en.wikipedia.org/wiki/Multimedia
,,
"In engineering, mathematics, physics, chemistry, bioinformatics, computational biology, meteorology and computer science, multiscale modeling  or multiscale mathematics is the field of solving problems which have important features at multiple scales of time and/or space. Important problems include multiscale modeling of fluids,[1][2] solids,[2][3] polymers,[4][5] proteins,[6][7][8][9] nucleic acids[10] as well as various physical and chemical phenomena (like adsorption, chemical reactions, diffusion).[8][11][12]",Multiscale modeling,https://en.wikipedia.org/wiki/Multiscale_modeling
,,
"In the petroleum industry, a well test is the execution of a set of planned data acquisition activities. The acquired data is analyzed to broaden the knowledge and increase the understanding of the hydrocarbon properties therein and characteristics of the underground reservoir where the hydrocarbons are trapped.The test will also provide information about the state of the particular well used to collect data. The overall objective is identifying the reservoir's capacity to produce hydrocarbons, such as oil, natural gas and condensate.Data gathered during the test period includes volumetric flow rate and pressure observed in the selected well. Outcomes of a well test, for instance flow rate data and gas oil ratio data, may support the well allocation process for an ongoing production phase, while other data about the reservoir capabilities will support reservoir management.",Well test (oil and gas),https://en.wikipedia.org/wiki/Well_test_(oil_and_gas)
,,
This List of fluid mechanics journals consists of some of the leading scientific journals related to the field of fluid mechanics.,List of fluid mechanics journals,https://en.wikipedia.org/wiki/List_of_fluid_mechanics_journals
,,
"Micro-Opto-Electro-Mechanical Systems (MOEMS) are not a special class of Micro-Electro-Mechanical Systems (MEMS) but rather the combination of  MEMS merged with Micro-optics; this involves sensing or manipulating optical signals on a very small size scale using integrated mechanical, optical, and electrical systems. MOEMS includes a wide variety of devices including optical switch, optical cross-connect, tunable VCSEL, microbolometers amongst others. These devices are usually fabricated using micro-optics and standard micromachining technologies using materials like silicon, silicon dioxide, silicon nitride and gallium arsenide.",Micro-Opto-Electro-Mechanical Systems,https://en.wikipedia.org/wiki/Micro-Opto-Electro-Mechanical_Systems
,,
"Mechanical engineering is the discipline that applies engineering, physics, engineering mathematics, and materials science principles to design, analyze, manufacture, and maintain mechanical systems.  It is one of the oldest and broadest of the engineering disciplines.The mechanical engineering field requires an understanding of core areas including mechanics, dynamics, thermodynamics, materials science, structural analysis, and electricity.  In addition to these core principles, mechanical engineers use tools such as computer-aided design (CAD), computer-aided manufacturing (CAM), and product life cycle management to design and analyze manufacturing plants, industrial equipment and machinery, heating and cooling systems, transport systems, aircraft, watercraft, robotics, medical devices, weapons, and others.  It is the branch of engineering that involves the design, production, and operation of machinery.[1][2]Mechanical engineering emerged as a field during the Industrial Revolution in Europe in the 18th century; however, its development can be traced back several thousand years around the world.  In the 19th century, developments in physics led to the development of mechanical engineering science. The field has continually evolved to incorporate advancements; today mechanical engineers are pursuing developments in such areas as composites, mechatronics, and nanotechnology.  It also overlaps with aerospace engineering, metallurgical engineering, civil engineering, electrical engineering, manufacturing engineering, chemical engineering, industrial engineering, and other engineering disciplines to varying amounts.  Mechanical engineers may also work in the field of biomedical engineering, specifically with biomechanics, transport phenomena, biomechatronics, bionanotechnology, and modeling of biological systems.",Mechanical engineering,https://en.wikipedia.org/wiki/Mechanical_engineering
,,
"Aerospace (or aeronautical) engineering can be studied at the bachelors, masters and Ph.D. levels in aerospace engineering departments at many universities, and in mechanical engineering departments at others.Institution names are followed by accreditation where applicable.  The Accreditation Board for Engineering and Technology (ABET) accredits 2 institutions in Turkey while the others are in United States of America.",List of aerospace engineering schools,https://en.wikipedia.org/wiki/List_of_aerospace_engineering_schools
,,
"Operations research, or operational research in British usage,  is a discipline that deals with the application of advanced analytical methods to help make better decisions.[1] Further, the term 'operational analysis' is used in the British (and some British Commonwealth) military as an intrinsic part of capability development, management and assurance.  In particular, operational analysis forms part of the Combined Operational Effectiveness and Investment Appraisals (COEIA), which support British defense capability acquisition decision-making.It is often considered to be a sub-field of applied mathematics.[2]  The terms management science and decision science are sometimes used as synonyms.[3]Employing techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. Operations research is often concerned with determining the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost) of some real-world objective. Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.[4]",Operations research,https://en.wikipedia.org/wiki/Operations_research
,,
"Mass transfer is the net movement of mass from one location, usually meaning  stream, phase, fraction or component, to another. Mass transfer occurs in many processes, such as absorption, evaporation, drying, precipitation, membrane filtration, and distillation. Mass transfer is used by different scientific disciplines for different processes and mechanisms. The phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems.Some common examples of mass transfer processes are the evaporation of water from a pond to the atmosphere, the purification of blood in the kidneys and liver, and the distillation of alcohol. In industrial processes, mass transfer operations include separation of chemical components in distillation columns, absorbers such as scrubbers or stripping, adsorbers such as activated carbon beds, and liquid-liquid extraction. Mass transfer is often coupled to additional transport processes, for instance in industrial cooling towers. These towers couple heat transfer to mass transfer by allowing hot water to flow in contact with air. The water is cooled by expelling some of its content in the form of water vapour.",Mass transfer,https://en.wikipedia.org/wiki/Mass_transfer
,,
This page about an IT-related or software-related company or corporation is a stub.,Material,https://en.wikipedia.org/wiki/Material
,,
"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to ""learn"" (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed.[2]The name machine learning was coined in 1959 by Arthur Samuel.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data[3] ?€? such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions,[4]:2 through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders, and computer vision.Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining,[5] where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.[6][7]Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to ""produce reliable, repeatable decisions and results"" and uncover ""hidden insights"" through learning from historical relationships and trends in the data.[8]",Machine learning,https://en.wikipedia.org/wiki/Machine_learning
,,
"Manufacturing is the production of merchandise for use or sale using labor and machines, tools, chemical and biological processing, or formulation. The term may refer to a range of human activity, from handicraft to high tech, but is most commonly applied to industrial production, in which raw materials are transformed into finished goods on a large scale. Such finished goods may be sold to other manufacturers for the production of other, more complex products, such as aircraft, household appliances, furniture, sports equipment or automobiles, or sold to wholesalers, who in turn sell them to retailers, who then sell them to end users and consumers.Manufacturing engineering or manufacturing process are the steps through which raw materials are transformed into a final product. The manufacturing process begins with the product design, and materials specification from which the product is made. These materials are then modified through manufacturing processes to become the required part.Modern manufacturing includes all intermediate processes required in the production and integration of a product's components. Some industries, such as semiconductor and steel manufacturers use the term fabrication instead.The manufacturing sector is closely connected with engineering and industrial design. Examples of major manufacturers in North America include General Motors Corporation, General Electric, Procter & Gamble, General Dynamics, Boeing, Pfizer, and Precision Castparts. Examples in Europe include Volkswagen Group, Siemens, FCA  and Michelin. Examples in Asia include Toyota, Yamaha, Panasonic, LG, Samsung and Tata Motors.",Manufacturing,https://en.wikipedia.org/wiki/Manufacturing
,,
"Logistics is generally the detailed organization and implementation of a complex operation. In a general business sense, logistics is the management of the flow of things between the point of origin and the point of consumption in order to meet requirements of customers or corporations. The resources managed in logistics can include physical items such as food, materials, animals, equipment, and liquids; as well as intangible items, such as time and information. The logistics of physical items usually involves the integration of information flow, materials handling, production, packaging, inventory, transportation, warehousing, and often security.In military science, logistics is concerned with maintaining army supply lines while disrupting those of the enemy, since an armed force without resources and transportation is defenseless. Military logistics was already practiced in the ancient world and as modern military have a significant need for logistics solutions, advanced implementations have been developed. In military logistics, logistics officers manage how and when to move resources to the places they are needed.Logistics management is the part of supply chain management that plans, implements, and controls the efficient, effective forward, and reverse flow and storage of goods, services, and related information between the point of origin and the point of consumption in order to meet customer's requirements. The complexity of logistics can be modeled, analyzed, visualized, and optimized by dedicated simulation software. The minimization of the use of resources is a common motivation in all logistics fields. A professional working in the field of logistics management is called a logistician.",Logistics,https://en.wikipedia.org/wiki/Logistics
,,
"Lean manufacturing or lean production, often simply ""lean"", is a systematic method for waste minimization (""Muda"") within a manufacturing system without sacrificing productivity.  Lean also takes into account waste created through overburden (""Muri"") and waste created through unevenness in work loads (""Mura""). Working from the perspective of the client who consumes a product or service, ""value"" is any action or process that a customer would be willing to pay for.Lean manufacturing makes obvious what adds value, by reducing everything else (which is not adding value). This management philosophy is derived mostly from the Toyota Production System (TPS) and identified as ""lean"" only in the 1990s.[1][page??needed], [2] TPS is renowned for its focus on reduction of the original Toyota seven wastes to improve overall customer value, but there are varying perspectives on how this is best achieved. The steady growth of Toyota, from a small company to the world's largest automaker,[3] has focused attention on how it has achieved this success.",Lean manufacturing,https://en.wikipedia.org/wiki/Lean_manufacturing
,,
"A laser is a device that emits light through a process of optical amplification based on the stimulated emission of electromagnetic radiation. The term ""laser"" originated as an acronym for ""light amplification by stimulated emission of radiation"".[1][2]  The first laser was built in 1960 by Theodore H. Maiman at Hughes Research Laboratories, based on theoretical work by Charles Hard Townes and Arthur Leonard Schawlow.A laser differs from other sources of light in that it emits light coherently, spatially and temporally. Spatial coherence allows a laser to be focused to a tight spot, enabling applications such as laser cutting and lithography. Spatial coherence also allows a laser beam to stay narrow over great distances (collimation), enabling applications such as laser pointers. Lasers can also have high temporal coherence, which allows them to emit light with a very narrow spectrum, i.e., they can emit a single color of light. Temporal coherence can be used to produce pulses of light as short as a femtosecond.Among their many applications, lasers are used in optical disk drives, laser printers, and barcode scanners; DNA sequencing instruments, fiber-optic and free-space optical communication; laser surgery and skin treatments; cutting and welding materials; military and law enforcement devices for marking targets and measuring range and speed; and laser lighting displays in entertainment.",Laser,https://en.wikipedia.org/wiki/Laser
,,
"Law is a system of rules that are created and enforced through social or governmental institutions to regulate behavior.[2] Law is a system that regulates and ensures that individuals or a community adhere to the will of the state. State-enforced laws can be made by a collective legislature or by a single legislator, resulting in statutes, by the executive through decrees and regulations, or established by judges through precedent, normally in common law jurisdictions. Private individuals can create legally binding contracts, including arbitration agreements that may elect to accept alternative arbitration to the normal court process. The formation of laws themselves may be influenced by a constitution, written or tacit, and the rights encoded therein. The law shapes politics, economics, history and society in various ways and serves as a mediator of relations between people.A general distinction can be made between (a) civil law jurisdictions, in which a legislature or other central body codifies and consolidates their laws, and (b) common law systems, where judge-made precedent is accepted as binding law. Historically, religious laws played a significant role even in settling of secular matters, and is still used in some religious communities. Islamic Sharia law is the world's most widely used religious law, and is used as the primary legal system in some countries, such as Iran and Saudi Arabia.[3]The adjudication of the law is generally divided into two main areas. Criminal law deals with conduct that is considered harmful to social order and in which the guilty party may be imprisoned or fined. Civil law (not to be confused with civil law jurisdictions above) deals with the resolution of lawsuits (disputes) between individuals or organizations.[4]Law provides a source of scholarly inquiry into legal history, philosophy, economic analysis and sociology. Law also raises important and complex issues concerning equality, fairness, and justice.",Law,https://en.wikipedia.org/wiki/Law
,,
"The word Infrastructure refers to the fundamental facilities and systems serving a country, city, or other area,[1] including the services and facilities necessary for its economy to function.[2] Infrastructure is composed of public and private physical improvements such as roads, bridges, tunnels, water supply, sewers, electrical grids, telecommunications (including Internet connectivity and broadband speeds). In general, it has also been defined as ""the physical components of interrelated systems providing commodities and services essential to enable, sustain, or enhance societal living conditions.""[3]",Infrastructure,https://en.wikipedia.org/wiki/Infrastructure
,,
"Information technology (IT) is the use of computers to store, retrieve, transmit, and manipulate data,[1] or information, often in the context of a business or other enterprise.[2] IT is considered to be a subset of information and communications technology (ICT).Humans have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000??BC,[3] but the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that ""the new technology does not yet have a single established name. We shall call it information technology (IT)."" Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.[4]The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce.[5][a]Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000??BC???€? 1450??AD), mechanical (1450?€?1840), electromechanical (1840?€?1940), and electronic (1940?€?present).[3] This article focuses on the most recent period (electronic), which began in about 1940.",Information technology,https://en.wikipedia.org/wiki/Information_technology
,,
This belongs to Disambiguation pages. Open it in your browser,LFRD,https://en.wikipedia.org/wiki/LFRD
,,
This belongs to Disambiguation pages. Open it in your browser,ITS,https://en.wikipedia.org/wiki/ITS
,,
"Immunology is a branch of biology[1] that covers the study of immune systems[2] in all organisms.[3] Immunology charts, measures, and contextualizes the: physiological functioning of the immune system in states of both health and diseases; malfunctions of the immune system in immunological disorders (such as autoimmune diseases,[4] hypersensitivities[5] immune deficiency[6], and transplant rejection[7]); the physical, chemical and physiological characteristics of the components of the immune system in vitro[8], in situ, and in vivo[9]. Immunology has applications in numerous disciplines of medicine, particularly in the fields of organ transplantation, oncology, rheumatology, virology, bacteriology, parasitology, psychiatry, and dermatology.The term was coined by Russian biologist Ilya Ilyich Mechnikov[10] who advanced studies on immunology and received the Nobel Prize for his work in 1908. He pinned small thorns into starfish larvae and noticed unusual cells surrounding the thorns. This was the active response of the body trying to maintain its integrity. It was Mechnikov who first observed the phenomenon of phagocytosis,[11] in which the body defends itself against a foreign body.Prior to the designation of immunity,[12] from the etymological root immunis, which is Latin for ""exempt""; early physicians characterized organs that would later be proven as essential components of the immune system. The important lymphoid organs of the immune system are the thymus[13] and bone marrow,  and chief lymphatic tissues such as spleen, tonsils, lymph vessels, lymph nodes, adenoids, and liver. When health conditions worsen to emergency status, portions of immune system organs including the thymus, spleen, bone marrow, lymph nodes and other lymphatic tissues can be surgically excised for examination while patients are still alive.Many components of the immune system are typically cellular in nature and not associated with any specific organ; but rather are embedded or circulating in various tissues located throughout the body.",Immunology,https://en.wikipedia.org/wiki/Immunology
,,
"An information system (IS) is an organized system for the collection, organization, storage and communication of information.
More specifically, it is the study of complementary networks that people and organizations use to collect, filter, process, create and distribute data. Further, ""[a]n information system (IS) is a group of components that interact to produce information.  It focuses on the internal rather than the external."" Information system can also be described as a combination of hardware, software, data, business process and functions which can be used to increase efficiency and management of an organization. Information Systems is the expression used to describe an Automated System (which may be referred to as a Computerized Information System), be it manual, which covers people, machines or organized methods to collect, process, transmit and disseminate data representing information for the user or client.[1]A computer information system is a system that a branch of Science composed of people and computers that processes or interprets information.[2][3][4][5]
The term is also sometimes used in more restricted senses to refer to only the software used to run a computerized database or to refer to only a computer system.Information Systems is an academic study of systems with a specific reference to information and the complementary networks of hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks.[6]Any specific information system aims to support operations, management and decision-making.[7][8] An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes.[9]Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes.[10]Alter[11][12] argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information.[13]As such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action.Information systems are the primary focus of study for organizational informatics.[14]",Information system,https://en.wikipedia.org/wiki/Information_system
,,
"Industrial engineering  is a branch of engineering which deals with the optimization of complex processes, systems, or organizations. Industrial engineers work to eliminate waste of time, money, materials, person-hours, machine time, energy and other resources that do not generate value. According to the Institute of Industrial and Systems Engineers, they create engineering processes and systems that improve quality and productivity.[1]Industrial engineering is concerned with the development, improvement, and implementation of integrated systems of people, money, knowledge, information, equipment, energy, materials, analysis and synthesis, as well as the mathematical, physical and social sciences together with the principles and methods of engineering design to specify, predict, and evaluate the results to be obtained from such systems or processes.[2] While industrial engineering is a longstanding engineering discipline subject to (and eligible for) professional engineering licensure in most jurisdictions, its underlying concepts overlap considerably with certain business-oriented disciplines such as operations management.Depending on the sub-specialties involved, industrial engineering may also be known as, or overlap with, operations research, systems engineering, manufacturing engineering, production engineering, management science, management engineering, financial engineering, ergonomics or human factors engineering, safety engineering, or others, depending on the viewpoint or motives of the user.",Industrial engineering,https://en.wikipedia.org/wiki/Industrial_engineering
,,
"In computer science, digital image processing is the use of computer algorithms to perform image processing on digital images.[1] As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and signal distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems.",Digital image processing,https://en.wikipedia.org/wiki/Digital_image_processing
,,
"Image analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques.[1] Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.Computers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information.  On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications ?€? including medicine, security, and remote sensing ?€? human analysts still cannot be replaced by computers.  For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.",Image analysis,https://en.wikipedia.org/wiki/Image_analysis
,,
This belongs to Disambiguation pages. Open it in your browser,Imaging,https://en.wikipedia.org/wiki/Imaging
,,
"Hydrogen production is the family of industrial methods for generating hydrogen.  Currently the dominant technology for direct production is steam reforming from hydrocarbons. Many other methods are known including electrolysis and thermolysis.In 2006, the United States was estimated to have a production capacity of 11 million tons of hydrogen. 5 million tons of hydrogen were consumed on-site in oil refining, and in the production of ammonia (Haber process) and methanol (reduction of carbon monoxide). 0.4 million tons were an incidental by-product of the chlor-alkali process.[1]  Hydrogen production is an estimated $100 billion industry.[2] According to the U.S. Department of Energy, 53 million metric tons were consumed worldwide in 2004. There are no natural hydrogen deposits, and for this reason the production of hydrogen plays a key role in modern society.[3]As of 1999, the majority of hydrogen (???95%) is produced from fossil fuels by steam reforming or partial oxidation of methane and coal gasification with only a small quantity by other routes such as biomass gasification or electrolysis of water.[4] Around 8GW of electrolysis capacity is installed 
worldwide, accounting for around 4% of global hydrogen production (Decourt et al., 2014). Developing affordable methods for producing hydrogen with less damage to the environment is a goal of the hydrogen economy.",Hydrogen production,https://en.wikipedia.org/wiki/Hydrogen_production
,,
"In physics and engineering, fluid dynamics  is a subdiscipline of fluid mechanics that describes the flow of fluids - liquids and gases.  It has several subdisciplines, including aerodynamics (the study of air and other gases in motion) and hydrodynamics (the study of liquids in motion).  Fluid dynamics has a wide range of applications, including calculating forces and moments on aircraft, determining the mass flow rate of petroleum through pipelines, predicting weather patterns, understanding nebulae in interstellar space and modelling fission weapon detonation,Fluid dynamics offers a systematic structure?€?which underlies these practical disciplines?€?that embraces empirical and semi-empirical laws derived from flow measurement and used to solve practical problems. The solution to a fluid dynamics problem typically involves the calculation of various properties of the fluid, such as flow velocity, pressure, density, and temperature, as functions of space and time.Before the twentieth century, hydrodynamics was synonymous with fluid dynamics.  This is still reflected in names of some fluid dynamics topics, like magnetohydrodynamics and hydrodynamic stability, both of which can also be applied to gases.[1]",Fluid dynamics,https://en.wikipedia.org/wiki/Fluid_dynamics
,,
"Hydraulics (from Greek: ??????????????????) is a technology and applied science using engineering, chemistry, and other sciences involving the mechanical properties and use of liquids. At a very basic level, hydraulics is the liquid counterpart of pneumatics, which concerns gases. Fluid mechanics provides the theoretical foundation for hydraulics, which focuses on the applied engineering using the properties of fluids. In its fluid power applications, hydraulics is used for the generation, control, and transmission of power by the use of pressurized liquids. Hydraulic topics range through some parts of science and most of engineering modules, and cover concepts such as pipe flow, dam design, fluidics and fluid control circuitry. The principles of hydraulics are in use naturally in the human body within the vascular system and erectile tissue.[3][4]
Free surface hydraulics is the branch of hydraulics dealing with free surface flow, such as occurring in rivers, canals, lakes, estuaries and seas. Its sub-field open-channel flow studies the flow in open channels.The word ""hydraulics"" originates from the Greek word ????????????????????? (hydraulikos) which in turn originates from ????????? (hydor, Greek for water) and ??????????? (aulos, meaning pipe).",Hydraulics,https://en.wikipedia.org/wiki/Hydraulics
,,
"Human factors and ergonomics (commonly referred to as human factors) is the application of psychological and physiological principles to the (engineering and) design of products, processes, and systems. The goal of human factors is to reduce human error, increase productivity, and enhance safety and comfort with a specific focus on the interaction between the human and the thing of interest.[1]The field is a combination of numerous disciplines, such as psychology, sociology, engineering, biomechanics, industrial design, physiology, anthropometry, interaction design, visual design, user experience, and user interface design. In research, human factors employs the scientific method to study human behavior so that the resultant data may be applied to the four primary goals. In essence, it is the study of designing equipment, devices and processes that fit the human body and its cognitive abilities. The two terms ""human factors"" and ""ergonomics"" are essentially synonymous.[2][3][4]The International Ergonomics Association defines ergonomics or human factors as follows:[5]",Human factors and ergonomics,https://en.wikipedia.org/wiki/Human_factors_and_ergonomics
,,
"A health system, also sometimes referred to as health care system or as healthcare system, is the organization of people, institutions, and resources that deliver health care services to meet the health needs of target populations.There is a wide variety of health systems around the world, with as many histories and organizational structures as there are nations. Implicitly, nations must design and develop health systems in accordance with their needs and resources, although common elements in virtually all health systems are primary healthcare and public health measures.[1] In some countries, health system planning is distributed among market participants. In others, there is a concerted effort among governments, trade unions, charities, religious organizations, or other co-ordinated bodies to deliver planned health care services targeted to the populations they serve. However, health care planning has been described as often evolutionary rather than revolutionary.[2][3]",Health system,https://en.wikipedia.org/wiki/Health_system
,,
"Heat transfer is a discipline of thermal engineering that concerns the generation, use, conversion, and exchange of thermal energy (heat) between physical systems. Heat transfer is classified into various mechanisms, such as thermal conduction, thermal convection, thermal radiation, and transfer of energy by phase changes. Engineers also consider the transfer of mass of differing chemical species, either cold or hot, to achieve heat transfer. While these mechanisms have distinct characteristics, they often occur simultaneously in the same system.Heat conduction, also called diffusion, is the direct microscopic exchange of kinetic energy of particles through the boundary between two systems. When an object is at a different temperature from another body or its surroundings, heat flows so that the body and the surroundings reach the same temperature, at which point they are in thermal equilibrium. Such spontaneous heat transfer always occurs from a region of high temperature to another region of lower temperature, as described in the second law of thermodynamics.Heat convection occurs when bulk flow of a fluid (gas or liquid) carries heat along with the flow of matter in the fluid. The flow of fluid may be forced by external processes, or sometimes (in gravitational fields) by buoyancy forces caused when thermal energy expands the fluid (for example in a fire plume), thus influencing its own transfer. The latter process is often called ""natural convection"". All convective processes also move heat partly by diffusion, as well. Another form of convection is forced convection. In this case the fluid is forced to flow by use of a pump, fan or other mechanical means.Thermal radiation occurs through a vacuum or any transparent medium (solid or fluid or gas). It is the transfer of energy by means of photons in electromagnetic waves governed by the same laws.[1]",Heat transfer,https://en.wikipedia.org/wiki/Heat_transfer
,,
This belongs to Disambiguation pages. Open it in your browser,Vine (disambiguation),https://en.wikipedia.org/wiki/Vine_(disambiguation)
,,
"Health care or healthcare is the maintenance or improvement of health via the prevention, diagnosis, and  treatment of disease, illness, injury, and other physical and mental impairments in human beings. Healthcare is delivered by health professionals (providers or practitioners) in allied health fields. Physicians and physician associates are a part of these health professionals. Dentistry, midwifery, nursing, medicine, optometry, audiology, pharmacy, psychology, occupational therapy, physical therapy and other health professions are all part of healthcare. It includes work done in providing primary care, secondary care, and tertiary care, as well as in public health.Access to health care may vary across countries, communities, and individuals, largely influenced by social and economic conditions as well as the health policies in place. Countries and jurisdictions have different policies and plans in relation to the personal and population-based health care goals within their societies. Healthcare systems are organizations established to meet the health needs of targeted populations. Their exact configuration varies between national and subnational entities. In some countries and jurisdictions, health care planning is distributed among market participants, whereas in others, planning occurs more centrally among governments or other coordinating bodies. In all cases, according to the World Health Organization (WHO), a well-functioning healthcare system requires a robust financing mechanism; a well-trained and adequately paid workforce; reliable information on which to base decisions and policies; and well maintained health facilities and logistics to deliver quality medicines and technologies.[1]Healthcare can contribute to a significant part of a country's economy. In 2011, the healthcare industry consumed an average of 9.3 percent of the GDP or US$ 3,322 (PPP-adjusted) per capita across the 34 members of OECD countries. The US (17.7%, or US$ PPP 8,508), the Netherlands (11.9%, 5,099), France (11.6%, 4,118), Germany (11.3%, 4,495), Canada (11.2%, 5669), and Switzerland (11%, 5,634) were the top spenders, however life expectancy in total population at birth was highest in Switzerland (82.8 years), Japan and Italy (82.7), Spain and Iceland (82.4), France (82.2) and Australia (82.0), while OECD's average exceeds 80 years for the first time ever in 2011: 80.1 years, a gain of 10 years since 1970. The US (78.7 years) ranges only on place 26 among the 34 OECD member countries, but has the highest costs by far. All OECD countries have achieved universal (or almost universal) health coverage, except the US and Mexico.[2][3] (see also international comparisons.)Health care is conventionally regarded as an important determinant in promoting the general physical and mental health and well-being of people around the world. An example of this was the worldwide eradication of smallpox in 1980, declared by the WHO as the first disease in human history to be completely eliminated by deliberate health care interventions.[4]",Health care,https://en.wikipedia.org/wiki/Health_care
,,
"A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics and image processing, and their highly parallel structure makes them more efficient than general-purpose CPUs for algorithms where the processing of large blocks of data is done in parallel. In a personal computer, a GPU can be present on a video card, or it can be embedded on the motherboard or?€?in certain CPUs?€?on the CPU die.[1]The term GPU was popularized by Nvidia in 1999, who marketed the GeForce 256 as ""the world's first GPU"", or Graphics Processing Unit,[2] although the term had been in use since at least the 1980s.[3] It was presented as a ""single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines"".[4] Rival ATI Technologies coined the term ""visual processing unit"" or VPU with the release of the Radeon 9700 in 2002.[5]",Graphics processing unit,https://en.wikipedia.org/wiki/Graphics_processing_unit
,,
"A supercomputer is a computer with a high level of performance compared to a general-purpose computer. Performance of a supercomputer is measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS). As of 2017, there are supercomputers which can perform up to nearly a hundred quadrillion FLOPS.[3] As of November 2017, all of the world's fastest 500 supercomputers run Linux-based operating systems.[4] Additional research is being conducted in China, the United States, the European Union, Taiwan and Japan to build even faster, more powerful and more technologically superior exascale supercomputers.[5]Supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields, including quantum mechanics, weather forecasting, climate research, oil and gas exploration, molecular modeling (computing the structures and properties of chemical compounds, biological macromolecules, polymers, and crystals), and physical simulations (such as simulations of the early moments of the universe, airplane and spacecraft aerodynamics, the detonation of nuclear weapons, and nuclear fusion). Throughout their history, they have been essential in the field of cryptanalysis.[6]Supercomputers were introduced in the 1960s, and for several decades the fastest were made by Seymour Cray at Control Data Corporation (CDC), Cray Research and subsequent companies bearing his name or monogram. The first such machines were highly tuned conventional designs that ran faster than their more general-purpose contemporaries. Through the 1960s, they began to add increasing amounts of parallelism with one to four processors being typical. From the 1970s, the vector computing concept with specialized math units operating on large arrays of data came to dominate. A notable example is the highly successful Cray-1 of 1976. Vector computers remained the dominant design into the 1990s. From then until today, massively parallel supercomputers with tens of thousands of off-the-shelf processors became the norm.[7][8]The US has long been a leader in the supercomputer field, first through Cray's almost uninterrupted dominance of the field, and later through a variety of technology companies. Japan made major strides in the field in the 1980s and 90s, but since then China has become increasingly active in the field. As of June 2018, the fastest supercomputer on the TOP500 supercomputer list is the Summit, in the United States, with a LINPACK benchmark score of 122.3??PFLOPS, exceeding the previous record holder, Sunway TaihuLight, by around 29??PFLOPS.[3][9] Sunway TaihuLight's  is notable for its use of indigenous chips and is the first Chinese computer to enter the TOP500 list without using hardware from the United States.  As of June 2018, China had more computers (206) on the TOP500 list than the United States (124); however, US built computers held eight of the top 20 positions;[10][11] the U.S. has six of the top 10 and China has two.",Supercomputer,https://en.wikipedia.org/wiki/Supercomputer
,,
This belongs to Disambiguation pages. Open it in your browser,Energy efficiency,https://en.wikipedia.org/wiki/Energy_efficiency
,,
"Geotechnical engineering is the branch of civil engineering concerned with the engineering behavior of earth materials. Geotechnical     engineering is important in civil engineering, but also has applications in military, mining, petroleum and other engineering disciplines that are concerned with construction occurring on the surface or within the ground. Geotechnical engineering uses principles of soil mechanics and rock mechanics to investigate subsurface conditions and materials; determine the relevant physical/mechanical and chemical properties of these materials; evaluate stability of natural slopes and man-made soil deposits; assess risks posed by site conditions; design earthworks and structure foundations; and monitor site conditions, earthwork and foundation construction.[1][2]A typical geotechnical engineering project begins with a review of project needs to define the required material properties. Then follows a site investigation of soil, rock, fault distribution and bedrock properties on and below an area of interest to determine their engineering properties including how they will interact with, on or in a proposed construction. Site investigations are needed to gain an understanding of the area in or on which the engineering will take place. Investigations can include the assessment of the risk to humans, property and the environment from natural hazards such as earthquakes, landslides, sinkholes, soil liquefaction, debris flows and rockfalls.A geotechnical engineer then determines and designs the type of foundations, earthworks, and/or pavement subgrades required for the intended man-made structures to be built. Foundations are designed and constructed for structures of various sizes such as high-rise buildings, bridges, medium to large commercial buildings, and smaller structures where the soil conditions do not allow code-based design.Foundations built for above-ground structures include shallow and deep foundations. Retaining structures include earth-filled dams and retaining walls. Earthworks include embankments, tunnels, dikes and levees, channels, reservoirs, deposition of hazardous waste and sanitary landfills.Geotechnical engineering is also related to coastal and ocean engineering. Coastal engineering can involve the design and construction of wharves, marinas, and jetties. Ocean engineering can involve foundation and anchor systems for offshore structures such as oil platforms.The fields of geotechnical engineering and engineering geology are closely related, and have large areas of overlap.  However, the field of geotechnical engineering is a specialty of engineering, where the field of engineering geology is a specialty of geology.",Geotechnical engineering,https://en.wikipedia.org/wiki/Geotechnical_engineering
,,
"Electronics comprises the physics, engineering, technology and applications that deal with the emission, flow and control of electrons in vacuum and matter.[1]  The identification of the electron in 1897, along with the invention of vacuum tube, which could amplify and rectify small electrical signals,  inaugurated the field of electronics and the electron age.[2]Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, integrated circuits, optoelectronics, and sensors, associated passive electrical components, and interconnection technologies. Commonly, electronic devices contain circuitry consisting primarily or exclusively of active semiconductors supplemented with passive elements; such a circuit is described as an electronic circuit.The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible.  Electronics is widely used in information processing, telecommunication, and signal processing.  The ability of electronic devices to act as switches makes digital information-processing possible.  Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.Electrical and  electromechanical science and technology deals with the generation, distribution, switching, storage, and conversion of electrical energy to and from other energy forms (using wires,  motors,  generators, batteries, switches, relays, transformers, resistors, and other passive components). This distinction started around 1906 with the invention by Lee De Forest of the triode, which made electrical  amplification of weak radio signals and audio signals possible with a non-mechanical device.  Until 1950 this field was called ""radio technology"" because its principal application was the design and theory of radio transmitters,  receivers, and vacuum tubes.As of  2018[update] most electronic devices use semiconductor components to perform electron control.  The study of semiconductor devices and related technology is considered a branch of solid-state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.  This article focuses on engineering aspects of electronics.",Electronics,https://en.wikipedia.org/wiki/Electronics
,,
"In physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object.[note 1]  Energy is a conserved quantity; the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The SI unit of energy is the joule, which is the energy transferred to an object by the work of moving it a distance of 1 metre against a force of 1 newton.Common forms of energy include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature.Mass and energy are closely related. Due to mass?€?energy equivalence, any object that has mass when stationary (called rest mass) also has an equivalent amount of energy whose form is called rest energy, and any additional energy (of any form) acquired by the object above that rest energy will increase the object's total mass just as it increases its total energy. For example, after heating an object, its increase in energy could be measured as a small increase in mass, with a sensitive enough scale.Living organisms require available energy to stay alive, such as the energy humans get from food.  Human civilization requires energy to function, which it gets from  energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.",Energy,https://en.wikipedia.org/wiki/Energy
,,
"Electromagnetism is a branch of physics involving the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force usually exhibits electromagnetic fields such as electric fields, magnetic fields and light, and is one of the four fundamental interactions (commonly called forces) in nature. The other three fundamental interactions are the strong interaction, the weak interaction and gravitation.[1]",Electromagnetism,https://en.wikipedia.org/wiki/Electromagnetism
,,
"Electrochemistry is the branch of physical chemistry that studies the relationship between electricity, as a measurable and quantitative phenomenon, and identifiable chemical change, with either electricity considered an outcome of a particular chemical change or vice versa. These reactions involve electric charges moving between electrodes and an electrolyte (or ionic species in a solution). Thus electrochemistry deals with the interaction between electrical energy and chemical change.When a chemical reaction is caused by an externally supplied current, as in electrolysis, or if an electric current is produced by a spontaneous chemical reaction as in a battery, it is called an electrochemical reaction. Chemical reactions where electrons are transferred directly between molecules and/or atoms are called oxidation-reduction or (redox) reactions. In general, electrochemistry describes the overall reactions when individual redox reactions are separate but connected by an external electric circuit and an intervening electrolyte.",Electrochemistry,https://en.wikipedia.org/wiki/Electrochemistry
,,
"Elderly care, or simply eldercare (also known in parts of the English speaking world as aged care), is the fulfillment of the special needs and requirements that are unique to senior citizens.  This broad term encompasses such services as assisted living, adult day care, long term care, nursing homes (often referred to as residential care), hospice care, and home care. Because of the wide variety of elderly care found nationally, as well as differentiating cultural perspectives on elderly citizens, cannot be limited to any one practice. For example, many countries in Asia use government-established elderly care quite infrequently, preferring the traditional methods of being cared for by younger generations of family members.Elderly care emphasizes the social and personal requirements of senior citizens who need some assistance with daily activities and health care, but who desire to age with dignity. It is an important distinction, in that the design of housing, services, activities, employee training and such should be truly customer-centered. It is also noteworthy that a large amount of global elderly care falls under the unpaid market sector.[1]",Elderly care,https://en.wikipedia.org/wiki/Elderly_care
,,
This page about an IT-related or software-related company or corporation is a stub.,Computer Science and Engineering,https://en.wikipedia.org/wiki/Computer_Science_and_Engineering
,,
"An economic impact analysis (EIA) examines the effect of an event on the economy in a specified area, ranging from a single neighborhood to the entire globe. It usually measures changes in business revenue, business profits, personal wages, and/or jobs.  The economic event analyzed can include implementation of a new policy or project, or may simply be the presence of a business or organization.  An economic impact analysis is commonly conducted when there is public concern about the potential impacts of a proposed project or policy.[1][2]An economic impact analysis typically measures or estimates the change in economic activity between two scenarios, one assuming the economic event occurs, and one assuming it does not occur (which is referred to as the counterfactual case).  This can be accomplished either before or after the event (ex ante or ex post).",Economic impact analysis,https://en.wikipedia.org/wiki/Economic_impact_analysis
,,
"An earthquake (also known as a quake, tremor or temblor) is the shaking of the surface of the Earth, resulting from the sudden release of energy in the Earth's lithosphere that creates seismic waves. Earthquakes can range in size from those that are so weak that they cannot be felt to those violent enough to toss people around and destroy whole cities. The seismicity, or seismic activity, of an area is the frequency, type and size of earthquakes experienced over a period of time. The word tremor is also used for non-earthquake seismic rumbling.At the Earth's surface, earthquakes manifest themselves by shaking and displacing or disrupting the ground. When the epicenter of a large earthquake is located offshore, the seabed may be displaced sufficiently to cause a tsunami. Earthquakes can also trigger landslides, and occasionally volcanic activity.In its most general sense, the word earthquake is used to describe any seismic event ?€? whether natural or caused by humans ?€? that generates seismic waves. Earthquakes are caused mostly by rupture of geological faults, but also by other events such as volcanic activity, landslides, mine blasts, and nuclear tests. An earthquake's point of initial rupture is called its focus or hypocenter. The epicenter is the point at ground level directly above the hypocenter.",Earthquake,https://en.wikipedia.org/wiki/Earthquake
,,
"Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which then communicate and coordinate their actions by passing messages to one other.[1] The components interact with one other in order to achieve a common goal. Three significant characteristics[why?] of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.[1] Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.A computer program that runs within a distributed system is called a distributed program (and distributed programming is the process of writing such programs).[2] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues[3].Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[4] which communicate with each other via message passing.[5]",Distributed computing,https://en.wikipedia.org/wiki/Distributed_computing
,,
This belongs to Disambiguation pages. Open it in your browser,Dynamics,https://en.wikipedia.org/wiki/Dynamics
,,
This page about an IT-related or software-related company or corporation is a stub.,Digital Systems,https://en.wikipedia.org/wiki/Digital_Systems
,,
This belongs to Disambiguation pages. Open it in your browser,Deposition,https://en.wikipedia.org/wiki/Deposition
,,
"Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, beliefs, and habits. Educational methods include storytelling, discussion, teaching, training, and directed research. Education frequently takes place under the guidance of educators, but learners may also educate themselves.[1]  Education can take place in formal or informal settings and any experience that has a formative effect on the way one thinks, feels, or acts may be considered educational. The methodology of teaching is called pedagogy.Formal education is commonly divided formally into such stages as preschool or kindergarten, primary school, secondary school and then college, university, or apprenticeship.A right to education has been recognized by some governments and the United Nations.[2] In most regions, education is compulsory up to a certain age.",Education,https://en.wikipedia.org/wiki/Education
,,
"Design is the creation of a plan or convention for the construction of an object, system or measurable human interaction. Design has different connotations in different fields (see design disciplines below). In some cases, the direct construction of an object (as in pottery, engineering, management, coding, and graphic design) is also considered to use design thinking.Designing often necessitates considering the aesthetic, functional, economic, and sociopolitical dimensions of both the design object and design process. It may involve considerable research, thought, modeling, interactive adjustment, and re-design. Meanwhile, diverse kinds of objects may be designed, including clothing, graphical user interfaces, products, skyscrapers, corporate identities, business processes, and even methods or processes of designing.[1]Thus ""design"" may be a substantive referring to a categorical abstraction of a created thing or things (the design of something), or a verb for the process of creation as is made clear by grammatical context.",Design,https://en.wikipedia.org/wiki/Design
,,
"A decision support system (DSS) is an information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance?€?i.e. unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both.While academics have perceived DSS as a tool to support decision making process, DSS users see DSS as a tool to facilitate organizational processes.[1] Some authors have extended the definition of DSS to include any system that might support decision making and some DSS include a decision-making software component; Sprague (1980)[2] defines a properly termed DSS as follows:",Decision support system,https://en.wikipedia.org/wiki/Decision_support_system
,,
"United States federal research funders use the term cyberinfrastructure to describe research environments that support advanced data acquisition, data storage, data management, data integration, data mining, data visualization and other computing and information processing services distributed over the Internet beyond the scope of a single institution.  In scientific usage, cyberinfrastructure is a technological and sociological solution to the problem of efficiently connecting laboratories, data, computers, and people with the goal of enabling derivation of novel scientific theories and knowledge.",Cyberinfrastructure,https://en.wikipedia.org/wiki/Cyberinfrastructure
,,
"Data analysis is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, while being used in different business, science, and social science domains.Data mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information.[1] In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.Data integration is a precursor to data analysis,[according to whom?] and data analysis is closely linked[how?] to data visualization and data dissemination. The term data analysis is sometimes used as a synonym for data modeling.",Data analysis,https://en.wikipedia.org/wiki/Data_analysis
,,
"A cyber-physical (also styled cyberphysical) system (CPS) is a mechanism that is controlled or monitored by computer-based algorithms, tightly integrated with the Internet and its users. In cyber-physical systems, physical and software components are deeply intertwined, each operating on different spatial and temporal scales, exhibiting multiple and distinct behavioral modalities, and interacting with each other in a lot of ways that change with context.[1] Examples of CPS include smart grid, autonomous automobile systems, medical monitoring, process control systems, robotics systems, and automatic pilot avionics.[2]CPS involves transdisciplinary approaches, merging theory of cybernetics, mechatronics, design and process science.[3][4][5] The process control is often referred to as embedded systems.  In embedded systems, the emphasis tends to be more on the computational elements, and less on an intense link between the computational and physical elements. CPS is also similar to the Internet of Things (IoT), sharing the same basic architecture; nevertheless, CPS presents a higher combination and coordination between physical and computational elements.[6]Precursors of cyber-physical systems can be found in areas as diverse as aerospace, automotive, chemical processes, civil infrastructure, energy, healthcare, manufacturing, transportation, entertainment, and consumer appliances.[2]",Cyber-physical system,https://en.wikipedia.org/wiki/Cyber-physical_system
,,
"Engineering education is the activity of teaching knowledge and principles to the professional practice of engineering. It includes an initial education (bachelor's and/or master's degree), and any advanced education and specializations that follow. Engineering education is typically accompanied by additional postgraduate examinations and supervised training as the requirements for a professional engineering license. The length of education, and training to qualify as a basic professional engineer, is typically 8?€?12 years, with 15?€?20 years for an engineer who takes responsibility for major projects.Science, technology, engineering, and mathematics (STEM) education in primary and secondary schools often serves as the foundation for engineering education at the university level.[1] In the United States, engineering education is a part of the STEM initiative in public schools. Service-learning in engineering education is gaining popularity within the variety of disciplinary focuses within engineering education including mechanical engineering, industrial engineering, computer engineering, electrical engineering, and other engineering education.",Engineering education,https://en.wikipedia.org/wiki/Engineering_education
,,
"Content-based image retrieval (CBIR), also known as query by image content (QBIC) and content-based visual information retrieval (CBVIR) is the application of computer vision techniques to the image retrieval problem, that is, the problem of searching for digital images in large databases (see this survey[1] for a recent scientific overview of the CBIR field). Content-based image retrieval is opposed to traditional concept-based approaches (see Concept-based image indexing).""Content-based"" means that the search analyzes the contents of the image rather than the metadata such as keywords, tags, or descriptions associated with the image. The term ""content"" in this context might refer to colors, shapes, textures, or any other information that can be derived from the image itself.  CBIR is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness.Having humans manually annotate images by entering keywords or metadata in a large database can be time consuming and may not capture the keywords desired to describe the image.  The evaluation of the effectiveness of keyword image search is subjective and has not been well-defined.  In the same regard, CBIR systems have similar challenges in defining success.[2] ""Keywords also limit the scope of queries to the set of predetermined criteria."" and, ""having been set up"" are less reliable than using the content itself.[3]",Content-based image retrieval,https://en.wikipedia.org/wiki/Content-based_image_retrieval
,,
"In mathematics, convex geometry is the branch of geometry studying convex sets, mainly in Euclidean space. Convex sets occur naturally in many areas: computational geometry, convex analysis, discrete geometry, functional analysis, geometry of numbers, integral geometry, linear programming, probability theory, etc.",Convex geometry,https://en.wikipedia.org/wiki/Convex_geometry
,,
"""Controls"", released in March 2018 by Saved By Vinyl, Tiny Room Records, and Moorworks Records[1] is one of two singles for Astral Swans' 2018 release Strange Prison.[2][3]",Controls,https://en.wikipedia.org/wiki/Controls
,,
"Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[1][2][3]Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8]As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, and image restoration.[6]",Computer vision,https://en.wikipedia.org/wiki/Computer_vision
,,
"In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation.[1] In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.[2]",Computer architecture,https://en.wikipedia.org/wiki/Computer_architecture
,,
"The expression computational intelligence (CI) usually refers to the ability of a computer to learn a specific task from data or experimental observation. Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence.Generally, computational intelligence is a set of nature-inspired computational methodologies and approaches to address complex real-world problems to which mathematical or traditional modelling can be useless for a few reasons: the processes might be too complex for mathematical reasoning, it might contain some uncertainties during the process, or the process might simply be stochastic in nature.[1][page??needed] Indeed, many real-life problems cannot be translated into binary language (unique values of 0 and 1) for computers to process it. Computational Intelligence therefore provides solutions for such problems.The methods used are close to the human's way of reasoning, i.e. it uses inexact and incomplete knowledge, and it is able to produce control actions in an adaptive way. CI therefore uses a combination of five main complementary techniques.[1] The fuzzy logic which enables the computer to understand natural language,[2][page??needed][3] artificial neural networks which permits the system to learn experiential data by operating like the biological one, evolutionary computing, which is based on the process of natural selection, learning theory, and probabilistic methods which helps dealing with uncertainty imprecision.[1]Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence[4] and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. But although both Computational Intelligence (CI) and Artificial Intelligence (AI) seek similar goals, there's a clear distinction between them[according to whom?][citation needed].Computational Intelligence is thus a way of performing like human beings[citation needed]. Indeed, the characteristic of ""intelligence"" is usually attributed[by whom?] to humans. More recently, many products and items also claim to be ""intelligent"", an attribute which is directly linked to the reasoning and decision making[further explanation needed].",Computational intelligence,https://en.wikipedia.org/wiki/Computational_intelligence
,,
"Compressible flow (gas dynamics) is the branch of fluid mechanics that deals with flows having significant changes in fluid density. Gases, mostly, display such behaviour.[1] While all flows are compressible, flows are usually treated as being incompressible when the Mach number (the ratio of the speed of the flow to the speed of sound) is less than 0.3 (since the density change due to velocity is about 5% in that case). [2] The study of compressible flow is relevant to high-speed aircraft, jet engines, rocket motors, high-speed entry into a planetary atmosphere, gas pipelines, commercial applications such as abrasive blasting, and many other fields.",Compressible flow,https://en.wikipedia.org/wiki/Compressible_flow
,,
"Computational fluid dynamics (CFD) is a branch of fluid mechanics that uses numerical analysis and data structures to solve and analyze problems that involve fluid flows are used to perform the calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. With high-speed supercomputers, better solutions can be achieved. Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as transonic or turbulent flows. Initial experimental validation of such software is performed using a wind tunnel with the final validation coming in full-scale testing, e.g. flight tests.",Computational fluid dynamics,https://en.wikipedia.org/wiki/Computational_fluid_dynamics
,,
This page about an IT-related or software-related company or corporation is a stub.,Commercial building,https://en.wikipedia.org/wiki/Commercial_building
,,
"Computer security, cybersecurity[1], or IT security is the protection of computer systems from theft or damage to their hardware, software or electronic data, as well as from disruption or misdirection of the services they provide.The field is of growing importance due to increasing reliance on computer systems, the Internet[2] and wireless networks such as Bluetooth and Wi-Fi, and due to the growth of ""smart"" devices, including smartphones, televisions and the various tiny devices that constitute the Internet of Things. Due to its complexity, both in terms of politics and technology, it is also one of the major challenges of contemporary world.[3]",Computer security,https://en.wikipedia.org/wiki/Computer_security
,,
"A computer network, or data network, is a digital telecommunications network which allows nodes to share resources. In computer networks, computing devices exchange data with each other using connections (data links) between nodes. These data links are established over cable media such as wires or optic cables, or wireless media such as WiFi.Network computer devices that originate, route and terminate the data are called network nodes.[1] Nodes can include hosts such as personal computers, phones, servers as well as networking hardware. Two such devices can be said to be networked together when one device is able to exchange information with the other device, whether or not they have a direct connection to each other. In most cases, application-specific communications protocols are layered (i.e. carried as payload) over other more general communications protocols. This formidable collection of information technology requires skilled network management to keep it all running reliably.Computer networks support an enormous number of applications and services such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications as well as many others. Computer networks differ in the transmission medium used to carry their signals, communications protocols to organize network traffic, the network's size, topology, traffic control mechanism and organizational intent. The best-known computer network is the Internet.",Computer network,https://en.wikipedia.org/wiki/Computer_network
,,
"Cloud computing is shared pools of configurable computer system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet.  Cloud computing relies on sharing of resources to achieve coherence and economies of scale, similar to a public utility.Third-party clouds enable organizations to focus on their core businesses instead of expending resources on computer infrastructure and maintenance.[1] Advocates note that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand.[1][2][3] Cloud providers typically use a  ""pay-as-you-go"" model, which can lead to unexpected operating expenses if administrators are not familiarized with cloud-pricing models.[4]Since the launch of Amazon EC2 in 2006, the availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture, and autonomic and utility computing has led to growth in cloud computing.[5][6][7]",Cloud computing,https://en.wikipedia.org/wiki/Cloud_computing
,,
"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including works such as roads, bridges, canals, dams, airports, sewerage systems, pipelines, and railways.[1][2] Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering,[3] and it is defined to distinguish non-military engineering from military engineering.[4] Civil engineering takes place in the public sector from municipal through to national governments, and in the private sector from individual homeowners through to international companies.",Civil engineering,https://en.wikipedia.org/wiki/Civil_engineering
,,
"Chemical engineering is a branch of engineering that uses principles of chemistry, physics, mathematics, and economics to efficiently use, produce, transform, and transport chemicals, materials, and energy. A chemical engineer designs large-scale processes that convert chemicals, raw materials, living cells, microorganisms, and energy into useful forms and products.Chemical engineers are involved in many aspects of plant design and operation, including safety and hazard assessments, process design and analysis, control engineering, chemical reaction engineering, construction specification, and operating instructions.",Chemical engineering,https://en.wikipedia.org/wiki/Chemical_engineering
,,
"An integrated circuit or monolithic integrated circuit (also referred to as an IC, a chip, or a microchip) is a set of electronic circuits on one small flat piece (or ""chip"") of semiconductor material, normally silicon.  The integration of large numbers of tiny transistors into a small chip results in circuits that are orders of magnitude smaller, cheaper, and faster than those constructed of discrete electronic components. The IC's mass production capability, reliability and building-block approach to circuit design has ensured the rapid adoption of standardized ICs in place of designs using discrete transistors.  ICs are now used in virtually all electronic equipment and have revolutionized the world of electronics. Computers, mobile phones, and other digital home appliances are now inextricable parts of the structure of modern societies, made possible by the small size and low cost of ICs.Integrated circuits were made practical by mid-20th-century technology advancements in semiconductor device fabrication.  Since their origins in the 1960s, the size, speed, and capacity of chips have progressed enormously, driven by technical advances that fit more and more transistors on chips of the same size - a modern chip may have several billion transistors in an area the size of a human fingernail.  These advances, roughly following Moore's law, make computer chips of today possess millions of times the capacity and thousands of times the speed of the computer chips of the early 1970s.ICs have two main advantages over discrete circuits: cost and performance. Cost is low because the chips, with all their components, are printed as a unit by photolithography rather than being constructed one transistor at a time. Furthermore, packaged ICs use much less material than discrete circuits. Performance is high because the IC's components switch quickly and consume comparatively little power because of their small size and close proximity.  The main disadvantage of ICs is the high cost to design them and fabricate the required photomasks.  This high initial cost means ICs are only practical when high production volumes are anticipated.",Integrated circuit,https://en.wikipedia.org/wiki/Integrated_circuit
,,
"A bridge is a structure built to span physical obstacles without closing the way underneath such as a body of water, valley, or road, for the purpose of providing passage over the obstacle, usually something that can be detrimental to cross otherwise. There are many different designs that each serve a particular purpose and apply to different situations. Designs of bridges vary depending on the function of the bridge, the nature of the terrain where the bridge is constructed and anchored, the material used to make it, and the funds available to build it.",Bridge,https://en.wikipedia.org/wiki/Bridge
,,
"A biorefinery is a facility that integrates biomass conversion processes and equipment to produce fuels, power, heat, and value-added chemicals from biomass. The biorefinery concept is analogous to today's petroleum refinery, which produce multiple fuels and products from petroleum.[1]The International Energy Agency Bioenergy Task 42 on Biorefineries has defined biorefining as the sustainable processing of biomass into a spectrum of bio-based products (food, feed, chemicals, materials) and bioenergy (biofuels, power and/or heat).By producing multiple products, a biorefinery takes advantage of the various components in biomass and their intermediates therefore maximizing the value derived from the biomass feedstock. Some researcher have considered the exploration of a biorefinery as a practical method of improving the economic performance of stand-alone biomass to bioenergy system since biochemicals are produced[2] A biorefinery could, for example, produce one or several low-volume, but high-value, chemical or nutraceutical products and a low-value, but high-volume liquid transportation fuel such as biodiesel or bioethanol (see also alcohol fuel).[3] At the same time generating electricity and process heat, through combined heat and power (CHP) technology, for its own use and perhaps enough for sale of electricity to the local utility. The high-value products increase profitability, the high-volume fuel helps meet energy needs, and the power production helps to lower energy costs and reduce greenhouse gas emissions from traditional power plant facilities.  Although some facilities exist that can be called bio-refineries, the bio-refinery has yet to be fully realized. Future biorefineries may play a major role in producing chemicals and materials that are traditionally produced from petroleum.",Biorefinery,https://en.wikipedia.org/wiki/Biorefinery
,,
"In transportation, the Physical Internet?? refers to the combination of digital transportation networks that are deploying to replace analog road networks. As the Internet has resolved itself into niche implementations for high-speed (fiberoptics), local area networks (Wifi), and local device (BlueTooth). ET3 and Hyperloop are currently high speed examples. JPods are examples of urban networks.",Physical Internet,https://en.wikipedia.org/wiki/Physical_Internet
,,
"Catalysis is the process of increasing the rate of a chemical reaction by adding a substance known as a catalyst[1] (/??k??t??l??st/), which is not consumed in the catalyzed reaction and can continue to act repeatedly. Because of this, only very small amounts of catalyst are required to alter the reaction rate in principle.[2]In general, chemical reactions occur faster in the presence of a catalyst because the catalyst provides an alternative reaction mechanism with a lower activation energy than the non-catalyzed mechanism. In catalyzed mechanisms, the catalyst usually reacts to form a temporary intermediate, which then regenerates the original catalyst in a cyclic process.A substance which provides a mechanism with a higher activation energy does not increase the rate because the reaction can still occur by the non-catalyzed route.[3] An added substance which does increase the reaction rate is not considered a catalyst[1] but a reaction inhibitor (see below).Catalysts may be classified as either homogeneous or heterogeneous. A homogeneous catalyst is one whose molecules are dispersed in the same phase (usually gaseous or liquid) as the reactant's molecules. A heterogeneous catalyst is one whose molecules are not in the same phase as the reactant's, which are typically gases or liquids that are adsorbed onto the surface of the solid catalyst. Enzymes and other biocatalysts are often considered as a third category.",Catalysis,https://en.wikipedia.org/wiki/Catalysis
,,
"Biomedical engineering (BME), also known as bioengineering, is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g. diagnostic or therapeutic). This field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy.[1]
Also included under the scope of a biomedical engineer is the management of current medical equipment within hospitals while adhering to relevant industry standards. This involves equipment recommendations, procurement, routine testing and preventative maintenance, through to decommissioning and disposal. This role is also known as a Biomedical Equipment Technician (BMET) or clinical engineering.Biomedical engineering has recently emerged as its own study, as compared to many other engineering fields.  Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EKG/ECGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.",Biomedical engineering,https://en.wikipedia.org/wiki/Biomedical_engineering
,,
"Bioprocess engineering, also biochemical engineering, is a specialization of  chemical engineering or Biological engineering, It deals with the design and development of equipment and processes for the manufacturing of products such as agriculture, food, feed, pharmaceuticals, nutraceuticals, chemicals, and polymers and paper from biological materials & treatment of waste water. 
Bioprocess engineering is a conglomerate of mathematics, biology and industrial design,and consists of various spectrums like designing of bioreactors, study of fermentors (mode of operations etc.). It also deals with studying various biotechnological processes used in industries for large scale production of biological product for optimization of yield in the end product and the quality of end product. Bioprocess engineering may include the work of mechanical, electrical, and industrial engineers to apply principles of their disciplines to processes based on using living cells or sub component of such cells.[1]",Bioprocess engineering,https://en.wikipedia.org/wiki/Bioprocess_engineering
,,
"Microscopy is the technical field of using microscopes to view objects and areas of objects that cannot be seen with the naked eye (objects that are not within the resolution range of the normal eye).[1] There are three well-known branches of microscopy: optical, electron, and scanning probe microscopy.Optical microscopy and electron microscopy involve the diffraction, reflection, or refraction of electromagnetic radiation/electron beams interacting with the specimen, and the collection of the scattered radiation or another signal in order to create an image. This process may be carried out by wide-field irradiation of the sample (for example standard light microscopy and transmission electron microscopy) or by scanning a fine beam over the sample (for example confocal laser scanning microscopy and scanning electron microscopy). Scanning probe microscopy involves the interaction of a scanning probe with the surface of the object of interest. The development of microscopy revolutionized biology, gave rise to the field of histology and so remains an essential technique in the life and physical sciences.",Microscopy,https://en.wikipedia.org/wiki/Microscopy
,,
"Bio-inspired computing, short for biologically inspired computing, is a field of study that loosely knits together subfields related to the topics of connectionism, social behaviour and emergence. It is often closely related to the field of artificial intelligence, as many of its pursuits can be linked to machine learning. It relies heavily on the fields of biology, computer science and mathematics. Briefly put, it is the use of computers to model the living phenomena, and simultaneously the study of life to improve the usage of computers. Biologically inspired computing is a major subset of natural computation.",Bio-inspired computing,https://en.wikipedia.org/wiki/Bio-inspired_computing
,,
"Biological engineering or bio-engineering is the application of principles of biology and the tools of engineering to create usable, tangible, economically viable products.[1]  Biological engineering employs knowledge and expertise from a number of pure and applied sciences,[2] such as mass and heat transfer, kinetics, biocatalysts, biomechanics, bioinformatics, separation and purification processes, bioreactor design, surface science, fluid mechanics, thermodynamics, and polymer science. It is used in the design of medical devices, diagnostic equipment, biocompatible materials, renewable bioenergy, ecological engineering, agricultural engineering, and other areas that improve the living standards of societies. Examples of bioengineering research include bacteria engineered to produce chemicals, new medical imaging technology, portable disease diagnostic devices, prosthetics, and tissue engineered organs[3]. Bio-engineering is pretty overlapping with the fields of biotechnology and biomedical sciences[4]. But, the three fields are not the same.[how?]In general, biological engineers (or biomedical engineers) attempt to either mimic biological systems to create products or modify and control biological systems so that they can replace, augment, sustain, or predict chemical and mechanical processes.[5] Bioengineers can apply their expertise to other applications of engineering and biotechnology, including genetic modification of plants and microorganisms, bioprocess engineering, and biocatalysis. Working with doctors, clinicians and researchers, bioengineers use traditional engineering principles and techniques and apply them to real-world biological and medical problems[6].",Biological engineering,https://en.wikipedia.org/wiki/Biological_engineering
,,
"Bio-MEMS is an abbreviation for biomedical (or biological) microelectromechanical systems. Bio-MEMS have considerable overlap, and is sometimes considered synonymous, with lab-on-a-chip (LOC) and micro total analysis systems (??TAS). Bio-MEMS is typically more focused on mechanical parts and microfabrication technologies made suitable for biological applications. On the other hand, lab-on-a-chip is concerned with miniaturization and integration of laboratory processes and experiments into single (often microfluidic) chips. In this definition, lab-on-a-chip devices do not strictly have biological applications, although most do or are amendable to be adapted for biological purposes. Similarly, micro total analysis systems may not have biological applications in mind, and are usually dedicated to chemical analysis. A broad definition for bio-MEMS can be used to refer to the science and technology of operating at the microscale for biological and biomedical applications, which may or may not include any electronic or mechanical functions.[2] The interdisciplinary nature of bio-MEMS combines material sciences, clinical sciences, medicine, surgery, electrical engineering, mechanical engineering, optical engineering, chemical engineering, and biomedical engineering.[2] Some of its major applications include genomics, proteomics, molecular diagnostics, point-of-care diagnostics, tissue engineering, single cell analysis and implantable microdevices.[2]",Bio-MEMS,https://en.wikipedia.org/wiki/Bio-MEMS
,,
"Bioinformatics /??ba??.o??????nf??r??m??t??ks/??(??listen) is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines biology, computer science, mathematics and statistics to analyze and interpret biological data. Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques.Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.[1]",Bioinformatics,https://en.wikipedia.org/wiki/Bioinformatics
,,
"Bioproducts or bio-based products are materials, chemicals and energy derived from renewable biological resources.[1][2][3]",Bioproducts,https://en.wikipedia.org/wiki/Bioproducts
,,
"A biofuel is a fuel that is produced through contemporary biological processes, such as agriculture and anaerobic digestion, rather than a fuel produced by geological processes such as those involved in the formation of fossil fuels, such as coal and petroleum, from prehistoric biological matter.Biofuels can be derived directly from plants (i.e. energy crops), or indirectly from agricultural, commercial, domestic, and/or industrial wastes.[1] Renewable biofuels generally involve contemporary carbon fixation, such as those that occur in plants or microalgae through the process of photosynthesis. Other renewable biofuels are made through the use or conversion of biomass  (referring to recently living organisms, most often referring to plants or plant-derived materials). This biomass can be converted to convenient energy-containing substances in three different ways: thermal conversion, chemical conversion, and biochemical conversion. This biomass conversion can result in fuel in solid, liquid, or gas form. This new biomass can also be used directly for biofuels.Biofuels are in theory carbon-neutral because the carbon dioxide that is absorbed by the plants is equal to the carbon dioxide that is released when the fuel is burned.[2] However, in practice, whether or not a biofuel is carbon-neutral also depends greatly on whether the land which is used to grow the biofuel (with 1st and 2nd generation biofuel) needed to be cleared of carbon-holding vegetation or not.Bioethanol is an alcohol made by fermentation, mostly from carbohydrates produced in sugar or starch crops such as corn, sugarcane, or sweet sorghum. Cellulosic biomass, derived from non-food sources, such as trees and grasses, is also being developed as a feedstock for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form (E100), but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil. Current plant design does not provide for converting the lignin portion of plant raw materials to fuel components by fermentation.Biodiesel can be used as a fuel for vehicles in its pure form (B100), but it is usually used as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe.In 2010, worldwide biofuel production reached 105 billion liters (28 billion gallons US), up 17% from 2009,[3] and biofuels provided 2.7% of the world's fuels for road transport. Global ethanol fuel production reached 86 billion liters (23 billion gallons US) in 2010, with the United States and Brazil as the world's top producers, accounting together for about 90% of global production. The world's largest biodiesel producer is the European Union, accounting for 53% of all biodiesel production in 2010.[3] As of 2011, mandates for blending biofuels exist in 31 countries at the national level and in 29 states or provinces.[4] The International Energy Agency has a goal for biofuels to meet more than a quarter of world demand for transportation fuels by 2050 to reduce dependence on petroleum and coal.[5] The production of biofuels also led into a flourishing automotive industry, where by 2010, 79% of all cars produced in Brazil were made with a hybrid fuel system of bioethanol and gasoline.[6]There are various social, economic, environmental and technical issues relating to biofuels production and use, which have been debated in the popular media and scientific journals.",Biofuel,https://en.wikipedia.org/wiki/Biofuel
,,
"Big data is a term used to refer to the study and applications of data sets that are so big and complex that traditional data-processing application software are inadequate to deal with them. Big data challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy and data source. There are a number of concepts associated with big data: originally there were 3 concepts volume, variety, velocity.[2] Other concepts later attributed with big data are veracity (i.e., how much noise is in the data) [3] and value.[4]Lately, the term ""big data"" tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from data, and seldom to a particular size of data set. ""There is little doubt that the quantities of data now available are indeed large, but that?€?s not the most relevant characteristic of this new data ecosystem.""[5]
Analysis of data sets can find new correlations to ""spot business trends, prevent diseases, combat crime and so on.""[6] Scientists, business executives, practitioners of medicine, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet search, fintech, urban informatics, and business informatics.  Scientists encounter limitations in e-Science work, including meteorology, genomics,[7] connectomics, complex physics simulations, biology and environmental research.[8]Data sets grow rapidly - in part because they are increasingly gathered by cheap and numerous information-sensing Internet of things devices such as mobile devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks.[9][10] The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;[11] as of  2012[update], every day 2.5 exabytes (2.5??1018) of data are generated.[12] Based on an IDC report prediction, the global data volume will grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020.[13] By 2025, IDC predicts there will be 163 zettabytes of data.[14] One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.[15]Relational database management systems and desktop statistics[clarification needed] and software packages to visualize data often have difficulty handling big data. The work may require ""massively parallel software running on tens, hundreds, or even thousands of servers"".[16] What counts as ""big data"" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target. ""For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.""[17]",Big data,https://en.wikipedia.org/wiki/Big_data
,,
"Approximate computing is a computation technique which returns a possibly inaccurate result rather than a guaranteed accurate result, and can be used for applications where an approximate result is sufficient for its purpose.[1][2] One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.[clarification needed]  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.[1]The key requirement in approximate computing is that approximation can be introduced only in non-critical data, since approximating critical data (e.g., control operations) can lead to disastrous consequences, such as program crash or erroneous output.",Approximate computing,https://en.wikipedia.org/wiki/Approximate_computing
,,
"Applied mathematics is the application of mathematical methods by different fields such as science, engineering, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term ""applied mathematics"" also describes the professional specialty  in which mathematicians work on practical problems by formulating and studying mathematical models. In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics.",Applied mathematics,https://en.wikipedia.org/wiki/Applied_mathematics
,,
"Automation is the technology by which a process or procedure is performed without human assistance.[1] Automation [2] or automatic control is the use of various control systems for operating equipment such as machinery, processes in factories, boilers and heat treating ovens, switching on telephone networks, steering and stabilization of ships, aircraft and other applications and vehicles with minimal or reduced human intervention.  Some processes have been completely automated.Automation covers applications ranging from a household thermostat controlling a boiler, to a large industrial control system with tens of thousands of input measurements and output control signals. In control complexity it can range from simple on-off control to multi-variable high level algorithms.In the simplest type of an automatic control loop, a controller compares a measured value of a process with a desired set value, and processes the resulting error signal to change some input to the process, in such a way that the process stays at its set point despite disturbances. This closed-loop control is an application of negative feedback to a system. The mathematical basis of control theory was begun in the 18th century, and advanced rapidly in the 20th.Automation has been achieved by various means including mechanical, hydraulic, pneumatic, electrical, electronic devices and computers, usually in combination. Complicated systems, such as modern factories, airplanes and ships typically use all these combined techniques. The benefit of automation include labor savings, savings in electricity costs, savings in material costs, and improvements to quality, accuracy and precision.The term automation, inspired by the earlier word automatic (coming from automaton), was not widely used before 1947, when Ford established an automation department.[2] It was during this time that industry was rapidly adopting feedback controllers, which were introduced in the 1930s.[3]",Automation,https://en.wikipedia.org/wiki/Automation
,,
"Aquaculture (less commonly spelled aquiculture[2]), also known as aquafarming, is the farming of fish, crustaceans, molluscs, aquatic plants, algae, and other organisms. Aquaculture involves cultivating freshwater and saltwater populations under controlled conditions, and can be contrasted with commercial fishing, which is the harvesting of wild fish.[3]  Mariculture refers to aquaculture practiced in marine environments and in underwater habitats.According to the Food and Agriculture Organization (FAO), aquaculture ""is understood to mean the farming of aquatic organisms including fish, molluscs, crustaceans and aquatic plants. Farming implies some form of intervention in the rearing process to enhance production, such as regular stocking, feeding, protection from predators, etc. Farming also implies individual or corporate ownership of the stock being cultivated.""[4] The reported output from global aquaculture operations in 2014 supplied over one half of the fish and shellfish that is directly consumed by humans;[5][6] however, there are issues about the reliability of the reported figures.[7] Further, in current aquaculture practice, products from several pounds of wild fish are used to produce one pound of a piscivorous fish like salmon.[8]Particular kinds of aquaculture include fish farming, shrimp farming, oyster farming, mariculture, algaculture (such as seaweed farming), and the cultivation of ornamental fish. Particular methods include aquaponics and integrated multi-trophic aquaculture, both of which integrate fish farming and aquatic plant farming.",Aquaculture,https://en.wikipedia.org/wiki/Aquaculture
,,
"Asphalt, also known as bitumen (UK: /??b??tj??m??n/, US: /b????tju??m??n, ba??-/),[1] is a sticky, black, and highly viscous liquid or semi-solid form of petroleum. It may be found in natural deposits or may be a refined product, and is classed as a pitch. Before the 20th century, the term asphaltum was also used.[2] The word is derived from the Ancient Greek ????????????????? ??sphaltos.The primary use (70%) of asphalt is in road construction, where it is used as the glue or binder mixed with aggregate particles to create asphalt concrete. Its other main uses are for bituminous waterproofing products, including production of roofing felt and for sealing flat roofs.[3]The terms ""asphalt"" and ""bitumen"" are often used interchangeably to mean both natural and manufactured forms of the substance. In American English, ""asphalt"" (or ""asphalt cement"") is commonly used for a refined residue from the distillation process of selected crude oils. Outside the United States, the product is often called ""bitumen"", and geologists worldwide often prefer the term for the naturally occurring variety. Common colloquial usage often refers to various forms of asphalt as ""tar"", as in the name of the La Brea Tar Pits.Naturally occurring asphalt is sometimes specified by the term ""crude bitumen"". Its viscosity is similar to that of cold molasses[4][5] while the material obtained from the fractional distillation of crude oil boiling at 525????C (977????F) is sometimes referred to as ""refined bitumen"". The Canadian province of Alberta has most of the world's reserves of natural asphalt in the Athabasca oil sands, which cover 142,000 square kilometres (55,000??sq??mi), an area larger than England.[6]",Asphalt,https://en.wikipedia.org/wiki/Asphalt
,,
"Assistive technology is an umbrella term that includes assistive, adaptive, and rehabilitative devices for people with disabilities while also including the process used in selecting, locating, and using them. People who have disabilities often have difficulty performing activities of daily living (ADLs) independently, or even with assistance. ADLs are self-care activities that include toileting, mobility (ambulation), eating, bathing, dressing and grooming. Assistive technology can ameliorate the effects of disabilities that limit the ability to perform ADLs. Assistive technology promotes greater independence by enabling people to perform tasks they were formerly unable to accomplish, or had great difficulty accomplishing, by providing enhancements to, or changing methods of interacting with, the technology needed to accomplish such tasks. For example, wheelchairs provide independent mobility for those who cannot walk, while assistive eating devices can enable people who cannot feed themselves to do so. Due to assistive technology, people with disabilities have an opportunity of a more positive and easygoing lifestyle, with an increase in ""social participation,"" ""security and control,"" and a greater chance to ""reduce institutional costs without significantly increasing household expenses.""[1]",Assistive technology,https://en.wikipedia.org/wiki/Assistive_technology
,,
"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured,[1][2] similar to data mining.Data science is a ""concept to unify statistics, data analysis, machine learning and their related methods"" in order to ""understand and analyze actual phenomena"" with data.[3] It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.Turing award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge.[4][5]In 2012, when Harvard Business Review called it ""The Sexiest Job of the 21st Century"",[6] the term ""data science"" became a buzzword.  It is now often used interchangeably with earlier concepts like business analytics,[7] business intelligence, predictive modeling, and statistics.  Even the suggestion that data science is sexy was paraphrasing Hans Rosling, featured in a 2011 BBC documentary with the quote, ""Statistics is now the sexiest subject around.""[8] Nate Silver referred to data science as a sexed up term for statistics.[9]   In many cases, earlier approaches and solutions are now simply rebranded as ""data science"" to be more attractive, which can cause the term to become ""dilute[d] beyond usefulness.""[10] While many university programs now offer a data science degree, there exists no consensus on a definition or suitable curriculum contents.[7] To its discredit, however, many data-science and big-data projects fail to deliver useful results, often as a result of poor management and utilization of resources.[11][12][13][14]",Data science,https://en.wikipedia.org/wiki/Data_science
,,
"Agriculture is the cultivation of land and breeding of animals and plants to provide food, fiber, medicinal plants and other products to sustain and enhance life.[1] Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that enabled people to live in cities. The study of agriculture is known as agricultural science. The history of agriculture dates back thousands of years; people gathered wild grains at least 105,000 years ago and began to plant them around 11,500 years ago before they became domesticated. Pigs, sheep, and cattle were domesticated over 10,000 years ago. Crops originate from at least 11 regions of the world. Industrial agriculture based on large-scale monoculture has in the past century come to dominate agricultural output, though about 2 billion people worldwide still depend on subsistence agriculture.Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased yields from cultivation, but at the same time have caused widespread ecological and environmental  damage. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage through contributions to global warming, depletion of aquifers, deforestation, antibiotic resistance, and growth hormones in industrially produced meat. Genetically modified organisms are widely used, although they are banned in several countries.The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials (such as rubber). Classes of foods include cereals (grains), vegetables, fruits, oils, meat, milk, fungi and eggs. Over one-third of the world's workers are employed in agriculture, second only to the service sector, although the number of agricultural workers in developed countries has decreased significantly over the past several centuries.",Agriculture,https://en.wikipedia.org/wiki/Agriculture
,,
"Alzheimer's disease (AD), also referred to simply as Alzheimer's, is a chronic neurodegenerative disease that usually starts slowly and worsens over time.[1][2] It is the cause of 60?€?70% of cases of dementia.[1][2] The most common early symptom is difficulty in remembering recent events (short-term memory loss).[1] As the disease advances, symptoms can include problems with language, disorientation (including easily getting lost), mood swings, loss of motivation, not managing self care, and behavioural issues.[1][2] As a person's condition declines, they often withdraw from family and society.[1] Gradually, bodily functions are lost, ultimately leading to death.[10] Although the speed of progression can vary, the typical life expectancy following diagnosis is three to nine years.[7][11]The cause of Alzheimer's disease is poorly understood.[1] About 70% of the risk is believed to be genetic with many genes usually involved.[4] Other risk factors include a history of head injuries, depression, or hypertension.[1] The disease process is associated with plaques and tangles in the brain.[4] A probable diagnosis is based on the history of the illness and cognitive testing with medical imaging and blood tests to rule out other possible causes.[5] Initial symptoms are often mistaken for normal ageing.[1] Examination of brain tissue is needed for a definite diagnosis.[4] Mental and physical exercise, and avoiding obesity may decrease the risk of AD; however, evidence to support these recommendations is not strong.[4][12] There are no medications or supplements that have been shown to decrease risk.[13]No treatments stop or reverse its progression, though some may temporarily improve symptoms.[2] Affected people increasingly rely on others for assistance, often placing a burden on the caregiver; the pressures can include social, psychological, physical, and economic elements.[14] Exercise programmes may be beneficial with respect to activities of daily living and can potentially improve outcomes.[15] Behavioural problems or psychosis due to dementia are often treated with antipsychotics, but this is not usually recommended, as there is little benefit with an increased risk of early death.[16][17]In 2015, there were approximately 29.8 million people worldwide with AD.[2][8] It most often begins in people over 65??years of age, although 4% to 5% of cases are early-onset Alzheimer's which begin before this.[3] It affects about 6% of people 65 years and older.[1] In 2015, dementia resulted in about 1.9 million deaths.[9] It was first described by, and later named after, German psychiatrist and pathologist Alois Alzheimer in 1906.[18]  In developed countries, AD is one of the most financially costly diseases.[19][20]",Alzheimer's disease,https://en.wikipedia.org/wiki/Alzheimer's_disease
,,
"The atmosphere of Earth is the layer of gases, commonly known as air, that surrounds the planet Earth and is retained by Earth's gravity. The atmosphere of Earth protects life on Earth by creating pressure allowing for liquid water to exist on the Earth's surface, absorbing ultraviolet solar radiation, warming the surface through heat retention (greenhouse effect), and reducing temperature extremes between day and night (the diurnal temperature variation).By volume, dry air contains 78.09% nitrogen, 20.95% oxygen,[2] 0.93% argon, 0.04% carbon dioxide, and small amounts of other gases. Air also contains a variable amount of water vapor, on average around 1% at sea level, and 0.4% over the entire atmosphere. Air content and atmospheric pressure vary at different layers, and air suitable for use in photosynthesis by terrestrial plants and breathing of terrestrial animals is found only in Earth's troposphere and in artificial atmospheres.The atmosphere has a mass of about 5.15??1018??kg,[3] three quarters of which is within about 11??km (6.8??mi; 36,000??ft) of the surface. The atmosphere becomes thinner and thinner with increasing altitude, with no definite boundary between the atmosphere and outer space. The K??rm??n line, at 100??km (62??mi), or 1.57% of Earth's radius, is often used as the border between the atmosphere and outer space. Atmospheric effects become noticeable during atmospheric reentry of spacecraft at an altitude of around 120??km (75??mi). Several layers can be distinguished in the atmosphere, based on characteristics such as temperature and composition.The study of Earth's atmosphere and its processes is called atmospheric science (aerology). Early pioneers in the field include L??on Teisserenc de Bort and Richard Assmann.[4]",Atmosphere of Earth,https://en.wikipedia.org/wiki/Atmosphere_of_Earth
,,
"Aerospace is the human effort in science, engineering and business to fly in the atmosphere of Earth (aeronautics) and surrounding space (astronautics). Aerospace organizations research, design, manufacture, operate, or maintain aircraft or spacecraft. Aerospace activity is very diverse, with a multitude of commercial, industrial and military applications.Aerospace is not the same as airspace, which is the physical air space directly above a location on the ground. The beginning of space and the ending of the air is considered as 100??km above the ground according to the physical explanation that the air pressure is too low for a lifting body to generate meaningful lift force without exceeding orbital velocity.[1]",Aerospace,https://en.wikipedia.org/wiki/Aerospace
,,
"Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals.  In computer science  AI research is defined as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"".[2]The scope of AI is disputed: as machines become increasingly capable, tasks considered as requiring ""intelligence"" are often removed from the definition, a phenomenon known as the AI effect, leading to the quip, ""AI is whatever hasn't been done yet.""[3] For instance, optical character recognition is frequently excluded from ""artificial intelligence"", having become a routine technology.[4] Modern machine capabilities generally classified as AI include successfully understanding human speech,[5] competing at the highest level in strategic game systems (such as chess and Go),[6] autonomously operating cars, and intelligent routing in content delivery networks and military simulations.Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,[7][8] followed by disappointment and the loss of funding (known as an ""AI winter""),[9][10] followed by new approaches, success and renewed funding.[8][11] For most of its history, AI research has been divided into subfields that often fail to communicate with each other.[12] These sub-fields are based on technical considerations, such as particular goals (e.g. ""robotics"" or ""machine learning""),[13] the use of particular tools (""logic"" or artificial neural networks), or deep philosophical differences.[14][15][16] Subfields have also been based on social factors (particular institutions or the work of particular researchers).[12]The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects.[13] General intelligence is among the field's long-term goals.[17] Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy and many others.The field was founded on the claim that human intelligence ""can be so precisely described that a machine can be made to simulate it"".[18] This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence which are issues that have been explored by myth, fiction and philosophy since antiquity.[19] Some people also consider AI to be a danger to humanity if it progresses unabated.[20] Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.[21]In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.[22][11]",Artificial intelligence,https://en.wikipedia.org/wiki/Artificial_intelligence
,,
"Adsorption is the adhesion of atoms, ions or molecules from a gas, liquid or dissolved solid to a surface.[1] This process creates a film of the adsorbate on the surface of the adsorbent. This process differs from absorption, in which a fluid (the absorbate) is dissolved by or permeates a liquid or solid (the absorbent), respectively.[2] Adsorption is a surface phenomenon, while absorption involves the whole volume of the material. The term sorption encompasses both processes, while desorption is the reverse of it.",Adsorption,https://en.wikipedia.org/wiki/Adsorption
,,
"Acoustics is the branch of physics that deals with the study of all mechanical waves in gases, liquids, and solids including topics such as vibration, sound, ultrasound and infrasound. A scientist who works in the field of acoustics is an acoustician while someone working in the field of acoustics technology may be called an acoustical engineer. The application of acoustics is present in almost all aspects of modern society with the most obvious being the audio and noise control industries.Hearing is one of the most crucial means of survival in the animal world, and speech is one of the most distinctive characteristics of human development and culture. Accordingly, the science of acoustics spreads across many facets of human society?€?music, medicine, architecture, industrial production, warfare and more. Likewise, animal species such as songbirds and frogs use sound and hearing as a key element of mating rituals or marking territories. Art, craft, science and technology have provoked one another to advance the whole, as in many other fields of knowledge. Robert Bruce Lindsay's 'Wheel of Acoustics' is a well accepted overview of the various fields in acoustics.[1]The word ""acoustic"" is derived from the Greek word ??€?????????????????? (akoustikos), meaning ""of or for hearing, ready to hear""[2] and that from ??€?????????????? (akoustos), ""heard, audible"",[3] which in turn derives from the verb ??€???????? (akouo), ""I hear"".[4]The Latin synonym is ""sonic"", after which the term sonics used to be a synonym for acoustics[5] and later a branch of acoustics.[6] Frequencies above and below the audible range are called ""ultrasonic"" and ""infrasonic"", respectively.",Acoustics,https://en.wikipedia.org/wiki/Acoustics
,,
"In computer vision and computer graphics, 3D reconstruction is the process of capturing the shape and appearance of real objects.
This process can be accomplished either by active or passive methods. If the model is allowed to change its shape in time, this is referred to as non-rigid or spatio-temporal reconstruction.",3D reconstruction,https://en.wikipedia.org/wiki/3D_reconstruction
,,
"Aerial photography (or airborne imagery) is the taking of photographs from an aircraft or other flying object.[1] Platforms for aerial photography include fixed-wing aircraft, helicopters, unmanned aerial vehicles (UAVs or ""drones""), balloons, blimps and dirigibles, rockets, pigeons, kites, parachutes, stand-alone telescoping and vehicle-mounted poles. Mounted cameras may be triggered remotely or automatically; hand-held photographs may be taken by a photographer.Aerial photography should not be confused with air-to-air photography, where one or more aircraft are used as chase planes that ""chase"" and photograph other aircraft in flight.",Aerial photography,https://en.wikipedia.org/wiki/Aerial_photography
,,
"An academic degree is a qualification awarded to students upon successful completion of a course of study in higher education, normally at a college or university. These institutions commonly offer degrees at various levels, typically including bachelor's, master?€?s and doctorates, often alongside other academic certificates, and professional degrees. The most common undergraduate degree is the bachelor's degree, although in some countries lower qualifications are titled degrees (e.g. associate degrees in the US or foundation degrees in the UK) while in others a higher-level first degree is more usual.",Academic degree,https://en.wikipedia.org/wiki/Academic_degree
,,
"Advanced driver-assistance systems, or ADAS, are systems to help the driver in the driving process. When designed with a safe human-machine interface, they should increase car safety and more generally road safety.Most road accidents occurred due to the human error. Advanced driver-assistance systems are systems developed to automate, adapt and enhance vehicle systems for safety and better driving. The automated system which is provided by ADAS to the vehicle is proven to reduce road fatalities, by minimizing the human error. [2] Safety features are designed to avoid collisions and accidents by offering technologies that alert the driver to potential problems, or to avoid collisions by implementing safeguards and taking over control of the vehicle. Adaptive features may automate lighting, provide adaptive cruise control and collision avoidance, incorporate satnav/traffic warnings, connect to smartphones, alert driver to other cars or dangers, lane departure warning system, automatic lane centering, or show what is in blind spots.",Advanced driver-assistance systems,https://en.wikipedia.org/wiki/Advanced_driver-assistance_systems
