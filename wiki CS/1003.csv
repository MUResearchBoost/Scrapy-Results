introduction,name,urlofmajors
,,
"A printed circuit board (PCB) mechanically supports and electrically connects electronic components or electrical components using conductive tracks, pads and other features etched from one or more sheet layers of copper laminated onto and/or between sheet layers of a non-conductive substrate. Components are generally soldered onto the PCB to both electrically connect and mechanically fasten them to it.Printed circuit boards are used in all but the simplest electronic products. They are also used in some electrical products, such as passive switch boxes.Alternatives to PCBs include wire wrap and point-to-point construction, both once popular but now rarely used. PCBs require additional design effort to lay out the circuit, but manufacturing and assembly can be automated. Specialized CAD software is available to do much of the work of layout. Mass-producing circuits with PCBs is cheaper and faster than with other wiring methods, as components are mounted and wired in one operation.  Large numbers of PCBs can be fabricated at the same time, and the layout only has to be done once.  PCBs can also be made manually in small quantities, with reduced benefits.PCBs can be single-sided (one copper layer), double-sided (two copper layers on both sides of one substrate layer), or multi-layer (outer and inner layers of copper, alternating with layers of substrate).  Multi-layer PCBs allow for much higher component density, because circuit traces on the inner layers would otherwise take up surface space between components.  The rise in popularity of multilayer PCBs with more than two, and especially with more than four, copper planes was concurrent with the adoption of surface mount technology.  However, multilayer PCBs make repair, analysis, and field modification of circuits much more difficult and usually impractical.The world market for bare PCBs exceeded $60.2 billion in 2014.[1] In 2018, the Global Single Sided Printed Circuit Board Market Analysis Report estimated that the PCB market would reach $79 billion by 2024.[2][3]",Printed circuit board,https://en.wikipedia.org/wiki/Printed_circuit_board
,,
"A system on a chip or system on chip (SoC, /????s??o????si??/ es-oh-SEE or /s??k/ sock) is an integrated circuit (also known as a ""chip"") that integrates all components of a computer or other electronic system. These components typically include a central processing unit (CPU), memory, input/output ports and secondary storage ?€? all on a single substrate.  It may contain digital, analog, mixed-signal, and often radio-frequency signal processing functions, depending on the application. As they are integrated on a single electronic substrate, SoCs consume much less power and take up much less area than multi-chip designs with equivalent functionality. Because of this, SoCs are very common in the mobile computing[1] and edge computing[2] markets. Systems on chip are commonly applied in the areas of embedded systems and the Internet of Things.Systems on Chip are in contrast to the common traditional motherboard-based PC architecture, which separates components based on function and connects them through a central interfacing circuit board, so-termed a ""mother board"". Whereas a motherboard houses and connects detachable or replaceable components such as a CPU, graphics and memory interfaces, hard-disk and USB connectivity, memories (both random access and read only) and secondary storage; SoCs integrate all of these components into a single integrated circuit, as if all these functions were built into the motherboard.More tightly integrated hardware designs improve performance and reduce power consumption and semiconductor die area needed for an equivalent design at the cost of reduced modularity and replaceability of components. By definition, SoC designs are fully or nearly fully integrated across different components. For these reasons, there has been a general trend towards tighter integration of components in the computer hardware industry, in part due to the influence of SoCs and lessons learned from the mobile and embedded computing markets. Systems on Chip can also be viewed as part of a larger trend towards embedded computing and hardware acceleration.An SoC integrates a microcontroller or microprocessor with advanced peripherals like graphics processing unit (GPU), Wi-Fi module, or coprocessor. Similar to how a microcontroller integrates a microprocessor with peripheral circuits and memory, an SoC can be seen as integrating a microcontroller with such advanced peripherals.In general, there are three distinguishable types of SoCs: SoCs built around a microcontroller; SoCs built around a microprocessor, often found in mobile phones; and specialized SoCs designed for specific applications that do not fit into the above two categories. A separate category may be programmable systems-on-chip (PSoC), where some of the internal elements can be reprogrammable in a manner analogous to a field-programmable gate array (FPGA) or a complex programmable logic device (CPLD).",System on a chip,https://en.wikipedia.org/wiki/System_on_a_chip
,,
"In computer science, real-time computing (RTC), or reactive computing describes hardware and software systems subject to a ""real-time constraint"", for example from event to system response.[1] Real-time programs must guarantee response within specified time constraints, often referred to as ""deadlines"".[2]  The correctness of these types of systems depends on their temporal aspects as well as their functional aspects. Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually guarantee a response within any timeframe, although typical or expected response times may be given.A real-time system has been described as one which ""controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time"".[3] The term ""real-time"" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean ""without significant delay"".Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems, and real-time networks, each of which provide essential frameworks on which to build a real-time software application.Systems used for many mission critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes on a vehicle, which must produce maximum deceleration but intermittently stop braking to prevent skidding.[4] Real-time processing fails if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.",Real-time computing,https://en.wikipedia.org/wiki/Real-time_computing
,,
"In telecommunication, a communication protocol is a system of rules that allow two or more entities of a communications system to transmit information via any kind of variation of a physical quantity. The protocol defines the rules, syntax, semantics and synchronization of communication and possible error recovery methods. Protocols may be implemented by hardware, software, or a combination of both.[1][not in citation given]Communicating systems use well-defined formats for exchanging various messages. Each message has an exact meaning intended to elicit a response from a range of possible responses pre-determined for that particular situation. The specified behavior is typically independent of how it is to be implemented. Communication protocols have to be agreed upon by the parties involved.[2] To reach agreement, a protocol may be developed into a technical standard. A programming language describes the same for computations, so there is a close analogy between protocols and programming languages: protocols are to communication what programming languages are to computations.[3]Multiple protocols often describe different aspects of a single communication.  A group of protocols designed to work together are known as a protocol suite; when implemented in software they are a protocol stack.Internet communication protocols are published by the Internet Engineering Task Force (IETF). The IEEE handles wired and wireless networking, and the International Organization for Standardization (ISO) handles other types. The ITU-T handles telecommunication protocols and formats for the public switched telephone network (PSTN). As the PSTN and Internet converge, the standards are also being driven towards convergence.",Communication protocol,https://en.wikipedia.org/wiki/Communication_protocol
,,
"A peripheral device is ""an ancillary device used to put information into and get information out of the computer.""[1]Three categories of peripheral devices exist based on their relationship with the computer:",Peripheral,https://en.wikipedia.org/wiki/Peripheral
,,
"Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining hundreds of thousands of transistors or devices into a single chip. VLSI began in the 1970s when complex semiconductor and communication technologies were being developed. The microprocessor is a VLSI device. Before the introduction of VLSI technology most ICs had a limited set of functions they could perform. An electronic circuit might consist of a CPU, ROM, RAM and other glue logic. VLSI lets IC designers add all of these into one chip.",Very-large-scale integration,https://en.wikipedia.org/wiki/Very-large-scale_integration
,,
"An integrated circuit or monolithic integrated circuit (also referred to as an IC, a chip, or a microchip) is a set of electronic circuits on one small flat piece (or ""chip"") of semiconductor material, normally silicon.  The integration of large numbers of tiny transistors into a small chip results in circuits that are orders of magnitude smaller, cheaper, and faster than those constructed of discrete electronic components. The IC's mass production capability, reliability and building-block approach to circuit design has ensured the rapid adoption of standardized ICs in place of designs using discrete transistors.  ICs are now used in virtually all electronic equipment and have revolutionized the world of electronics. Computers, mobile phones, and other digital home appliances are now inextricable parts of the structure of modern societies, made possible by the small size and low cost of ICs.Integrated circuits were made practical by mid-20th-century technology advancements in semiconductor device fabrication.  Since their origins in the 1960s, the size, speed, and capacity of chips have progressed enormously, driven by technical advances that fit more and more transistors on chips of the same size - a modern chip may have several billion transistors in an area the size of a human fingernail.  These advances, roughly following Moore's law, make computer chips of today possess millions of times the capacity and thousands of times the speed of the computer chips of the early 1970s.ICs have two main advantages over discrete circuits: cost and performance. Cost is low because the chips, with all their components, are printed as a unit by photolithography rather than being constructed one transistor at a time. Furthermore, packaged ICs use much less material than discrete circuits. Performance is high because the IC's components switch quickly and consume comparatively little power because of their small size and close proximity.  The main disadvantage of ICs is the high cost to design them and fabricate the required photomasks.  This high initial cost means ICs are only practical when high production volumes are anticipated.",Integrated circuit,https://en.wikipedia.org/wiki/Integrated_circuit
,,
"An embedded system is a programmed controlling and operating system with a dedicated function within a larger mechanical or electrical system, often with real-time computing constraints.[1][2] It is embedded as part of a complete device often including hardware and mechanical parts. Embedded systems control many devices in common use today.[3] Ninety-eight percent of all microprocessors are manufactured components of embedded systems.[4]Examples of properties of typical embedded computers when compared with general-purpose counterparts are low power consumption, small size, rugged operating ranges, and low per-unit cost. This comes at the price of limited processing resources, which make them significantly more difficult to program and to interact with. However, by building intelligence mechanisms on top of the hardware, taking advantage of possible existing sensors and the existence of a network of embedded units, one can both optimally manage available resources at the unit and network levels as well as provide augmented functions, well beyond those available.[5] For example, intelligent techniques can be designed to manage power consumption of embedded systems.[6]Modern embedded systems are often based on microcontrollers (i.e. CPUs with integrated memory or peripheral interfaces),[7] but ordinary microprocessors (using external chips for memory and peripheral interface circuits) are also common, especially in more-complex systems. In either case, the processor(s) used may be types ranging from general purpose to those specialized in certain class of computations, or even custom designed for the application at hand. A common standard class of dedicated processors is the digital signal processor (DSP).Since the embedded system is dedicated to specific tasks, design engineers can optimize it to reduce the size and cost of the product and increase the reliability and performance. Some embedded systems are mass-produced, benefiting from economies of scale.Embedded systems range from portable devices such as digital watches and MP3 players, to large stationary installations like traffic lights, factory controllers, and largely complex systems like hybrid vehicles, MRI, and avionics. Complexity varies from low, with a single microcontroller chip, to very high with multiple units, peripherals and networks mounted inside a large chassis or enclosure.",Embedded system,https://en.wikipedia.org/wiki/Embedded_system
,,
"Electronic design automation (EDA), also referred to as electronic computer-aided design (ECAD),[1] is a category of software tools for designing electronic systems such as integrated circuits and printed circuit boards. The tools work together in a design flow that chip designers use to design and analyze entire semiconductor chips.  Since a modern semiconductor chip can have billions of components, EDA tools are essential for their design.This article describes EDA specifically with respect to integrated circuits.",Electronic design automation,https://en.wikipedia.org/wiki/Electronic_design_automation
,,
"In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation.[1] In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.[2]",Computer architecture,https://en.wikipedia.org/wiki/Computer_architecture
,,
"Network architecture is the design of a communication network. It is a framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as data formats used. In telecommunication, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated.The network architecture of the Internet is predominantly expressed by its use of the Internet Protocol Suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links.",Network architecture,https://en.wikipedia.org/wiki/Network_architecture
,,
"In computer science, an interpreter is a computer program that directly executes, i.e. performs, instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution:",Interpreter (computing),https://en.wikipedia.org/wiki/Interpreter_(computing)
,,
"In systems engineering, dependability is a measure of a system's availability, reliability, and its maintainability, and maintenance support performance, and, in some cases, other characteristics such as durability, safety and security.[1]  In software engineering, dependability is the ability to provide services that can defensibly be trusted within a time-period.[2] This may also encompass mechanisms designed to increase and maintain the dependability of a system or software.[3]The International Electrotechnical Commission (IEC), via its Technical Committee TC 56 develops and maintains international standards that provide systematic methods and tools for dependability assessment and management of equipment, services, and systems throughout their life cycles.Dependability can be broken down into three elements:Attributes - A way to assess the dependability of a system
Threats - An understanding of the things that can affect the dependability of a system
Means - Ways to increase a system's dependability",Dependability,https://en.wikipedia.org/wiki/Dependability
,,
"Networking hardware, also known as network equipment or computer networking devices, are physical devices which are required for communication and interaction between devices on a computer network. Specifically, they mediate data in a computer network.[1] Units which are the last receiver or generate data are called hosts or data terminal equipment.",Networking hardware,https://en.wikipedia.org/wiki/Networking_hardware
,,
"Green computing, green ICT as per International Federation of Global & Green ICT ""IFGICT"", green IT, or ICT sustainability, is the study and practice of environmentally sustainable computing or IT.The goals of green computing are similar to green chemistry: reduce the use of hazardous materials, maximize energy efficiency during the product's lifetime,  the recyclability or biodegradability of defunct products and factory waste. Green computing is important for all classes of systems, ranging from handheld systems[1] to large-scale data centers.[2]Many corporate IT departments have green computing initiatives to reduce the environmental effect of their IT operations.[3]",Green computing,https://en.wikipedia.org/wiki/Green_computing
,,
"Open-source software (OSS) is a type of computer software whose source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose.[1] Open-source software may be developed in a collaborative public manner. According to scientists who studied it, open-source software is a prominent example of open collaboration.[2]  The term is often written without a hyphen as ""open source software"".[3][4][5]Open-source software development, or collaborative development between multiple independent contributors, generates an increasingly more diverse scope of design perspective than any one company is capable of developing and sustaining long term.[citation needed] A 2008 report by the Standish Group states that adoption of open-source software models has resulted in savings of about $60??billion (??48 billion) per year to consumers.[6][7]",Open-source software,https://en.wikipedia.org/wiki/Open-source_software
,,
"In computer networking, a network service is an application running at the network application layer and above, that provides data storage, manipulation, presentation, communication or other capability which is often implemented using a client-server or peer-to-peer architecture based on application layer network protocols.Each service is usually provided by a server component running on one or more computers (often a dedicated server computer offering multiple services) and accessed via a network by client components running on other devices. However, the client and server components can both be run on the same machine.Clients and servers will often have a user interface, and sometimes other hardware associated with it.",Network service,https://en.wikipedia.org/wiki/Network_service
,,
"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity.Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).The main branches of computational geometry are:Combinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term ""computational geometry"" in this sense by 1975.[1]
Numerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term ""computational geometry"" in this meaning has been in use since 1971.[2]",Computational geometry,https://en.wikipedia.org/wiki/Computational_geometry
,,
"Mathematical software is software used to model, analyze or calculate numeric, symbolic or geometric data.[1]It is a type of application software which is used for solving mathematical problems or mathematical study. There are various views to what is the mathematics, so there is various views of the category of mathematical software which used for them, over from narrow to wide sense.A type of mathematical software (math library) also used by built in the part of an another scientific software. A most primary them (for example, to calculate elementary function by floating point arithmetic) may be in the category of mathematical software. They are often usually built in the general purpose systems as middleware. So to speak, mathematical software is not only an application software but also basis of another scientific software. And that is its one of the characteristic of mathematical software as that mean. 
Several mathematical software often have good user interface for educational purpose (see educational math software). But the core parts of solver of them direct dependent to the algorism by the mathematical knowledge. So it may be common sense that it does not process if it not well solved on mathematical construction at least. (There is physical limitation of hardware.) That is typical difference of mathematical software for another application software.
Specially, It may be sure common sense that to the attention that there is a such as next case in mathematical software using:",Mathematical software,https://en.wikipedia.org/wiki/Mathematical_software
,,
"Network performance refers to measures of service quality of a network as seen by the customer.There are many different ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled and stimulated instead of measured; one example of this is using state transition diagrams to model queuing performance or to use a Network Stimulator.",Network performance,https://en.wikipedia.org/wiki/Network_performance
,,
"Statistics is a branch of mathematics dealing with the collection, organization, analysis, interpretation and presentation of data.[1][2] In applying statistics to, for example, a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as ""all people living in a country"" or ""every atom composing a crystal"". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1]
See glossary of probability and statistics.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[3] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.A standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a ""false positive"") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a ""false negative"").[4] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. In more recent years statistics has relied more on statistical software to produce tests such as descriptive analysis.[5]",Statistics,https://en.wikipedia.org/wiki/Statistics
,,
"A database is an organized collection of data, stored and accessed electronically.  Database designers typically organize the data to model aspects of reality in a way that supports processes requiring information, such as (for example) modeling the availability of rooms in hotels in a way that supports finding a hotel with vacancies.The database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze data. A general-purpose DBMS allows the definition, creation, querying, update, and administration of databases. A database is generally stored in a DBMS-specific format which is not portable, but different DBMSs can share data by using standards such as SQL and ODBC or JDBC.  The sum total of the database, the DBMS and its associated applications can be referred to as a ""database system"".  Often the term ""database"" is used to loosely refer to any of the DBMS, the database system or an application associated the database.Computer scientists may classify database-management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data.  In the 2000s, non-relational databases became popular, referred to as NoSQL because they use different query languages.",Database,https://en.wikipedia.org/wiki/Database#Database_management_system
,,
"A software repository, colloquially known as a ""repo"" for short, is a storage location from which software packages may be retrieved and installed on a computer.",Software repository,https://en.wikipedia.org/wiki/Software_repository
,,
"A geographic information system (GIS) is a system designed to capture, store, manipulate, analyze, manage, and present spatial or geographic data. GIS applications are tools that allow users to create interactive queries (user-created searches), analyze spatial information, edit data in maps, and present the results of all these operations.[1][2] GIS (more commonly GIScience) sometimes refers to  geographic information science (GIScience),  the science underlying geographic concepts, applications, and systems.[3]GIS can refer to a number of different technologies, processes, and methods. It is attached to many operations and has many applications related to engineering, planning, management, transport/logistics, insurance, telecommunications, and business.[2] For that reason, GIS and location intelligence applications can be the foundation for many location-enabled services that rely on analysis and visualization.GIS can relate unrelated information by using location as the key index variable. Locations or extents in the Earth space?€?time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude, latitude, and elevation, respectively. All Earth-based spatial?€?temporal location and extent references should be relatable to one another and ultimately to a ""real"" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry.",Geographic information system,https://en.wikipedia.org/wiki/Geographic_information_system
,,
"A Multimedia database (MMDB) is a collection of related multimedia data.[1] The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.A Multimedia Database Management System (MMDBMS) is a framework that manages different types of data potentially represented in a wide diversity of formats on a wide array of media sources. It provides support for multimedia data types, and facilitate for creation, storage, access, query and control of a multimedia database.[2]",Multimedia database,https://en.wikipedia.org/wiki/Multimedia_database
,,
"A decision support system (DSS) is an information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance?€?i.e. unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both.While academics have perceived DSS as a tool to support decision making process, DSS users see DSS as a tool to facilitate organizational processes.[1] Some authors have extended the definition of DSS to include any system that might support decision making and some DSS include a decision-making software component; Sprague (1980)[2] defines a properly termed DSS as follows:",Decision support system,https://en.wikipedia.org/wiki/Decision_support_system
,,
"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st??century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. As an aspect of mathematics and computer science that generates, analyzes, and implements algorithms, the growth in power and the revolution in computing has raised the use of realistic mathematical models in science and engineering, and complex numerical analysis is required to provide solutions to these more involved models of the world. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.Before the advent of modern computers, numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.One of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.[2]Numerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of the square root of 2, modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.",Numerical analysis,https://en.wikipedia.org/wiki/Numerical_analysis
,,
"Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.[1]:15?€?16The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy,[1]:468?€?473 which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Generally the fast volatile technologies (which lose data when off power) are referred to as ""memory"", while slower persistent technologies are referred to as ""storage"".In the Von Neumann architecture, the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.",Computer data storage,https://en.wikipedia.org/wiki/Computer_data_storage
,,
"Social software, also known as Web 2.0 applications or social apps, include communication and interactive tools often based on the Internet. Communication tools typically handle the capturing, storing and presentation of communication, usually written but increasingly including audio and video as well. Interactive tools handle mediated interactions between a pair or group of users. They focus on establishing and maintaining a connection among users, facilitating the mechanics of conversation and talk.[1] Although we do not have a generally accepted definition, social software generally refers to software that makes collaborative behaviour, the organisation and moulding of communities, self-expression, social interaction and feedback possible for individuals. Another important element of the existing definition of ""social software"" is that it allows for the structured mediation of opinion between people, in a centralized or self-regulating manner. The most improved area for social software is that Web 2.0 applications can all promote cooperation between people and the creation of online communities more than ever before.",Social software,https://en.wikipedia.org/wiki/Social_software
,,
"An enterprise information system (EIS) is any kind of information system which improves the functions of enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of data and capable of supporting some large and possibly complex organization or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.[1]The word enterprise can have various connotations. Frequently the term is used only to refer to very large organizations such as multi-national companies or public-sector organizations. However, the term may be used to mean virtually anything, by virtue of it having become the latest corporate-speak buzzword.[citation needed]",Enterprise information system,https://en.wikipedia.org/wiki/Enterprise_information_system
,,
"Automatic process control in continuous production processes is a combination of control engineering and chemical engineering disciplines that uses industrial control systems  to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as oil refining, pulp and paper manufacturing, chemical processing and power generating  plants.There is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large automatic process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.The applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops.",Process control,https://en.wikipedia.org/wiki/Process_control
,,
"A digital library, digital repository, or digital collection, is an online database of digital objects that can include text, still images, audio, video, or other digital media formats.  Objects can consist of digitized content like print or photographs, as well as born-digital content like word processor files or social media posts.  In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection.Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations.[1] The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through interoperability and sustainability.[2]",Digital library,https://en.wikipedia.org/wiki/Digital_library
,,
"A computing platform or digital platform[1] is the environment in which a piece of software is executed. It may be the hardware or the operating system (OS), even a web browser and associated application programming interfaces, or other underlying software, as long as the program code is executed with it. Computing platforms have different abstraction levels, including a computer architecture, an OS, or runtime libraries.[2] A computing platform is the stage on which computer programs can run.A platform can be seen both as a constraint on the software development process, in that different platforms provide different functionality and restrictions; and as an assistance to the development process, in that they provide low-level functionality ready-made.  For example, an OS may be a platform that abstracts the underlying differences in hardware and provides a generic command for saving files or accessing the network.",Computing platform,https://en.wikipedia.org/wiki/Computing_platform
,,
"Data mining  is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]The term ""data mining"" is in fact a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics ?€? or, when referring to actual methods, artificial intelligence and machine learning ?€? are more appropriate.The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.",Data mining,https://en.wikipedia.org/wiki/Data_mining
,,
"Mathematical analysis is the branch of mathematics dealing with limits
and related theories, such as differentiation, integration, measure, infinite series, and analytic functions.[1][2]These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.
Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).",Mathematical analysis,https://en.wikipedia.org/wiki/Mathematical_analysis
,,
"Information theory studies the quantification, storage, and communication of information.  It was originally proposed by Claude E. Shannon in 1948 to find fundamental limits on signal processing and communication operations such as data compression, in a landmark paper entitled ""A Mathematical Theory of Communication"". Applications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for digital subscriber line (DSL)). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields.A key measure in information theory is ""entropy"". Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy.The field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. The theory has also found applications in other areas, including statistical inference, natural language processing, cryptography, neurobiology,[1] human vision,[2] the evolution[3] and function[4] of molecular codes (bioinformatics), model selection in statistics,[5] thermal physics,[6] quantum computing, linguistics, plagiarism detection,[7] pattern recognition, and anomaly detection.[8] Important sub-fields of information theory include source coding, channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.",Information theory,https://en.wikipedia.org/wiki/Information_theory
,,
"Probability is the measure of the likelihood that an event will occur.[1] See glossary of probability and statistics. Probability is quantified as a number between 0 and 1, where, loosely speaking,[2] 0 indicates impossibility and 1 indicates certainty.[3][4] The higher the probability of an event, the more likely it is that the event will occur.     A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (""heads"" and ""tails"") are both equally probable; the probability of ""heads"" equals the probability of ""tails""; and since no other outcomes are possible, the probability  of either ""heads"" or ""tails"" is 1/2 (which could also be written as 0.5 or 50%).These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.[5]",Probability,https://en.wikipedia.org/wiki/Probability
,,
"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the ""average case"" over all possible choices of random bits. Formally, the algorithm's performance will be a random variable determined by the random bits; thus either the running time, or the output (or both) are random variables.One has to distinguish between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite (Las Vegas algorithms, example of which is Quicksort[1]), and algorithms which have a chance of producing an incorrect result (Monte Carlo algorithms, example of which is Monte Carlo algorithm for MFAS[2]) or fail to produce a result either by signaling a failure or failing to terminate.In the second case, random performance and random output, the term ""algorithm"" for a procedure is somewhat questionable.  In the case of random output, it is no longer formally effective.[3]
However, in some cases, probabilistic algorithms are the only practical means of solving a problem.[4]In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.",Randomized algorithm,https://en.wikipedia.org/wiki/Randomized_algorithm
,,
"Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying ""smoothly"", the objects studied in discrete mathematics ?€? such as integers, graphs, and statements in logic[1] ?€? do not vary smoothly in this way, but have distinct, separated values.[2] Discrete mathematics therefore excludes topics in ""continuous mathematics"" such as calculus or Euclidean geometry. Discrete objects can often be enumerated by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets[3] (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term ""discrete mathematics.""[4] Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notions.The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in discrete steps and store data in discrete bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems, such as in operations research.Although the main objects of study in discrete mathematics are discrete objects, analytic methods from continuous mathematics are often employed as well.In university curricula, ""Discrete Mathematics"" appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in freshmen; therefore it is nowadays a prerequisite for mathematics majors in some universities as well.[5][6] Some high-school-level discrete mathematics textbooks have appeared as well.[7] At this level, discrete mathematics is sometimes seen as a preparatory course, not unlike precalculus in this respect.[8]The Fulkerson Prize is awarded for outstanding papers in discrete mathematics.",Discrete mathematics,https://en.wikipedia.org/wiki/Discrete_mathematics
,,
"Cryptography or cryptology (from Ancient Greek: ???????€??????, translit.??krypt??s ""hidden, secret""; and ?????????????? graphein, ""to write"", or -?????????? -logia, ""study"", respectively[1]) is the practice and study of techniques for secure communication in the presence of third parties called adversaries.[2] More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages;[3] various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation[4] are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.Cryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message shared the decoding technique needed to recover the original information only with intended recipients, thereby precluding unwanted persons from doing the same. The cryptography literature often uses the name Alice (""A"") for the sender, Bob (""B"") for the intended recipient, and Eve (""eavesdropper"") for the adversary.[5] Since the development of rotor cipher machines in World War??I and the advent of computers in World War??II, the methods used to carry out cryptology have become increasingly complex and its application more widespread.Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that probably cannot be broken even with unlimited computing power?€?an example is the one-time pad?€?but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.The growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export.[6] In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation.[7][8] Cryptography also plays a major role in digital rights management and copyright infringement of digital media.[9]",Cryptography,https://en.wikipedia.org/wiki/Cryptography
,,
"Information retrieval (IR) is the activity of obtaining information system resources relevant to an information need from a collection of information resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for metadata that describe data, and for databases of texts, images or sounds.Automated information retrieval systems are used to reduce what has been called information overload. An IR systems is a software that provide access to books, journals and other documents, stores them and manages the document. Web search engines are the most visible IR applications.",Information retrieval,https://en.wikipedia.org/wiki/Information_retrieval
,,
"The World Wide Web (WWW), also called the Web, is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and accessible via the Internet.[1] English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser in 1990 while employed at CERN in Switzerland.[2][3] The browser was released outside CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991.The World Wide Web has been central to the development of the Information Age and is the primary tool billions of people use to interact on the Internet.[4][5][6] Web pages are primarily text documents formatted and annotated with Hypertext Markup Language (HTML).[7] In addition to formatted text, web pages may contain images, video, audio, and software components that are rendered in the user's web browser as coherent pages of multimedia content.Embedded hyperlinks permit users to navigate between web pages. Multiple web pages with a common theme, a common domain name, or both, make up a website. Website content can largely be provided by the publisher, or interactively where users contribute content or the content depends upon the users or their actions. Websites may be mostly informative, primarily for entertainment, or largely for commercial, governmental, or non-governmental organisational purposes.",World Wide Web,https://en.wikipedia.org/wiki/World_Wide_Web
,,
"Digital marketing is the marketing of products or services using digital technologies, mainly on the Internet, but also including mobile phones, display advertising, and any other digital medium.[1]Digital marketing's development since the 1990s and 2000s has changed the way brands and businesses use technology for marketing.[2] As digital platforms are increasingly incorporated into marketing plans and everyday life,[3] and as people use digital devices instead of visiting physical shops,[4][5] digital marketing campaigns are becoming more prevalent and efficient.Digital marketing methods such as search engine optimization (SEO), search engine marketing (SEM), content marketing, influencer marketing, content automation, campaign marketing, data-driven marketing,[6] e-commerce marketing, social media marketing, social media optimization, e-mail direct marketing, display advertising, e?€?books, and optical disks and games are becoming more common in our advancing technology. In fact, digital marketing now extends to non-Internet channels that provide digital media, such as mobile phones (SMS and MMS), callback, and on-hold mobile ring tones.[7] In essence, this extension to non-Internet channels helps to differentiate digital marketing from online marketing, another catch-all term for the marketing methods mentioned above, which strictly occur online.",Digital marketing,https://en.wikipedia.org/wiki/Digital_marketing
,,
"In computer science, algorithmic efficiency is a property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.For maximum efficiency we wish to minimize resource usage. However, different resources such as time and space complexity cannot be compared directly, so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important.For example, bubble sort and timsort are both algorithms to sort a list of items from smallest to largest. Bubble sort sorts the list in time proportional to the number of elements squared (
  
    
      
        
          
            
              
                O
              
            
            
              (
              
                n
                
                  2
                
              
              )
            
          
        
      
    
    {\displaystyle \scriptstyle {{\mathcal {O}}\left(n^{2}\right)}}
  
, see Big O notation), but only requires a small amount of extra memory which is constant with respect to the length of the list (
  
    
      
        
          
            
              
                O
              
            
            
              (
              1
              )
            
          
        
      
    
    {\textstyle \scriptstyle {{\mathcal {O}}\left(1\right)}}
  
). Timsort sorts the list in time linearithmic (proportional to a quantity times its logarithm) in the list's length (
  
    
      
        
          
            
              O
              
                (
                
                  n
                  log
                  ???
                  n
                
                )
              
            
          
        
      
    
    {\textstyle \scriptstyle {\mathcal {O\left(n\log n\right)}}}
  
), but has a space requirement linear in the length of the list (
  
    
      
        
          
            
              O
              
                (
                n
                )
              
            
          
        
      
    
    {\textstyle \scriptstyle {\mathcal {O\left(n\right)}}}
  
). If large lists must be sorted at high speed for a given application, timsort is a better choice; however, if minimizing the memory footprint of the sorting is more important, bubble sort is a better choice.",Algorithmic efficiency,https://en.wikipedia.org/wiki/Algorithmic_efficiency
,,
"In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.Formal semantics, for instance, helps to write compilers, better understand what a program is doing and to prove, e.g., that the following if statementhas the same effect as S1 alone.",Semantics (computer science),https://en.wikipedia.org/wiki/Semantics_(computer_science)
,,
"In computer science, the analysis of algorithms is the determination of the computational complexity of algorithms, that is the amount of time, storage and/or other resources necessary to execute them. Usually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same length may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.The term ""analysis of algorithms"" was coined by Donald Knuth.[1] Algorithm analysis is an important part of a broader computational complexity theory, which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search for efficient algorithms.In theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. Big O notation, Big-omega notation and Big-theta notation are used to this end. For instance, binary search is said to run in a number of steps proportional to the logarithm of the length of the sorted list being searched, or in O(log(n)), colloquially ""in logarithmic time"". Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency. However the efficiencies of any two ""reasonable"" implementations of a given algorithm are related by a constant multiplicative factor  called a hidden constant.Exact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called model of computation. A model of computation may be defined in terms of an abstract computer, e.g., Turing machine, and/or by postulating that certain operations are executed in unit time.
For example, if the sorted list to which we apply binary search has n elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log2 n + 1 time units are needed to return an answer.",Analysis of algorithms,https://en.wikipedia.org/wiki/Analysis_of_algorithms
,,
"In mathematics and computer science, an algorithm (/????l????r??????m/??(??listen)) is an unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing and automated reasoning tasks.As an effective method, an algorithm can be expressed within a finite amount of space and time[1] and in a well-defined formal language[2] for calculating a function.[3] Starting from an initial state and initial input (perhaps empty),[4] the instructions describe a computation that, when executed, proceeds through a finite[5] number of well-defined successive states, eventually producing ""output""[6] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[7]The concept of algorithm has existed for centuries and the use of the concept can be ascribed to Greek mathematicians, e.g. the sieve of Eratosthenes and Euclid's algorithm;[8] the term algorithm itself derives from the 9th Century mathematician Mu???ammad ibn M??s?? al'Khw??rizm??, Latinized 'Algoritmi'. A partial formalization of what would become the modern notion of algorithm began with attempts to solve the Entscheidungsproblem (the ""decision problem"") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define ""effective calculability""[9] or ""effective method"";[10] those formalizations included the G??del?€?Herbrand?€?Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936?€?7 and 1939.",Algorithm,https://en.wikipedia.org/wiki/Algorithm#Design
,,
"An intrusion detection system (IDS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any malicious activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources, and uses alarm filtering techniques to distinguish malicious activity from false alarms.[citation needed]IDS types range in scope from single computers to large networks.[1] The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS). A system that monitors important operating system files is an example of an HIDS, while a system that analyzes incoming network traffic is an example of an NIDS. It is also possible to classify IDS by detection approach: the most well-known variants are signature-based detection (recognizing bad patterns, such as malware); and anomaly-based detection (detecting deviations from a model of ""good"" traffic, which often relies on machine learning). Some IDS products have the ability to respond to detected intrusions. Systems with response capabilities are typically referred to as an intrusion prevention system.[citation needed]",Intrusion detection system,https://en.wikipedia.org/wiki/Intrusion_detection_system
,,
"Computer security compromised by hardware failure is a branch of computer security applied to hardware.
The objective of computer security includes protection of information and property from theft, corruption, or natural disaster, while allowing the information and property to remain accessible and productive to its intended users.[1] Such secret information could be retrieved by different ways. This article focus on the retrieval of data thanks to misused hardware or hardware failure. Hardware could be misused or exploited to get secret data. This article collects main types of attack that can be lead in a data thief.Computer security can be comprised by devices, such as keyboards, monitors or printers (thanks to electromagnetic or acoustic emanation for example) or by components of the computer, such as the memory, the network card or the processor (thanks to time or temperature analysis for example).",Computer security compromised by hardware failure,https://en.wikipedia.org/wiki/Computer_security_compromised_by_hardware_failure
,,
"Interaction design, often abbreviated as IxD, is ""the practice of designing interactive digital products, environments, systems, and services.""[1]:xxxi,1 Beyond the digital aspect, interaction design is also useful when creating physical (non-digital) products, exploring how a user might interact with it. Common topics of interaction design include design, human?€?computer interaction, and software development. While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior.[1]:1 Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what characterizes IxD as a design field as opposed to a science or engineering field.[1]:xviiiWhile disciplines such as software engineering have a heavy focus on designing for technical stakeholders,  interaction design is geared toward satisfying the majority of users.[1]:xviii",Interaction design,https://en.wikipedia.org/wiki/Interaction_design
,,
"Application security encompasses measures taken to improve the security of an application often by finding, fixing and preventing security vulnerabilities.Different techniques are used to surface such security vulnerabilities at different stages of an applications lifecycle such design, development, deployment, upgrade, maintenance.An always evolving but largely consistent set of common security flaws are seen across different applications, see common flaws",Application security,https://en.wikipedia.org/wiki/Application_security
,,
"Network security consists of the policies and practices adopted to prevent and monitor unauthorized access, misuse, modification, or denial of a computer network and network-accessible resources. Network security involves the authorization of access to data in a network, which is controlled by the network administrator. Users choose or are assigned an ID and password or other authenticating information that allows them access to information and programs within their authority.  Network security covers a variety of computer networks, both public and private, that are used in everyday jobs; conducting transactions and communications among businesses, government agencies and individuals. Networks can be private, such as within a company, and others which might be open to public access. Network security is involved in organizations, enterprises, and other types of institutions. It does as its title explains: It secures the network, as well as protecting and overseeing operations being done. The most common and simple way of protecting a network resource is by assigning it a unique name and a corresponding  password.",Network security,https://en.wikipedia.org/wiki/Network_security
,,
"Information security, sometimes shortened to InfoSec, is the practice of preventing unauthorized access, use, disclosure, disruption, modification, inspection, recording or destruction of information. The information or data may take any form, e.g. electronic or physical.[1] Information security's primary focus is the balanced protection of the confidentiality, integrity and availability of data (also known as the CIA triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[2] This is largely achieved through a multi-step risk management process that identifies assets, threat sources, vulnerabilities, potential impacts, and possible controls, followed by assessment of the effectiveness of the risk management plan.To standardize this discipline, academics and professionals collaborate and seek to set basic guidance, policies, and industry standards on password, antivirus software, firewall, encryption software, legal liability and user/administrator training standards.[3] This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, and transferred. However, the implementation of any standards and guidance within an entity may have limited effect if a culture of continual improvement isn't adopted.[4]",Information security,https://en.wikipedia.org/wiki/Information_security
,,
"Security service is a service, provided by a layer of communicating open systems, which ensures adequate security of the systems or of data transfers[1] as defined by ITU-T X.800 Recommendation. 
X.800 and ISO 7498-2 (Information processing systems ?€? Open systems interconnection ?€? Basic Reference Model ?€? Part 2: Security architecture)[2]  are technically aligned. This model is widely recognized [3]
[4]A more general definition is in CNSS Instruction No. 4009 dated 26 April 2010 by Committee on National Security Systems of United States of America:[5]",Security service (telecommunication),https://en.wikipedia.org/wiki/Security_service_(telecommunication)
,,
"In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically based techniques for the specification, development and verification of software and hardware systems.[1] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.[2]Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, discrete event dynamic system and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.[3]",Formal methods,https://en.wikipedia.org/wiki/Formal_methods
,,
"Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:Theoretical foundations and analysis
Use of computer technology to aid logicians
Use of concepts from logic for computer applications",Logic in computer science,https://en.wikipedia.org/wiki/Logic_in_computer_science
,,
"Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.",Social computing,https://en.wikipedia.org/wiki/Social_computing
,,
"Ubiquitous computing (or ""ubicomp"") is a concept in software engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets and terminals in everyday objects such as a refrigerator or a pair of glasses.  The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, networks, mobile protocols, location and positioning, and new materials.This paradigm is also described as pervasive computing,[1] ambient intelligence,[2] or ""everyware"".[3] Each term emphasizes slightly different aspects. When primarily concerning the objects involved, it is also known as physical computing, the Internet of Things, haptic computing,[4] and ""things that think"".
Rather than propose a single definition for ubiquitous computing and for these related terms, a taxonomy of properties for ubiquitous computing has been proposed, from which different kinds or flavors of ubiquitous systems and applications can be described.[5]Ubiquitous computing touches on a wide range of research topics, including distributed computing, mobile computing, location computing, mobile networking, context-aware computing, sensor networks, human?€?computer interaction, and artificial intelligence.",Ubiquitous computing,https://en.wikipedia.org/wiki/Ubiquitous_computing
,,
"In mathematics, computer science, and linguistics, a formal language is a set of strings of symbols together with a set of rules that are specific to it.The alphabet of a formal language is the set of symbols, letters, or tokens from which the strings of the language may be formed.[1] The strings formed from this alphabet are called words, and the words that belong to a particular formal language are sometimes called well-formed words or well-formed formulas. A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, also called its formation rule.The field of formal language theory studies primarily the purely syntactical aspects of such languages?€?that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.
In computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.",Formal language,https://en.wikipedia.org/wiki/Formal_language
,,
"In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how a set of outputs are computed given a set of inputs. This model describes how units of computations, memories, and communications are organized. The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.",Model of computation,https://en.wikipedia.org/wiki/Model_of_computation
,,
"Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science). The word automata (the plural of automaton) comes from the Greek word ?????????????????, which means ""self-acting"".The figure at right illustrates a finite-state machine, which belongs to a well-known type of automaton.  This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows).  As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the current state and the recent symbol as its inputs.Automata theory is closely related to formal language theory. An automaton is a finite representation of a formal language that may be an infinite set. Automata are often classified by the class of formal languages they can recognize, typically illustrated by the Chomsky hierarchy, which describes the relations between various languages and kinds of formalized logic.Automata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification.",Automata theory,https://en.wikipedia.org/wiki/Automata_theory
,,
"Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating the resulting complexity classes to each other.[1] A computational problem is understood to be a task that is in principle amenable to being solved by mechanical application of mathematical steps, such as an algorithm, which is equivalent to stating that the problem may be solved by a computer.A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millenium Prize Problems, is dedicated to the field of computational complexity.[2]Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.",Computational complexity theory,https://en.wikipedia.org/wiki/Computational_complexity_theory
,,
"In human?€?computer interaction, computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability type or severity of impairment. The term accessibility is most often used in reference to specialized hardware or software, or a combination of both, designed to enable use of a computer by a person with a disability or impairment. Specific technologies may be referred to as assistive technology.There are many disabilities or impairments that can be a barrier to effective computer use. These impairments, which can be acquired from disease, trauma, or may be congenital, include but are not limited to:Cognitive impairments (head injury, autism, developmental disabilities) and learning disabilities (such as dyslexia, dyscalculia or ADHD).
Visual impairment such as low-vision, complete or partial blindness, and color blindness.
Hearing-related disabilities including deafness, being hard of hearing, or hyperacusis.
Motor or dexterity impairment such as paralysis, cerebral palsy, dyspraxia, carpal tunnel syndrome and repetitive strain injury.",Computer accessibility,https://en.wikipedia.org/wiki/Computer_accessibility
,,
"In computer architecture, multithreading is the ability of a central processing unit (CPU) (or a single core in a multi-core processor) to execute multiple processes or threads concurrently, supported by the operating system. This approach differs from multiprocessing. In a multithreaded application, the processes and threads share the resources of a single or multiple cores, which include the computing units, the CPU caches, and the translation lookaside buffer (TLB).Where multiprocessing systems include multiple complete processing units in one or more cores, multithreading aims to increase utilization of a single core by using thread-level parallelism, as well as instruction-level parallelism. As the two techniques are complementary, they are sometimes combined in systems with multiple multithreading CPUs and with CPUs with multiple multithreading cores.",Multithreading (computer architecture),https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)
,,
"Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which then communicate and coordinate their actions by passing messages to one other.[1] The components interact with one other in order to achieve a common goal. Three significant characteristics[why?] of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.[1] Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.A computer program that runs within a distributed system is called a distributed program (and distributed programming is the process of writing such programs).[2] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues[3].Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[4] which communicate with each other via message passing.[5]",Distributed computing,https://en.wikipedia.org/wiki/Distributed_computing
,,
"Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but it's gaining broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]Parallel computing is closely related to concurrent computing?€?they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU).[5][6] In parallel computing, a computational task is typically broken down into several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones,[7] because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.",Parallel computing,https://en.wikipedia.org/wiki/Parallel_computing
,,
"Concurrent computing is a form of computing in which several computations are executed during overlapping time periods?€?concurrently?€?instead of sequentially (one completing before the next starts). This is a property of a system?€?this may be an individual program, a computer, or a network?€?and there is a separate execution point or ""thread of control"" for each computation (""process""). A concurrent system is one where a computation can advance without waiting for all other computations to complete.[1]As a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.",Concurrent computing,https://en.wikipedia.org/wiki/Concurrent_computing
,,
"A programming team is a team of people who develop or maintain computer software.[1]  They may be organised in numerous ways, but the egoless programming team and chief programmer team are two common structures typically used.[2]",Programming team,https://en.wikipedia.org/wiki/Programming_team
,,
"Visualization or visualisation (see spelling differences) is any technique for creating images, diagrams, or animations to communicate a message. Visualization through visual imagery has been an effective way to communicate both abstract and concrete ideas since the dawn of humanity. Examples from history include cave paintings, Egyptian hieroglyphs, Greek geometry, and Leonardo da Vinci's revolutionary methods of technical drawing for engineering and scientific purposes.Visualization today has ever-expanding applications in science, education, engineering (e.g., product visualization), interactive multimedia, medicine, etc. Typical of a visualization application is the field of computer graphics. The invention of computer graphics may be the most important development in visualization since the invention of central perspective in the Renaissance period. The development of animation also helped advance visualization.",Visualization (graphics),https://en.wikipedia.org/wiki/Visualization_(graphics)
,,
"Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.[1]A common perception of maintenance is that it merely involves fixing defects. However, one study indicated that over 80% of maintenance effort is used for non-corrective actions.[2] This perception is perpetuated by users submitting problem reports that in reality are functionality enhancements to the system.[citation needed] More recent studies put the bug-fixing proportion closer to 21%.[3]",Software maintenance,https://en.wikipedia.org/wiki/Software_maintenance
,,
"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language??data.Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.",Natural language processing,https://en.wikipedia.org/wiki/Natural_language_processing
,,
"Software deployment is all of the activities that make a software system available for use.The general deployment process consists of several interrelated activities with possible transitions between them. These activities can occur at the producer side or at the consumer side or both. Because every software system is unique, the precise processes or procedures within each activity can hardly be defined. Therefore, ""deployment"" should be interpreted as a general process that has to be customized according to specific requirements or characteristics.",Software deployment,https://en.wikipedia.org/wiki/Software_deployment
,,
"In systems engineering and software engineering, requirements analysis encompasses those tasks that go into determining the needs or conditions to meet for a new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.[2]Requirements analysis is critical to the success or failure of a systems or software project.[3] The requirements should be documented, actionable, measurable, testable, traceable, related to identified business needs or opportunities, and defined to a level of detail sufficient for system design.",Requirements analysis,https://en.wikipedia.org/wiki/Requirements_analysis
,,
"Artificial intelligence has close connections with philosophy because both share several concepts and these include intelligence, action, consciousness, epistemology, and even free will.[1] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures) so the discipline is of considerable interest to philosophers.[2] These factors contributed to the emergence of the philosophy of artificial intelligence. Some scholars argue that the AI community's dismissal of philosophy is detrimental.The philosophy of artificial intelligence attempts to answer such questions as follows:[3]Can a machine act intelligently? Can it solve any problem that a person would solve by thinking?
Are human intelligence and machine intelligence the same?  Is the human brain essentially a computer?
Can a machine have a mind, mental states, and consciousness in the same way that a human being can? Can it feel how things are?",Philosophy of artificial intelligence,https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence
,,
"In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.  It is also known as a software development life cycle.  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.[1]Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.Some people consider a life-cycle ""model"" a more general term for a category of methodologies and a software development ""process"" a more specific term to refer to a specific process chosen by a specific organization. For example, there are many specific software development processes that fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.",Software development process,https://en.wikipedia.org/wiki/Software_development_process
,,
"Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system.[1][2] The term also refers to the ability of a system to support more than one processor or the ability to allocate tasks between them. There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined (multiple cores on one die, multiple dies in one package, multiple packages in one system unit, etc.).According to some on-line dictionaries, a multiprocessor is a computer system having two or more processing units (multiple processors) each sharing main memory and peripherals, in order to simultaneously process programs.[3][4] A 2009 textbook defined multiprocessor system similarly, but noting that the processors may share ""some or all of the system?€?s memory and I/O facilities""; it also gave tightly coupled system as a synonymous term.[5]At the operating system level, multiprocessing is sometimes used to refer to the execution of multiple concurrent processes in a system, with each process running on a separate CPU or core, as opposed to a single process at any one instant.[6][7] When used with this definition, multiprocessing is sometimes contrasted with multitasking, which may use just a single processor but switch it in time slices between tasks (i.e. a time-sharing system). Multiprocessing however means true parallel execution of multiple processes using more than one processor.[7] Multiprocessing doesn't necessarily mean that a single process or task uses more than one processor simultaneously; the term parallel processing is generally used to denote that scenario.[6] Other authors prefer to refer to the operating system techniques as multiprogramming and reserve the term multiprocessing for the hardware aspect of having more than one processor.[2][8] The remainder of this article discusses multiprocessing only in this hardware sense.In Flynn's taxonomy, multiprocessors as defined above are MIMD machines.[9][10] As they are normally construed to be tightly coupled (share memory), multiprocessors are not the entire class of MIMD machines, which also contains message passing multicomputer systems.[9]",Multiprocessing,https://en.wikipedia.org/wiki/Multiprocessing
,,
"Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.[1]",Software construction,https://en.wikipedia.org/wiki/Software_construction
,,
"Software design is the process by which an agent creates a specification of a software artifact, intended to accomplish goals, using a set of primitive components and subject to constraints.[1] Software design may refer to either ""all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems"" or ""the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.""[2]Software design usually involves problem solving and planning a software solution. This includes both a low-level component and algorithm design and a high-level, architecture design.",Software design,https://en.wikipedia.org/wiki/Software_design
,,
"Knowledge representation and reasoning (KR, KR??, KR&R) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology[1] about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.[2]Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.The KR conference series was established to share ideas and progress on this challenging field.[3]",Knowledge representation and reasoning,https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning
,,
"Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming.[1][2]
The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.[citation needed]In machine learning, the environment is typically formulated as a Markov Decision Process (MDP), as many reinforcement learning algorithms for this context utilize dynamic programming techniques.[2][1][3] The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.[2][1]Reinforcement learning differs from standard supervised learning in that correct input/output pairs[clarification needed] need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance[clarification needed], which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).[4] The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.[citation needed]",Reinforcement learning,https://en.wikipedia.org/wiki/Reinforcement_learning
,,
"Unsupervised learning is a branch of machine learning that learns from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. Alternatives include supervised learning and reinforcement learning.A central application of unsupervised learning is in the field of density estimation in statistics,[1] though unsupervised learning encompasses many other domains involving summarizing and explaining data features.",Unsupervised learning,https://en.wikipedia.org/wiki/Unsupervised_learning
,,
"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2]  In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias).The parallel task in human and animal psychology is often referred to as concept learning.",Supervised learning,https://en.wikipedia.org/wiki/Supervised_learning
,,
Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence[1] is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of Multi-Agent Systems.,Distributed artificial intelligence,https://en.wikipedia.org/wiki/Distributed_artificial_intelligence
,,
"In mathematics, computer science and operations research, mathematical optimization or mathematical programming, alternatively spelled optimisation, is the selection of a best element (with regard to some criterion) from some set of available alternatives.[1]In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding ""best available"" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.",Mathematical optimization,https://en.wikipedia.org/wiki/Mathematical_optimization
,,
"Control theory in control systems engineering deals with the control of continuously operating dynamical systems  in engineered processes and machines. The objective is to develop a control model for controlling such systems using a control action in an optimum manner without delay or overshoot and ensuring control stability.To do this, a controller with the requisite corrective behaviour is required. This controller monitors the controlled process variable (PV), and compares it with the reference or set point (SP). The difference between actual and desired value of the process variable, called the error signal, or SP-PV error, is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point. Other aspects which are also studied are  controllability and observability. On this is based the advanced type of automation that revolutionized manufacturing, aircraft, communications and other industries. This is feedback control, which is usually continuous and involves taking measurements using a sensor and making calculated adjustments to keep the measured variable within a set range by means of a ""final control element"", such as a control valve.[1]Extensive use is usually made of a diagrammatic style known as the block diagram. In it the transfer function, also known as the system function or network function, is a mathematical model of the relation between the input and output based on the differential equations describing the system.Control theory dates from the 19th century, when the theoretical basis for the operation of governors was first described by James Clerk Maxwell.[2]  Control theory was further advanced by Edward Routh in 1874, Charles Sturm and in 1895, Adolf Hurwitz, who all contributed to the establishment of control stability criteria; and from 1922 onwards, the development of PID control theory by Nicolas Minorsky.[3]
Although a major application of control theory is in control systems engineering, which deals with the design of process control systems for industry, other applications range far beyond this.  As the general theory of feedback systems, control theory is useful wherever feedback occurs.",Control theory,https://en.wikipedia.org/wiki/Control_theory
,,
"Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[1][2][3]Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[4][5][6][7] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[8]As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, and image restoration.[6]",Computer vision,https://en.wikipedia.org/wiki/Computer_vision
,,
"Automated planning and scheduling, sometimes denoted as simply AI Planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.",Automated planning and scheduling,https://en.wikipedia.org/wiki/Automated_planning_and_scheduling
,,
"Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic compression methods.[1]",Image compression,https://en.wikipedia.org/wiki/Image_compression
,,
"Mixed reality (MR), sometimes referred to as hybrid reality,[1] is the merging of real and virtual worlds to produce new environments and visualizations where physical and digital objects co-exist and interact in real time. Mixed reality takes place not only in the physical world or the virtual world,[1] but is a mix of reality and virtual reality, encompassing both augmented reality and augmented virtuality[2] via immersive technology. The first immersive mixed reality system, providing enveloping sight, sound, and touch was the Virtual Fixtures platform developed at the U.S. Air Force's Armstrong Laboratories in the early 1990s. In a study published in 1992, the Virtual Fixtures project at the U.S. Air Force demonstrated for the first time that human performance could be significantly amplified by the introduction of spatially registered virtual objects overlaid on top of a person's direct view of a real physical environment.[3]",Mixed reality,https://en.wikipedia.org/wiki/Mixed_reality
,,
"Virtual reality (VR) is an interactive computer-generated experience taking place within a simulated environment, that incorporates mainly auditory and visual, but also other types of sensory feedback like haptic. This immersive environment can be similar to the real world or it can be fantastical, creating an experience that is not possible in ordinary physical reality. Augmented reality systems may also be considered a form of VR that layers virtual information over a live camera feed into a headset or through a smartphone or tablet device giving the user the ability to view three-dimensional images.Current VR technology most commonly uses virtual reality headsets or multi-projected environments, sometimes in combination with physical environments or props, to generate realistic images, sounds and other sensations that simulate a user's physical presence in a virtual or imaginary environment. A person using virtual reality equipment is able to ""look around"" the artificial world, move around in it, and interact with virtual features or items. The effect is commonly created by VR headsets consisting of a head-mounted display with a small screen in front of the eyes, but can also be created through specially designed rooms with multiple large screens.VR systems that include transmission of vibrations and other sensations to the user through a game controller or other devices are known as haptic systems. This tactile information is generally known as force feedback in medical, video gaming and military training applications.",Virtual reality,https://en.wikipedia.org/wiki/Virtual_reality
,,
"A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics and image processing, and their highly parallel structure makes them more efficient than general-purpose CPUs for algorithms where the processing of large blocks of data is done in parallel. In a personal computer, a GPU can be present on a video card, or it can be embedded on the motherboard or?€?in certain CPUs?€?on the CPU die.[1]The term GPU was popularized by Nvidia in 1999, who marketed the GeForce 256 as ""the world's first GPU"", or Graphics Processing Unit,[2] although the term had been in use since at least the 1980s.[3] It was presented as a ""single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines"".[4] Rival ATI Technologies coined the term ""visual processing unit"" or VPU with the release of the Radeon 9700 in 2002.[5]",Graphics processing unit,https://en.wikipedia.org/wiki/Graphics_processing_unit
,,
"Rendering or image synthesis is the automatic process of generating a photorealistic or non-photorealistic image from a 2D or 3D model (or models in what collectively could be called a scene file) by means of computer programs. Also, the results of displaying such a model can be called a render. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texture, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term ""rendering"" may be by analogy with an ""artist's rendering"" of a scene.Though the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU. A GPU is a purpose-built device able to assist a CPU in performing complex rendering calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the rendering equation. The rendering equation doesn't account for all lighting phenomena, but is a general lighting model for computer-generated imagery. 'Rendering' is also used to describe the process of calculating effects in a video editing program to produce final video output.Rendering is one of the major sub-topics of 3D computer graphics, and in practice is always connected to the others. In the graphics pipeline, it is the last major step, giving the final appearance to the models and animation. With the increasing sophistication of computer graphics since the 1970s, it has become a more distinct subject.Rendering has uses in architecture, video games, simulators, movie or TV visual effects, and design visualization, each employing a different balance of features and techniques. As a product, a wide variety of renderers are available. Some are integrated into larger modeling and animation packages, some are stand-alone, some are free open-source projects. On the inside, a renderer is a carefully engineered program, based on a selective mixture of disciplines related to: light physics, visual perception, mathematics, and software development.In the case of 3D graphics, rendering may be done slowly, as in pre-rendering, or in realtime.  Pre-rendering is a computationally intensive process that is typically used for movie creation, while real-time rendering is often done for 3D video games which rely on the use of graphics cards with 3D hardware accelerators.",Rendering (computer graphics),https://en.wikipedia.org/wiki/Rendering_(computer_graphics)
,,
"Photo manipulation involves transforming or altering a photograph using various methods and techniques to achieve desired results.  Some photo manipulations are considered skillful artwork while others are frowned upon as unethical practices, especially when used to deceive the public, such as that used for political propaganda, or to make a product or person look better.Depending on the application and intent, some photo manipulations are considered an art form because it involves the creation of unique images and in some instances, signature expressions of art by photographic artists.  For example, Ansel Adams employed some of the more common manipulations using darkroom exposure techniques, such as burning (darkening) and dodging (lightening) a photograph.[1][2]  Other examples of photo manipulation include retouching photographs using ink or paint, airbrushing, double exposure, piecing photos or negatives together in the darkroom, scratching instant films, or through the use of software-based manipulation tools applied to digital images.  There are a number of software applications available for digital image manipulation, ranging from professional applications to very basic imaging software for casual users.",Photo manipulation,https://en.wikipedia.org/wiki/Photo_manipulation
,,
"Computer animation is the process used for generating animated images. The more general term computer-generated imagery (CGI) encompasses both static scenes and dynamic images, while computer animation only refers to the moving images. Modern computer animation usually uses 3D computer graphics, although 2D computer graphics are still used for stylistic, low bandwidth, and faster real-time renderings. Sometimes, the target of the animation is the computer itself, but sometimes film as well.Computer animation is essentially a digital successor to the stop motion techniques using 3D models, and traditional animation techniques using frame-by-frame animation of 2D illustrations. Computer-generated animations are more controllable than other more physically based processes, constructing miniatures for effects shots or hiring extras for crowd scenes, and because it allows the creation of images that would not be feasible using any other technology. It can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it, but advanced slightly in time (usually at a rate of 24, 25 or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures.For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. For 2D figure animations, separate objects (illustrations) and separate transparent layers are used with or without that virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.[1]For 3D animations, all frames must be rendered after the modeling is complete. For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use software on the end-users computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.",Computer animation,https://en.wikipedia.org/wiki/Computer_animation
,,
"Cross-validation, sometimes called rotation estimation,[1][2][3] or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[4] The goal of cross-validation is to test the model?€?s ability to predict new data that was not used in estimating it, in order to flag problems like overfitting[citation needed] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model?€?s predictive performance.In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.[5]",Cross-validation (statistics),https://en.wikipedia.org/wiki/Cross-validation_(statistics)
,,
"Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids. It is necessary because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form.  While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.Examples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles.The methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied.  That system can be one molecule, a group of molecules, or a solid.  Computational chemistry methods range from very approximate to highly accurate; the latter are usually feasible for small systems only. Ab initio methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters.Both ab initio and semi-empirical approaches involve approximations.  These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all.  For example, most ab initio calculations make the Born?€?Oppenheimer approximation, which greatly simplifies the underlying Schr??dinger equation by assuming that the nuclei remain in place during the calculation.  In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced.  In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains.  The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target.",Computational chemistry,https://en.wikipedia.org/wiki/Computational_chemistry
,,
"Computational mathematics may refer to two different aspect of the relation between computing and mathematics. 
Computational applied mathematics consists roughly of using mathematics for allowing and improving computer computation in applied mathematics. Computational mathematics may also refer to the use of computers for mathematics itself. This includes the use of computers for mathematical computations (computer algebra), the study of what can (and cannot) be computerized in mathematics (effective methods), which computations may be done with present technology (complexity theory), and which proofs can be done on computers (proof assistants).Computational applied mathematics involves mathematical research in areas of science where computing plays a central and essential role, emphasizing algorithms, numerical methods, and symbolic computations. Computation in research is prominent.[1] Computational mathematics emerged as a distinct part of applied mathematics by the early 1950s. Currently, computational mathematics can refer to or include:computational science, also known as  scientific computation or computational engineering
solving mathematical problems by computer simulation as opposed to analytic methods of applied mathematics
numerical methods used in scientific computation, for example numerical linear algebra and numerical solution of partial differential equations
stochastic methods,[2] such as Monte Carlo methods and other representations of uncertainty in scientific computation, for example stochastic finite elements
the mathematics of scientific computation[3] (the theoretical side involving mathematical proofs[4]), in particular numerical analysis, the theory of numerical methods (but theory of computation and  complexity of algorithms belong to theoretical computer science)
symbolic computation and computer algebra systems
computer-assisted research in various areas of mathematics, such as logic (automated theorem proving), discrete mathematics (search for mathematical structures such as groups), number theory (primality testing and factorization), cryptography, and computational algebraic topology
computational linguistics, the use of mathematical and computer techniques in natural languages
computational algebraic geometry
computational group theory
computational geometry
computational number theory
computational topology
computational statistics
algorithmic information theory
algorithmic game theory
use of mathematics in economics, finance and to certain extents of accounting i.e. use of differential and integral calculus(newton's method) and financial maths to solve real life problems.",Computational mathematics,https://en.wikipedia.org/wiki/Computational_mathematics
,,
"Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists.[1] Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science.It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics, a third way that supplements theory and experiment.[2]",Computational physics,https://en.wikipedia.org/wiki/Computational_physics
,,
"E-commerce is the activity of buying or selling of products on online services or over the Internet. Electronic commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems.Modern electronic commerce typically uses the World Wide Web for at least one part of the transaction's life cycle although it may also use other technologies such as e-mail. Typical e-commerce transactions include the purchase of online books (such as Amazon) and music purchases (music download in the form of digital distribution such as iTunes Store), and to a less extent, customized/personalized online liquor store inventory services.[1] There are three areas of e-commerce: online retailing, electric markets, and online auctions. E-commerce is supported by electronic business.[2]E-commerce businesses may also employ some or all of the followings:Online shopping for retail sales direct to consumers via Web sites and mobile apps, and conversational commerce via live chat, chatbots, and voice assistants[3]
Providing or participating in online marketplaces, which process third-party business-to-consumer or consumer-to-consumer sales
Business-to-business buying and selling;
Gathering and using demographic data through web contacts and social media
Business-to-business (B2B) electronic data interchange
Marketing to prospective and established customers by e-mail or fax (for example, with newsletters)
Engaging in pretail for launching new products and services
Online financial exchanges for currency exchanges or trading purposes.",E-commerce,https://en.wikipedia.org/wiki/E-commerce
,,
"Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than individual users. Such organizations include businesses, schools, interest-based user groups, clubs, charities, and governments.[1] Enterprise software is an integral part of a (computer-based) information system.Services provided by enterprise software are typically business-oriented tools, such as online shopping, and online payment processing, interactive product catalogue, automated billing systems, security, Business Process Management,  enterprise content management, IT service management, customer relationship management, enterprise resource planning, business intelligence, project management, collaboration, human resource management, manufacturing, occupational health and safety, enterprise application integration, and enterprise forms automation.As enterprises have similar departments and systems in common, enterprise software is often available as a suite of customizable programs. Generally, the complexity of these tools requires specialist capabilities and specific knowledge.",Enterprise software,https://en.wikipedia.org/wiki/Enterprise_software
,,
"In computer science, a library is a collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs.  A program invokes the library-provided behavior via a mechanism of the language.  For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call.  What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system.Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program.  This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program.  In that case, there may be internal libraries that are reused by independent sub-portions of the large program.  The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface, and not the internal details of the library.The value of a library lies in the reuse of the behavior.  When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion, and ease the distribution of the code.The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases.  If the code of the library is accessed during the build of the invoking program, then the library is called a static library.[1]  An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation.  The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution.  In this case the library is called a dynamic library (loaded at run time).  A dynamic library can be loaded and linked when preparing a program for execution, by the linker.  Alternatively, in the middle of execution, an application may explicitly request that a module be loaded.Most compiled languages have a standard library although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have commoditized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries.",Library (computing),https://en.wikipedia.org/wiki/Library_(computing)
,,
"Solid modeling (or modelling) is a consistent set of principles for mathematical and computer modeling of three-dimensional solids. Solid modeling is distinguished from related areas of geometric modeling and  computer graphics by its emphasis on  physical fidelity.[1] Together, the principles of geometric and solid modeling form the foundation of 3D-computer-aided design and in general support the creation, exchange, visualization, animation, interrogation, and annotation of  digital models of physical objects.",Solid modeling,https://en.wikipedia.org/wiki/Solid_modeling
,,
"Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Early versions of MTL were called ""hints""[4][5]In a widely cited 1997 paper, Rich Caruana gave the following characterization:",Multi-task learning,https://en.wikipedia.org/wiki/Multi-task_learning
,,
"Electronic voting (also known as e-voting) refers to voting using electronic means to either aid or take care of the chores of casting and counting votes.Depending on the particular implementation, e-voting may use standalone electronic voting machines (also called EVM) or computers connected to the Internet.  It may encompass a range of Internet services, from basic transmission of tabulated results to full-function online voting through common connectable household devices. The degree of automation may be limited to marking a paper ballot, or may be a comprehensive system of vote input, vote recording, data encryption and transmission to servers, and consolidation and tabulation of election results.A worthy e-voting system must perform most of these tasks while complying with a set of standards established by regulatory bodies, and must also be capable to deal successfully with strong requirements associated with security, accuracy, integrity, swiftness, privacy, auditability, accessibility, cost-effectiveness, scalability and ecological sustainability.Electronic voting technology can include punched cards, optical scan voting systems and specialized voting kiosks (including self-contained direct-recording electronic voting systems, or DRE). It can also involve transmission of ballots and votes via telephones, private computer networks, or the Internet.In general, two main types of e-voting can be identified:e-voting which is physically supervised by representatives of governmental or independent electoral authorities (e.g. electronic voting machines located at polling stations);
remote e-voting via the Internet (also called i-voting) where the voter submits his or her votes electronically to the election authorities, from any location.[1][2][3][4][5]",Electronic voting,https://en.wikipedia.org/wiki/Electronic_voting
,,
"Digital art is an artistic work or practice that uses digital technology as an essential part of the creative or presentation process. Since the 1970s, various names have been used to describe the process, including computer art and multimedia art. Digital art is itself placed under the larger umbrella term new media art.[1][2]After some initial resistance,[3] the impact of digital technology has transformed activities such as painting, drawing, sculpture and music/sound art,  while new forms, such as net art, digital installation art, and virtual reality, have become recognized artistic practices.[4] More generally the term digital artist is used to describe an artist who makes use of digital technologies in the production of art. In an expanded sense, ""digital art"" is contemporary art that uses the methods of mass production or digital media.[5]",Digital art,https://en.wikipedia.org/wiki/Digital_art
,,
"Electronic publishing (also referred to as e-publishing or digital publishing or online publishing) includes the digital publication of e-books, digital magazines, and the development of digital libraries and catalogues. It also includes an editorial aspect, that consists of editing books, journals or magazines that are mostly destined to be read on a screen (computer, e-reader, tablet, smartphone).[1]",Electronic publishing,https://en.wikipedia.org/wiki/Electronic_publishing
,,
"Not to be confused with computer engineering.Computational science and engineering (CSE) is a relatively new discipline that deals with the development and application of computational models and simulations, often coupled with high-performance computing, to solve complex physical problems arising in engineering analysis and design (computational engineering) as well as natural phenomena (computational science). CSE has been described as the ""third mode of discovery"" (next to theory and experimentation).[1] In many fields, computer simulation is integral and therefore essential to business and research. Computer simulation provides the capability to enter fields that are either inaccessible to traditional experimentation or where carrying out traditional empirical inquiries is prohibitively expensive. CSE should neither be confused with pure computer science, nor with computer engineering, although a wide domain in the former is used in CSE (e.g., certain algorithms, data structures, parallel programming, high performance computing) and some problems in the latter can be modeled and solved with CSE methods (as an application area).It is typically offered as a masters or doctorate program at several institutions.",Computational engineering,https://en.wikipedia.org/wiki/Computational_engineering
,,
"Computational social science refers to the academic sub-disciplines concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena.  Fields include computational economics, computational sociology, cliodynamics, culturomics,  and the automated analysis of contents, in social and traditional media. It focuses on investigating social and behavioral relationships and interactions through social simulation, modeling, network analysis, and media analysis.[1]",Computational social science,https://en.wikipedia.org/wiki/Computational_social_science
,,
"Health informatics (also called health care informatics, healthcare informatics, medical informatics, nursing informatics,  clinical informatics, or biomedical informatics) is information engineering applied to the field of health care, essentially the management and use of patient healthcare information. It is a multidisciplinary field[1] that uses health information technology (HIT) to improve health care via any combination of higher quality, higher efficiency (spurring lower cost and thus greater availability), and new opportunities. The disciplines involved include information science, computer science, social science, behavioral science, management science, and others. The NLM defines health informatics as ""the interdisciplinary study of the design, development, adoption and application of IT-based innovations in healthcare services delivery, management and planning"".[2] It deals with the resources, devices, and methods required to optimize the acquisition, storage, retrieval, and use of information in health and biomedicine. Health informatics tools include computers, clinical guidelines, formal medical terminologies, and information and communication systems, amongst others.[3][4] It is applied to the areas of nursing, clinical medicine, dentistry, pharmacy, public health, occupational therapy, physical therapy, biomedical research, and alternative medicine,[5][unreliable medical source?] all of which are designed to improve the overall of effectiveness of patient care delivery by ensuring that the data generated is of a high quality.[6]The international standards on the subject are covered by ICS 35.240.80[7] in which ISO 27799:2008 is one of the core components.[8]",Health informatics,https://en.wikipedia.org/wiki/Health_informatics
,,
"Cyberwarfare is the use or targeting in a battlespace or warfare context of computers, online control systems and networks.[1] It involves both offensive and defensive operations pertaining to the threat of cyberattacks, espionage and sabotage.[1] There has been controversy over whether such operations can be called ""war"". Nevertheless, powers have been developing cyber capabilities and engaged in cyberwarfare, both offensively and defensively, including the United States, China, Russia, Israel and the United Kingdom. Two other notable players are Iran and North Korea.[2]",Cyberwarfare,https://en.wikipedia.org/wiki/Cyberwarfare
,,
"A video game is an electronic game that involves interaction with a user interface to generate visual feedback on a video device such as a TV screen or computer monitor. The word video in video game traditionally referred to a raster display device, but as of the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Some theorists categorize video games as an art form, but this designation is controversial.The electronic systems used to play video games are known as platforms; in addition to general-purpose computers like a laptop/desktop being used, there are devices created exclusively for the playing of video games. Platforms range from large mainframe computers to small handheld computing devices. Video games are developed and released for specific platforms; for example, a video game that is available to Steam may not be available to Xbox One. Specialized video games such as arcade games, in which the video game components are housed in a large, typically coin-operated chassis, while common in the 1980s in video arcades, have gradually declined due to the widespread availability of affordable home video game consoles (e.g., PlayStation 4, Xbox One and Nintendo Switch) and video games on desktop/laptops and smartphones.The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, or even a person's body, using a Kinect sensor. Players view the game on a display device such as a television or computer monitor or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets.In the 2010s, the commercial importance of the video game industry is increasing. The emerging Asian markets and mobile games on smartphones in particular are driving the growth of the industry. As of 2015, video games generated sales of USD 74 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.",Video game,https://en.wikipedia.org/wiki/Video_game
,,
"A document management system (DMS) is a system (based on computer programs in the case of the management of digital documents) used to track, manage and store documents and reduce paper. Most are capable of keeping a record of the various versions created and modified by different users (history tracking). The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.",Document management system,https://en.wikipedia.org/wiki/Document_management_system
,,
"A word processor is a computer program or device that provides for input, editing, formatting and output of text, often plus other features.Early word processors were stand-alone devices dedicated to the function, but current word processors are word processor programs running on general purpose computers.The functions of a word processor program fall somewhere between those of a simple text editor and a fully functioned desktop publishing program. However the distinctions between these three have changed over time, and are somewhat unclear.[1][2]",Word processor,https://en.wikipedia.org/wiki/Word_processor
,,
"Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, ecological, behavioral, and social systems.[1] The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science and evolution.[2]Computational biology is different from biological computing, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an interdisciplinary science using computers to store and process biological data.",Computational biology,https://en.wikipedia.org/wiki/Computational_biology
,,
"Operations research, or operational research in British usage,  is a discipline that deals with the application of advanced analytical methods to help make better decisions.[1] Further, the term 'operational analysis' is used in the British (and some British Commonwealth) military as an intrinsic part of capability development, management and assurance.  In particular, operational analysis forms part of the Combined Operational Effectiveness and Investment Appraisals (COEIA), which support British defense capability acquisition decision-making.It is often considered to be a sub-field of applied mathematics.[2]  The terms management science and decision science are sometimes used as synonyms.[3]Employing techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. Operations research is often concerned with determining the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost) of some real-world objective. Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.[4]",Operations research,https://en.wikipedia.org/wiki/Operations_research
,,
"In software engineering, software configuration management (SCM or S/W CM)[1] is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.[2]  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine what was changed and who changed it.  If a configuration is working well, SCM can determine how to replicate it across many hosts.The acronym ""SCM"" is also expanded as source configuration management process and software change and configuration management.[3]  However, ""configuration"" is generally understood to cover changes typically made by a system administrator.",Software configuration management,https://en.wikipedia.org/wiki/Software_configuration_management
,,
"An integrated development environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of a source code editor, build automation tools, and a debugger. Most modern IDEs have intelligent code completion. Some IDEs, such as NetBeans and Eclipse, contain a compiler, interpreter, or both; others, such as SharpDevelop and Lazarus, do not. The boundary between an integrated development environment and other parts of the broader software development environment is not well-defined. Sometimes a version control system, or various tools to simplify the construction of a graphical user interface (GUI), are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram, for use in object-oriented software development.",Integrated development environment,https://en.wikipedia.org/wiki/Integrated_development_environment
,,
"Educational technology is ""the study and ethical practice of facilitating learning and improving performance by creating, using, and managing appropriate technological processes and resources"".[1]Educational technology is the use of both physical hardware and educational theoretics. It encompasses several domains including learning theory, computer-based training, online learning, and where mobile technologies are used, m-learning. Accordingly, there are several discrete aspects to describing the intellectual and technical development of educational technology:Educational technology as the theory and practice of educational approaches to learning.
Educational technology as technological tools and media, for instance massive online courses, that assist in the communication of knowledge, and its development and exchange. This is usually what people are referring to when they use the term 'EdTech'.
Educational technology for learning management systems (LMS), such as tools for student and curriculum management, and education management information systems (EMIS).
Educational technology as back-office management, such as training management systems for logistics and budget management, and Learning Record Store (LRS) for learning data storage and analysis.
Educational technology itself as an educational subject; such courses may be called ""Computer Studies"" or ""Information and communications technology (ICT)"".",Educational technology,https://en.wikipedia.org/wiki/Educational_technology
,,
"In computer programming, a software framework is an abstraction in which software providing generic functionality can be selectively changed by additional user-written code, thus providing application-specific software. A software framework provides a standard way to build and deploy applications. 
A software framework is a universal, reusable  software environment that provides particular functionality as part of a larger software platform to facilitate development of software applications, products and solutions. Software frameworks may include support programs, compilers, code libraries, tool sets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system.Frameworks have key distinguishing features that separate them from normal libraries:inversion of control: In a framework, unlike in libraries or in standard user applications, the overall program's flow of control is not dictated by the caller, but by the framework.[1]
extensibility: A user can extend the framework - usually by selective overriding; or programmers can add specialized user code to provide specific functionality.
non-modifiable framework code: The framework code, in general, is not supposed to be modified, while accepting user-implemented extensions. In other words, users can extend the framework, but should not modify its code.",Software framework,https://en.wikipedia.org/wiki/Software_framework
,,
"A compiler is computer software that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.[1]However, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[2]Compilers are not the only translators used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.",Compiler,https://en.wikipedia.org/wiki/Compiler#Compiler_construction
,,
A modeling language is any artificial language that can be used to express information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure.,Modeling language,https://en.wikipedia.org/wiki/Modeling_language
,,
"A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term ""domain-specific language"" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.The line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, PostScript is a Turing complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.",Domain-specific language,https://en.wikipedia.org/wiki/Domain-specific_language
,,
"A programming language is a formal language, which comprises a set of instructions used to produce various kinds of output. Programming languages are used to create programs that implement specific algorithms.Most programming languages consist of instructions for computers, although there are programmable machines that use a limited set of specific instructions, rather than the general programming languages of modern computers. Early ones preceded the invention of the digital computer, the first probably being the automatic flute player described in the 9th century by the brothers Musa in Baghdad, during the Islamic Golden Age.[1] From the early 1800s, programs were used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos.[2] However, their programs (such as a player piano's scrolls) could not produce different behavior in response to some input or condition.Thousands of different programming languages have been created, mainly in the computer field, and many more still are being created every year. Many programming languages require computation to be specified in an imperative form (i.e., as a sequence of operations to perform) while other languages use other forms of program specification such as the declarative form (i.e. the desired result is specified, not how to achieve it).The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.",Programming language,https://en.wikipedia.org/wiki/Programming_language
,,
"In the context of software engineering, software quality refers to two related but distinct notions that exist wherever quality is defined in a business context:Software functional quality reflects how well it complies with or conforms to a given design, based on functional requirements or specifications. That attribute can also be described as the fitness for purpose of a piece of software or how it compares to competitors in the marketplace as a worthwhile product.[1] It is the degree to which the correct software was produced.
Software structural quality refers to how it meets non-functional requirements that support the delivery of the functional requirements, such as robustness or maintainability. It has a lot more to do with the degree to which the software works as needed.",Software quality,https://en.wikipedia.org/wiki/Software_quality
,,
"In computing, a virtual machine (VM) is an emulation of a computer system.  Virtual machines are based on computer architectures and provide functionality of a physical computer. Their implementations may involve specialized hardware, software, or a combination.There are different kinds of virtual machines, each with different functions:System virtual machines (also termed full virtualization VMs) provide a substitute for a real machine. They provide functionality needed to execute entire operating systems. A hypervisor uses native execution to share and manage hardware, allowing for multiple environments which are isolated from one another, yet exist on the same physical machine. Modern hypervisors use hardware-assisted virtualization, virtualization-specific hardware, primarily from the host CPUs.
Process virtual machines are designed to execute computer programs in a platform-independent environment.",Virtual machine,https://en.wikipedia.org/wiki/Virtual_machine
,,
"Programming paradigms are a way to classify programming languages based on their features. Languages can be classified into multiple paradigms.Some paradigms are concerned mainly with implications for the execution model of the language, such as allowing side effects, or whether the sequence of operations is defined by the execution model.  Other paradigms are concerned mainly with the way that code is organized, such as grouping a code into units along with the state that is modified by the code.  Yet others are concerned mainly with the style of syntax and grammar.Common programming paradigms include:[1][2][3]imperative in which the programmer instructs the machine how to change its state,
procedural which groups instructions into procedures,
object-oriented which groups instructions together with the part of the state they operate on,
declarative in which the programmer merely declares properties of the desired result, but not how to compute it
functional in which the desired result is declared as the value of a series of function applications,
logic in which the desired result is declared as the answer to a question about a system of facts and rules,
mathematical in which the desired result is declared as the solution of an optimization problem",Programming paradigm,https://en.wikipedia.org/wiki/Programming_paradigm
,,
"Middleware is computer software that provides services to software applications beyond those available from the operating system. It can be described as ""software glue"".[1]Middleware makes it easier for software developers to implement communication and input/output, so they can focus on the specific purpose of their application. It gained popularity in the 1980s as a solution to the problem of how to link newer applications to older legacy systems, although the term had been in use since 1968.[2]",Middleware,https://en.wikipedia.org/wiki/Middleware
,,
"A network scheduler, also called packet scheduler, queueing discipline, qdisc or queueing algorithm, is an arbiter on a node in packet switching communication network. It manages the sequence of network packets in the transmit and receive queues of the network interface controller. There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms.The network scheduler logic decides which network packet to forward next. The network scheduler is associated with a queuing system, storing the network packets temporarily until they are transmitted. Systems may have a single or multiple queues in which case each may hold the packets of one flow, classification, or priority.In some cases it may not be possible to schedule all transmissions within the constraints of the system. In these cases the network scheduler is responsible for deciding which traffic to forward and what gets dropped.",Network scheduler,https://en.wikipedia.org/wiki/Network_scheduler
,,
"An operating system (OS) is system software that manages computer hardware and software resources and provides common services for computer programs.Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer???€?  from cellular phones and video game consoles to web servers and supercomputers.The dominant desktop operating system is Microsoft Windows with a market share of around 82.74%. macOS by Apple Inc. is in second place (13.23%), and the varieties of Linux are collectively in third place (1.57%).[3] In the mobile (smartphone and tablet combined) sector, use in 2017 is up to 70% of Google's Android[4] and according to third quarter 2016 data, Android on smartphones is dominant with 87.5 percent and a growth rate 10.3 percent per year, followed by Apple's iOS with 12.1 percent and a per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.[5] Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications.",Operating system,https://en.wikipedia.org/wiki/Operating_system
